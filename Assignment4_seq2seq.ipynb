{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment4_seq2seq.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Allen001822480/Info7374SpringTeam5/blob/Assignment4/Assignment4_seq2seq.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "mRlCrT_i-4eg",
        "colab_type": "code",
        "outputId": "c51ed7e5-3eae-4d75-c99e-685cb47524b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "import re\n",
        "tw = open('./TwitterLowerAsciiCorpus.txt')\n",
        "twitter = tw.read()\n",
        "data = [d for d in twitter.split('\\n')]\n",
        "data = [d for d in data if d != '']\n",
        "#data = eval('[%s]'%repr(data).replace('[', '').replace(']', ''))\n",
        "data = list(map(lambda x:re.sub(r'^A-Za-z\\d\\s\\,\\.\\!\\?\\'\\\"\\+\\-','',x), data))\n",
        "print(data[0:5])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[\"what's up dadyo when did you get back on twitter? haha\", \"like 2 weeks ago and it's going as terribly as i remember, but deg is still hilarious so it's ok\", 'literally never about that account, love it.', 'answer me this fellow apple peoples: how many times in the past year have you used the escape key?', 'about 50 times today. terminal vim user.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "f_ZI_NJL_F4d",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "l1 = ['won’t','won\\'t','wouldn’t','wouldn\\'t','’m', '’re', '’ve', '’ll', '’s','’d', 'n’t', '\\'m', '\\'re', '\\'ve', '\\'ll', '\\'s', '\\'d', 'can\\'t', 'n\\'t', 'B: ', 'A: ', ',', ';', '.', '?', '!', ':', '. ?', ',   .', '. ,', 'STA', 'END', 'sta', 'end']\n",
        "l2 = ['will not','will not','would not','would not',' am', ' are', ' have', ' will', ' is', ' had', ' not', ' am', ' are', ' have', ' will', ' is', ' had', 'can not', ' not', '', '', ' ,', ' ;', ' .', ' ?', ' !', ' :', '? ', '.', ',', '', '', '', '']\n",
        "\n",
        "for i, raw_word in enumerate(data):\n",
        "    for j, term in enumerate(l1):\n",
        "        raw_word = raw_word.replace(term,l2[j])\n",
        "    \n",
        "    data[i] = raw_word.lower()\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZJCSozFM_KYM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data = list(map(lambda x:'STA '+x+' END', data))\n",
        "context = data[::2]\n",
        "answers = data[1::2]\n",
        "all = context + answers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eEIFUO5n_L7F",
        "colab_type": "code",
        "outputId": "db74a864-5038-458c-8eed-2b0853defec5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import itertools\n",
        "from keras.preprocessing import sequence\n",
        "vocabulary_file = 'vocabulary_twitter'\n",
        "padded_context_file = 'Padded_context'\n",
        "padded_answers_file = 'Padded_answers'\n",
        "unknown_token = 'something'\n",
        "\n",
        "vocabulary_size = 10000\n",
        "max_features = vocabulary_size\n",
        "maxlen_input = 50\n",
        "maxlen_output = 50  # cut texts after this number of words\n",
        "\n",
        "all = ' '.join(all)\n",
        "tokenized_all = all.split()\n",
        "tokenized_context = [t.split() for t in context]\n",
        "tokenized_answers = [t.split() for t in answers]\n",
        "\n",
        "word_freq = nltk.FreqDist(itertools.chain(tokenized_all))\n",
        "print (\"Found %d unique words tokens.\" % len(word_freq.items()))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Found 11854 unique words tokens.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mKy5TunA_PNv",
        "colab_type": "code",
        "outputId": "063cd71a-bdcb-4f78-8952-a7783100cc44",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "vocab = word_freq.most_common(vocabulary_size-1)\n",
        "with open(vocabulary_file, 'wb') as v:\n",
        "  pickle.dump(vocab, v)\n",
        "\n",
        "print(vocab[0:20])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('STA', 10514), ('END', 10514), ('.', 6665), ('i', 5236), ('the', 2913), ('to', 2577), ('you', 2550), ('is', 2536), ('!', 2242), (',', 2213), ('it', 2051), ('a', 2046), ('not', 1964), ('?', 1839), ('and', 1655), ('that', 1436), ('my', 1196), ('in', 1171), ('of', 1082), ('am', 1056)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "SS9Jbs0A_SCl",
        "colab_type": "code",
        "outputId": "4b924e7a-3926-45c2-bf32-60cc378506cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "cell_type": "code",
      "source": [
        "vocab = pickle.load(open(vocabulary_file, 'rb'))\n",
        "index_to_word = [x[0] for x in vocab]\n",
        "index_to_word.append(unknown_token)\n",
        "word_to_index = dict([(w,i) for i,w in enumerate(index_to_word)])\n",
        "\n",
        "print (\"Using vocabulary of size %d.\" % vocabulary_size)\n",
        "print (\"The least frequent word in our vocabulary is '%s' and appeared %d times.\" % (vocab[-1][0], vocab[-1][1]))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using vocabulary of size 10000.\n",
            "The least frequent word in our vocabulary is 'mcdonalds' and appeared 1 times.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "rsElu_dP_Ua4",
        "colab_type": "code",
        "outputId": "71a4536b-276c-45ef-ccef-4979138ff8f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "# Replacing all words not in our vocabulary with the unknown token:\n",
        "for i, sent in enumerate(tokenized_answers):\n",
        "  tokenized_answers[i] = [w if w in word_to_index else unknown_token for w in sent]\n",
        "   \n",
        "for i, sent in enumerate(tokenized_context):\n",
        "  tokenized_context[i] = [w if w in word_to_index else unknown_token for w in sent]\n",
        "\n",
        "# Creating the training data:\n",
        "X = np.asarray([[word_to_index[w] for w in sent] for sent in tokenized_context])\n",
        "Y = np.asarray([[word_to_index[w] for w in sent] for sent in tokenized_answers])\n",
        "\n",
        "Q = sequence.pad_sequences(X, maxlen=maxlen_input, padding='post')\n",
        "A = sequence.pad_sequences(Y, maxlen=maxlen_output, padding='post')\n",
        "\n",
        "print(Q.shape)\n",
        "\n",
        "with open(padded_context_file, 'wb') as q:\n",
        "    pickle.dump(Q, q)\n",
        "    \n",
        "with open(padded_answers_file, 'wb') as a:\n",
        "    pickle.dump(A, a)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5257, 50)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "V4hG9clF_Xlk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import requests\n",
        "file_url = \"http://nlp.stanford.edu/data/glove.6B.zip\"\n",
        "tr = requests.get(file_url, stream=True)\n",
        "with open(\"glove.6B.zip\", \"wb\") as f:\n",
        "    for chunk in tr.iter_content(chunk_size=1024):\n",
        "        if chunk:\n",
        "            f.write(chunk)\n",
        "            \n",
        "            \n",
        "import zipfile\n",
        "import os\n",
        "def un_zip(file_name):\n",
        "    \"\"\"unzip zip file\"\"\"\n",
        "    zip_file = zipfile.ZipFile(file_name)\n",
        "    if os.path.isdir(file_name + \"_files\"):\n",
        "        pass\n",
        "    else:\n",
        "        os.mkdir(file_name + \"_files\")\n",
        "    for names in zip_file.namelist():\n",
        "        zip_file.extract(names,file_name + \"_files/\")\n",
        "    zip_file.close()\n",
        "        \n",
        "glove = un_zip(\"glove.6B.zip\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "X0zFIatn_bn4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "padded_context_file = 'Padded_context'\n",
        "padded_answers_file = 'Padded_answers'\n",
        "unknown_token = 'something'\n",
        "word_embedding_size = 100\n",
        "sentence_embedding_size = 300\n",
        "dictionary_size = 10000\n",
        "maxlen_input = 50\n",
        "maxlen_output = 50\n",
        "num_subsets = 2\n",
        "Epochs = 10\n",
        "BatchSize = 128 \n",
        "Patience = 0\n",
        "dropout = .25\n",
        "n_test = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vPSeHTgw_eeP",
        "colab_type": "code",
        "outputId": "ae1a4551-b82a-45d9-92d2-0d24a22cc37d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        }
      },
      "cell_type": "code",
      "source": [
        "import _pickle\n",
        "import numpy as np\n",
        "\n",
        "embeddings_index = {}\n",
        "f = open('./glove.6B.zip_files/glove.6B.100d.txt')\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print('Found %s word vectors.' % len(embeddings_index))\n",
        "embedding_matrix = np.zeros((dictionary_size, word_embedding_size))\n",
        "\n",
        "# Loading our vocabulary:\n",
        "vocabulary = _pickle.load(open(vocabulary_file, 'rb'))\n",
        "\n",
        "# Using the Glove embedding:\n",
        "i = 0\n",
        "for word in vocabulary:\n",
        "    embedding_vector = embeddings_index.get(word[0])\n",
        "    \n",
        "    if embedding_vector is not None:\n",
        "        # words not found in embedding index will be all-zeros.\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "    i += 1\n",
        "    \n",
        "print(embedding_matrix)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 400000 word vectors.\n",
            "[[ 0.          0.          0.         ...  0.          0.\n",
            "   0.        ]\n",
            " [ 0.          0.          0.         ...  0.          0.\n",
            "   0.        ]\n",
            " [-0.33978999  0.20941     0.46348    ... -0.23394001  0.47297999\n",
            "  -0.028803  ]\n",
            " ...\n",
            " [-0.16700999  0.10193    -0.62102997 ... -0.55523002 -0.60065001\n",
            "  -0.22685   ]\n",
            " [ 0.31239     0.031386   -0.27726999 ...  0.28920999  0.82100999\n",
            "   0.84512001]\n",
            " [ 0.          0.          0.         ...  0.          0.\n",
            "   0.        ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "s-pWlk3-_hen",
        "colab_type": "code",
        "outputId": "139a8427-6ec0-46e6-8817-9751741ab905",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.layers import Input, Embedding, LSTM, Dense, RepeatVector, Bidirectional, Dropout, concatenate\n",
        "from keras.optimizers import Adam, SGD\n",
        "from keras.models import Model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Activation, Dense\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.preprocessing import sequence\n",
        "import keras.backend as K\n",
        "import os\n",
        "import theano.tensor as T\n",
        "import os\n",
        "import pandas as pd\n",
        "import sys\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "weights_file = 'assignment4_twitter.h5'\n",
        "ad = Adam(lr=0.00005) \n",
        "\n",
        "input_context = Input(shape=(maxlen_input,), name='input_context')\n",
        "input_answer = Input(shape=(maxlen_input,), name='input_answer')\n",
        "LSTM_encoder = LSTM(sentence_embedding_size, init= 'lecun_uniform')\n",
        "LSTM_decoder = LSTM(sentence_embedding_size, init= 'lecun_uniform')\n",
        "if os.path.isfile(weights_file):\n",
        "    Shared_Embedding = Embedding(output_dim=word_embedding_size, input_dim=dictionary_size, input_length=maxlen_input)\n",
        "else:\n",
        "    Shared_Embedding = Embedding(output_dim=word_embedding_size, input_dim=dictionary_size, weights=[embedding_matrix], input_length=maxlen_input)\n",
        "word_embedding_context = Shared_Embedding(input_context)\n",
        "context_embedding = LSTM_encoder(word_embedding_context)\n",
        "\n",
        "word_embedding_answer = Shared_Embedding(input_answer)\n",
        "answer_embedding = LSTM_decoder(word_embedding_answer)\n",
        "\n",
        "merge_layer = concatenate([context_embedding, answer_embedding])\n",
        "out = Dense(int(dictionary_size/2), activation=\"relu\")(merge_layer)\n",
        "out = Dense(dictionary_size, activation=\"softmax\")(out)\n",
        "\n",
        "model = Model(input=[input_context, input_answer], output = [out])\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer=ad)\n",
        "\n",
        "if os.path.isfile(weights_file):\n",
        "    model.load_weights(weights_file)\n",
        "    \n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:21: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(300, kernel_initializer=\"lecun_uniform\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:22: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(300, kernel_initializer=\"lecun_uniform\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:37: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_context (InputLayer)      (None, 50)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_answer (InputLayer)       (None, 50)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_2 (Embedding)         (None, 50, 100)      1000000     input_context[0][0]              \n",
            "                                                                 input_answer[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "lstm_3 (LSTM)                   (None, 300)          481200      embedding_2[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "lstm_4 (LSTM)                   (None, 300)          481200      embedding_2[1][0]                \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 600)          0           lstm_3[0][0]                     \n",
            "                                                                 lstm_4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 5000)         3005000     concatenate_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 10000)        50010000    dense_3[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 54,977,400\n",
            "Trainable params: 54,977,400\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "UmuRmGWd_kmf",
        "colab_type": "code",
        "outputId": "8968282f-6596-4b2b-e4ea-8a5378655438",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "cell_type": "code",
      "source": [
        "q = _pickle.load(open(padded_context_file, 'rb'))\n",
        "a = _pickle.load(open(padded_answers_file, 'rb'))\n",
        "n_exem, n_words = a.shape\n",
        "\n",
        "qt = q[0:n_test,:]\n",
        "at = a[0:n_test,:]\n",
        "q = q[n_test + 1:,:]\n",
        "a = a[n_test + 1:,:]\n",
        "\n",
        "print('Number of exemples = %d'%(n_exem - n_test))\n",
        "step = int(np.around((n_exem - n_test)/num_subsets))\n",
        "round_exem = int(step * num_subsets)\n",
        "print(step, round_exem)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of exemples = 5157\n",
            "2578 5156\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "7KpVa5oL_oMD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def print_result(input):\n",
        "  ans_partial = np.zeros((1,maxlen_input))\n",
        "  ans_partial[0,-1] = 0 #index of STA\n",
        "  for k in range(maxlen_input - 1):\n",
        "    ye = model.predict([input, ans_partial])\n",
        "    mp = np.argmax(ye)\n",
        "    ans_partial[0, 0:-1] = ans_partial[0, 1:]\n",
        "    ans_partial[0, -1] = mp\n",
        "  text = ''\n",
        "  for k in ans_partial[0]:\n",
        "    k = k.astype(int)\n",
        "    if k < (dictionary_size-2):\n",
        "      w = vocabulary[k]\n",
        "      text = text + w[0] + ' '\n",
        "  return(text)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "H11kNtzI_rJ0",
        "colab_type": "code",
        "outputId": "25bbc390-bcda-48ed-dad3-d360eb6876e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4119
        }
      },
      "cell_type": "code",
      "source": [
        "x = range(0,Epochs) \n",
        "valid_loss = np.zeros(Epochs)\n",
        "train_loss = np.zeros(Epochs)\n",
        "for m in range(Epochs):\n",
        "  # Loop over training batches due to memory constraints:\n",
        "  for n in range(0,round_exem,step):\n",
        "    q2 = q[n:n+step]\n",
        "    s = q2.shape\n",
        "    count = 0\n",
        "    for i, sent in enumerate(a[n:n+step]):\n",
        "      l = np.where(sent==1)\n",
        "      limit = l[0][0]\n",
        "      count += limit + 1\n",
        "    Q = np.zeros((count,maxlen_input))\n",
        "    A = np.zeros((count,maxlen_input))\n",
        "    Y = np.zeros((count,dictionary_size))\n",
        "    \n",
        "    count = 0\n",
        "    for i, sent in enumerate(a[n:n+step]):\n",
        "      ans_partial = np.zeros((1,maxlen_input))\n",
        "      # Loop over the positions of the current target output (the current output sequence):\n",
        "      l = np.where(sent==1)\n",
        "      limit = l[0][0]\n",
        "      for k in range(1,limit+1):\n",
        "        # Mapping the target output (the next output word) for one-hot codding:\n",
        "        y = np.zeros((1, dictionary_size))\n",
        "        y[0, sent[k]] = 1\n",
        "        # preparing the partial answer to input:\n",
        "        ans_partial[0,-k:] = sent[0:k]\n",
        "        # training the model for one epoch using teacher forcing:\n",
        "        Q[count, :] = q2[i:i+1] \n",
        "        A[count, :] = ans_partial \n",
        "        Y[count, :] = y\n",
        "        count += 1\n",
        "    print('Training epoch: %d, training examples: %d - %d'%(m,n, n + step))\n",
        "    \n",
        "    model.fit([Q, A], Y, batch_size=BatchSize, epochs=1)\n",
        "    test_input = qt[6:7]\n",
        "    print(print_result(test_input))\n",
        "  model.save_weights(weights_file, overwrite=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training epoch: 0, training examples: 0 - 2578\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Epoch 1/1\n",
            "36329/36329 [==============================] - 89s 2ms/step - loss: 6.3038\n",
            "STA END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 0, training examples: 2578 - 5156\n",
            "Epoch 1/1\n",
            "37537/37537 [==============================] - 89s 2ms/step - loss: 5.5544\n",
            "STA i i am am END END END END i am END END END i am END END END i am END END END i am END END END i am END END END i am END END END i am END END END i am END END END i \n",
            "Training epoch: 1, training examples: 0 - 2578\n",
            "Epoch 1/1\n",
            "36329/36329 [==============================] - 86s 2ms/step - loss: 5.4789\n",
            "STA i am not not not not not not not not not not not not to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to \n",
            "Training epoch: 1, training examples: 2578 - 5156\n",
            "Epoch 1/1\n",
            "37537/37537 [==============================] - 89s 2ms/step - loss: 5.2103\n",
            "STA i am not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not \n",
            "Training epoch: 2, training examples: 0 - 2578\n",
            "Epoch 1/1\n",
            "36329/36329 [==============================] - 85s 2ms/step - loss: 5.2449\n",
            "STA i am not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not \n",
            "Training epoch: 2, training examples: 2578 - 5156\n",
            "Epoch 1/1\n",
            "37537/37537 [==============================] - 89s 2ms/step - loss: 5.0062\n",
            "STA i am not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not \n",
            "Training epoch: 3, training examples: 0 - 2578\n",
            "Epoch 1/1\n",
            "36329/36329 [==============================] - 86s 2ms/step - loss: 5.0451\n",
            "STA i am not not not you END END END END END END END END END i am not not not you END END END END END END END END END END i am not not not you END END END END END END END END END i am not \n",
            "Training epoch: 3, training examples: 2578 - 5156\n",
            "Epoch 1/1\n",
            "37537/37537 [==============================] - 89s 2ms/step - loss: 4.8430\n",
            "STA i am not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not \n",
            "Training epoch: 4, training examples: 0 - 2578\n",
            "Epoch 1/1\n",
            "36329/36329 [==============================] - 85s 2ms/step - loss: 4.8735\n",
            "STA i am not not not a END END END END END END END END END i am not not not a END END END END END END END END END i am not not not a END END END END END END END END END i \n",
            "Training epoch: 4, training examples: 2578 - 5156\n",
            "Epoch 1/1\n",
            "37537/37537 [==============================] - 88s 2ms/step - loss: 4.6997\n",
            "STA i am not not not not not not not . END . END . END . . . . . . . . . . . . . . . . END . END . END . END . END . END . END . END . END \n",
            "Training epoch: 5, training examples: 0 - 2578\n",
            "Epoch 1/1\n",
            "36329/36329 [==============================] - 86s 2ms/step - loss: 4.7183\n",
            "STA i am not not a END END END END END END END i am not not a END END END END END END END END i am not not a END END END END END END END END i am not not a END END \n",
            "Training epoch: 5, training examples: 2578 - 5156\n",
            "Epoch 1/1\n",
            "37537/37537 [==============================] - 88s 2ms/step - loss: 4.5658\n",
            "STA i am not not not not not END END END END END END END END END i am not not not not END END END END END END END END END i am not not not not END END \n",
            "Training epoch: 6, training examples: 0 - 2578\n",
            "Epoch 1/1\n",
            "36329/36329 [==============================] - 86s 2ms/step - loss: 4.5683\n",
            "STA i am not not a . END . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . \n",
            "Training epoch: 6, training examples: 2578 - 5156\n",
            "Epoch 1/1\n",
            "37537/37537 [==============================] - 89s 2ms/step - loss: 4.4357\n",
            "STA i am not not not not END END END END END END END END i am not not not not END END END END END END END END i am not not not not \n",
            "Training epoch: 7, training examples: 0 - 2578\n",
            "Epoch 1/1\n",
            "36329/36329 [==============================] - 85s 2ms/step - loss: 4.4175\n",
            "STA i am not not not a . END . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . \n",
            "Training epoch: 7, training examples: 2578 - 5156\n",
            "Epoch 1/1\n",
            "37537/37537 [==============================] - 89s 2ms/step - loss: 4.3007\n",
            "STA i am not not even i am not not even i am not not even i am not not even END END END END END END END END END END END END END END END END END i am not not \n",
            "Training epoch: 8, training examples: 0 - 2578\n",
            "Epoch 1/1\n",
            "36329/36329 [==============================] - 86s 2ms/step - loss: 4.2569\n",
            "STA i am not sure i am not sure i am not sure i am not sure i am not not even i am not not even END END END END END END END END END END END END END END END END END END END i am not sure \n",
            "Training epoch: 8, training examples: 2578 - 5156\n",
            "Epoch 1/1\n",
            "37537/37537 [==============================] - 88s 2ms/step - loss: 4.1597\n",
            "STA i am not not even i am not END END END END ; i am not not even END END END END END END END END END END END END END END END END i am not not even i am not END END END END ; \n",
            "Training epoch: 9, training examples: 0 - 2578\n",
            "Epoch 1/1\n",
            "36329/36329 [==============================] - 85s 2ms/step - loss: 4.0862\n",
            "STA i am not sure i am not sure i am not sure i am not sure i am not sure i am not sure . END END END END END END END END END END END END END END END END END END i am not sure i am \n",
            "Training epoch: 9, training examples: 2578 - 5156\n",
            "Epoch 1/1\n",
            "37537/37537 [==============================] - 89s 2ms/step - loss: 4.0100\n",
            "STA i am not not even . . . . . . . . . . . . . . . END . END . END . END . END . END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 10, training examples: 0 - 2578\n",
            "Epoch 1/1\n",
            "36329/36329 [==============================] - 86s 2ms/step - loss: 3.9069\n",
            "STA i am not sure i am not sure i am not sure i am not even i am not even i am not even END END END END END END END END END END END END END END END END END END END END i am not sure i \n",
            "Training epoch: 10, training examples: 2578 - 5156\n",
            "Epoch 1/1\n",
            "37537/37537 [==============================] - 89s 2ms/step - loss: 3.8648\n",
            "STA i am not sure . . . . . . . . . . . . . . . . . . END . END . END END END END END END END END END END END END END END END END END END END END END i am \n",
            "Training epoch: 11, training examples: 0 - 2578\n",
            "Epoch 1/1\n",
            "36329/36329 [==============================] - 86s 2ms/step - loss: 3.7350\n",
            "STA i am sorry . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . \n",
            "Training epoch: 11, training examples: 2578 - 5156\n",
            "Epoch 1/1\n",
            "37537/37537 [==============================] - 89s 2ms/step - loss: 3.7278\n",
            "STA i am not sure i am not even i am not even i am not even i am not even i am not END END END END END END END END END END END END END END END END i am not sure i am not END \n",
            "Training epoch: 12, training examples: 0 - 2578\n",
            "Epoch 1/1\n",
            "36329/36329 [==============================] - 85s 2ms/step - loss: 3.5847\n",
            "STA i am not sure . . . . . . . . . . . . . . . . . END END END END END END END END END END END END END END END END END END END END i have been a fan . END \n",
            "Training epoch: 12, training examples: 2578 - 5156\n",
            "Epoch 1/1\n",
            "37537/37537 [==============================] - 89s 2ms/step - loss: 3.6081\n",
            "STA i am not sure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . \n",
            "Training epoch: 13, training examples: 0 - 2578\n",
            "Epoch 1/1\n",
            "36329/36329 [==============================] - 86s 2ms/step - loss: 3.4620\n",
            "STA i am not sure i am not sure i am not sure i am not sure i am not sure i am not sure END END END END END END END END END END END END END END END END END END i am not sure i am not \n",
            "Training epoch: 13, training examples: 2578 - 5156\n",
            "Epoch 1/1\n",
            "37537/37537 [==============================] - 88s 2ms/step - loss: 3.5018\n",
            "STA i am not sure i am not even END END END ; i am not sure i am not sure i am not sure . END END END END END END END END END END END END END END END END i am not sure i am not \n",
            "Training epoch: 14, training examples: 0 - 2578\n",
            "Epoch 1/1\n",
            "36329/36329 [==============================] - 86s 2ms/step - loss: 3.3617\n",
            "STA i am not sure i am not sure . END END END END ; i am not sure END END END END END END END END END END END END END END i am not sure i am trying to work END END END END END END END END \n",
            "Training epoch: 14, training examples: 2578 - 5156\n",
            "Epoch 1/1\n",
            "37537/37537 [==============================] - 89s 2ms/step - loss: 3.4079\n",
            "STA i am not not even much much as i am not not even many things . END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 15, training examples: 0 - 2578\n",
            "Epoch 1/1\n",
            "36329/36329 [==============================] - 86s 2ms/step - loss: 3.2743\n",
            "STA i am just trying to \"debate\" END END ; i am not sure i am not sure . . . . . . . . END END END END END END END END END END END END END END END END END i have not seen . END . \n",
            "Training epoch: 15, training examples: 2578 - 5156\n",
            "Epoch 1/1\n",
            "37537/37537 [==============================] - 88s 2ms/step - loss: 3.3228\n",
            "STA i am not sure . . . . . . . . . . . . . . . . . . END . END . END . END . END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 16, training examples: 0 - 2578\n",
            "Epoch 1/1\n",
            "36329/36329 [==============================] - 86s 2ms/step - loss: 3.1914\n",
            "STA i am not sure i am not sure . END is working . END END END END END END END END END END END END END i am not sure END END END END END END END END END i am not sure i am not sure . END \n",
            "Training epoch: 16, training examples: 2578 - 5156\n",
            "Epoch 1/1\n",
            "37537/37537 [==============================] - 89s 2ms/step - loss: 3.2428\n",
            "STA i am not sure . . . . . . . . END END . END . END . END . END . END . END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 17, training examples: 0 - 2578\n",
            "Epoch 1/1\n",
            "36329/36329 [==============================] - 85s 2ms/step - loss: 3.1173\n",
            "STA i am not sure i am not sure . . . . . . . . . . . . . END END END END END END END END END END END END END END END END END END i have not seen END END END END END END \n",
            "Training epoch: 17, training examples: 2578 - 5156\n",
            "Epoch 1/1\n",
            "37537/37537 [==============================] - 88s 2ms/step - loss: 3.1646\n",
            "STA i am not sure . . . . . . . . . END . END . END . END . END . END . END . END . END END END END END END END END END END END END END END END END i do not know \n",
            "Training epoch: 18, training examples: 0 - 2578\n",
            "Epoch 1/1\n",
            "36329/36329 [==============================] - 85s 2ms/step - loss: 3.0470\n",
            "STA i am just trying to do the . i am not sure i am not living . END . END END END END END END END END END END END END END END END END END END i am sure i am trying to do to do . \n",
            "Training epoch: 18, training examples: 2578 - 5156\n",
            "Epoch 1/1\n",
            "37537/37537 [==============================] - 88s 2ms/step - loss: 3.0930\n",
            "STA i am not sure . . . . . . . . . . . . . . . . . . . END . END . END . END . END . END . END . END . END . END . END . END . END . \n",
            "Training epoch: 19, training examples: 0 - 2578\n",
            "Epoch 1/1\n",
            "36329/36329 [==============================] - 86s 2ms/step - loss: 2.9762\n",
            "STA i am not sure i am not sure . . . . . . . . . . . . . . . . END END END END END END END END END END END END END END END END END END END i am not sure i am \n",
            "Training epoch: 19, training examples: 2578 - 5156\n",
            "Epoch 1/1\n",
            "37537/37537 [==============================] - 89s 2ms/step - loss: 3.0225\n",
            "STA i am not allowed to talk about pig with a . i am 72 END END END END END END END END END END END END i am not even END END END END END END END END END END i am not sure i am not \n",
            "Training epoch: 20, training examples: 0 - 2578\n",
            "Epoch 1/1\n",
            "36329/36329 [==============================] - 86s 2ms/step - loss: 2.9140\n",
            "STA i am not sure i am not sure . . . . . . . . . END END END END END END END END END END END END END END END END i have a END END END END END END \n",
            "Training epoch: 20, training examples: 2578 - 5156\n",
            "Epoch 1/1\n",
            "37537/37537 [==============================] - 89s 2ms/step - loss: 2.9601\n",
            "STA i am not sure . . . . . . . . . END . END . END . END . END . END . END . END END END END END END END END END END END END END END END END END i am not sure . \n",
            "Training epoch: 21, training examples: 0 - 2578\n",
            "Epoch 1/1\n",
            "36329/36329 [==============================] - 86s 2ms/step - loss: 2.8528\n",
            "STA i am so bored . i am not sure i am not sure i am not sure i was gonna be fixed for a day END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 21, training examples: 2578 - 5156\n",
            "Epoch 1/1\n",
            "37537/37537 [==============================] - 88s 2ms/step - loss: 2.8932\n",
            "STA i am not gonna be a fan . END END END END END END END are not END END END ; END END ; END END ; END ; END ; END \n",
            "Training epoch: 22, training examples: 0 - 2578\n",
            "Epoch 1/1\n",
            "36329/36329 [==============================] - 85s 2ms/step - loss: 2.7930\n",
            "STA i am sure i could not know . . . . . . . . . . . . . . . . . . . . . . . . . . . . . END END END END END END END END END END END END END \n",
            "Training epoch: 22, training examples: 2578 - 5156\n",
            "Epoch 1/1\n",
            "37537/37537 [==============================] - 89s 2ms/step - loss: 2.8287\n",
            "STA i am not sure it is a . END is a explanation . END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 23, training examples: 0 - 2578\n",
            "Epoch 1/1\n",
            "36329/36329 [==============================] - 86s 2ms/step - loss: 2.7358\n",
            "STA i am sayinggg jealous END END END END END i have not seen the . END is gonna be a good fri END END END END END END END END END END END END END END END END END i am not sure . . . . . \n",
            "Training epoch: 23, training examples: 2578 - 5156\n",
            "Epoch 1/1\n",
            "37537/37537 [==============================] - 88s 2ms/step - loss: 2.7736\n",
            "STA i am not feelin class for a min but i am 72 in the END END END END END END END END END i am not sure they were END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 24, training examples: 0 - 2578\n",
            "Epoch 1/1\n",
            "36329/36329 [==============================] - 85s 2ms/step - loss: 2.6815\n",
            "STA i am sayinggg jealous END END END END END i have not seen . . . .but is gonna be lit . END END END END END END END END END END END END END END END END END END END END i have bernie i had a \n",
            "Training epoch: 24, training examples: 2578 - 5156\n",
            "Epoch 1/1\n",
            "37537/37537 [==============================] - 88s 2ms/step - loss: 2.7127\n",
            "STA i am not feelin END END END i have not seen yet yet . i am undecided END END END END END END END END i have not seen END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 25, training examples: 0 - 2578\n",
            "Epoch 1/1\n",
            "36329/36329 [==============================] - 85s 2ms/step - loss: 2.6273\n",
            "STA i am not sure . . . . . . END END END END END END , i think the same time ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? \n",
            "Training epoch: 25, training examples: 2578 - 5156\n",
            "Epoch 1/1\n",
            "37537/37537 [==============================] - 88s 2ms/step - loss: 2.6553\n",
            "STA i am not feelin class END END END END i have not played the in the league . END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 26, training examples: 0 - 2578\n",
            "Epoch 1/1\n",
            "36329/36329 [==============================] - 86s 2ms/step - loss: 2.5727\n",
            "STA i am not sure i am not sure . END is working for the morning . END is great game END END END END END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 26, training examples: 2578 - 5156\n",
            "Epoch 1/1\n",
            "37537/37537 [==============================] - 88s 2ms/step - loss: 2.6037\n",
            "STA i am not feelin class for a little little bit END END END END END END END END END END i am not even yet . END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 27, training examples: 0 - 2578\n",
            "Epoch 1/1\n",
            "36329/36329 [==============================] - 85s 2ms/step - loss: 2.5192\n",
            "STA i am sayinggg jealous . . . . . . .creepy . END . END . END . END . END ? END ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? \n",
            "Training epoch: 27, training examples: 2578 - 5156\n",
            "Epoch 1/1\n",
            "37537/37537 [==============================] - 89s 2ms/step - loss: 2.5452\n",
            "STA i am not feelin class for a boat END END END END END END i am not sure they are not hitting land END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 28, training examples: 0 - 2578\n",
            "Epoch 1/1\n",
            "36329/36329 [==============================] - 85s 2ms/step - loss: 2.4689\n",
            "STA i am sorry . . . . . . . . . teanna the time that is not the best ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? \n",
            "Training epoch: 28, training examples: 2578 - 5156\n",
            "Epoch 1/1\n",
            "37537/37537 [==============================] - 88s 2ms/step - loss: 2.4977\n",
            "STA i am so sleepy END END END are not a . END . END END END END END END END END END END i am END END END END END are not END END END END END END END END END END END END END END \n",
            "Training epoch: 29, training examples: 0 - 2578\n",
            "Epoch 1/1\n",
            "36329/36329 [==============================] - 85s 2ms/step - loss: 2.4246\n",
            "STA i am sayinggg jealous END END END END are not you working in the world END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END i am not \n",
            "Training epoch: 29, training examples: 2578 - 5156\n",
            "Epoch 1/1\n",
            "37537/37537 [==============================] - 88s 2ms/step - loss: 2.4398\n",
            "STA i am not feelin class for a boat END END END END END END i am not sure they are not hitting me END END END END END END END END END END END END END END END END END END END END END END END END END \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ixfIVGjk_y1D",
        "colab_type": "code",
        "outputId": "4388d224-a6ca-4a34-9c2c-736ac04dffd2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1717
        }
      },
      "cell_type": "code",
      "source": [
        "for i in range(50):\n",
        "  print('input: %s'%context[i].replace('STA','').replace('END',''))\n",
        "  output = print_result(qt[i:i+1])\n",
        "  print('output: %s'%output)\n",
        "  i += 1\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input:  what is up dadyo when did you get back on twitter ? haha \n",
            "output: STA i am so happy for you END END END END END END END END END i am END END END END END END i have not played in the middle of the . END END END END END END END END END END END END END END \n",
            "input:  literally never about that account , love it . \n",
            "output: STA i am so happy for the first episode of the END END END END END END END END END END END END END END END END \n",
            "input:  about 50 times today . terminal vim user . \n",
            "output: STA i am not sure . . . . . . . END END END END . END . END . END . END END END END END END END END END END END END END END END END END END END END END END \n",
            "input:  cmd+opt+esc is good but still available via menubar \n",
            "output: STA i am not sure . . . . . . . END END END END . END . END . END END END END END END END END END END END END END END END END END END END END END END \n",
            "input:  i am disgusted \n",
            "output: STA i am not a person . . . . . . END END , so , i will not have a fan . END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "input:  what a piece of shit \n",
            "output: STA i am not even joking slightly . . . . . . END END END END END END . END . END END END END END END END END END END END END END END END END END \n",
            "input:  yay , you great hunter . ive killed lots of lizards and bugs but never a mouse . \n",
            "output: STA i am not feelin class for a boat END END END END END END i am not sure they are not hitting me END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "input:  and then that mouse had the nerve to try to eat our kibble !  let this be a lesson fur all the other mousies !   \n",
            "output: STA i am so glad for the game , i am ready to y awake . END END END END END END END END END END i have been in the same . END END END END END END END END END END END END END END END END END \n",
            "input:  tomorrow \n",
            "output: STA i am not even fucking END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "input:  make sure i have a bed and seat saved next to you ! \n",
            "output: STA i am so happy for you END END END END END END END END END i am END END END END END END \n",
            "input:  wassup shorty .  \n",
            "output: STA i am not even pissed END END END END END END END i am not even do not even get a quick of . END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "input:  appreciate that shorty , you too .  \n",
            "output: STA i am not rich END END END END END END END END END END END END END END \n",
            "input:  yea \n",
            "output: STA i am not a person . . . . . . END END , so , i will not have a fan . END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "input:  gotchu \n",
            "output: STA i am not a person . . . . . . END END , so , i will not have a fan . END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "input:  good  wby \n",
            "output: STA i am not even a sports fan . . . . . END END , so many of the random of my jaw , and he is so END END END END END END END END END END END END END END END END END END END END END \n",
            "input:  that is wassup  \n",
            "output: STA i am not even pissed END END END END END END END i am not even mad END END END END END END END END END i am not sure . . . . . . END END END END END END END END END END END END END \n",
            "input:  and the dash for cash races too \n",
            "output: STA i am not sure . . . . . . . END END END END . END . END . END . END END END END END END END END END END END END END END END END END END END END END END \n",
            "input:  do you think this wouldve happened anyway if it was another driver stinkin it up like kyle has recently ? \n",
            "output: STA i am not feelin class END END END END i have not @ a fan END END END END END END END i am not sure they are not END END END END END END END END END END END END END END END END END END END \n",
            "input:  what did these niggas say \n",
            "output: STA i am not even joking slightly . . . . . END END END END END . END . END . END END END END END END END END END END END END END END END END END END \n",
            "input:  kenny is wife is white on top of that . u not gonna tell me u put a flag over the color of your skin . \n",
            "output: STA i am so glad for the energy , but i am fully to get the energy , but i am not sure . END END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "input:  u cute now \n",
            "output: STA i am not even pissed END END END END END END END i am not even mad END END END END END END END END i am not even END END END END END END END END END END END END END END END END END END END \n",
            "input:  any hot chicks wanna let me touch their but today ? \n",
            "output: STA i am so happy for you END END END END END END END END END i am END END END END END END END END END END \n",
            "input:  i havent groped that ass in a while i need dat \n",
            "output: STA i am so happy for you END END END END END END END END END END i am so happy for you END END END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "input:  so much to do before leaving for d .c . ahhh \n",
            "output: STA i am so happy for the first episode of the members . END END END END END END END END END END END END END END END END END END END END END END END \n",
            "input:   ? ? ? \n",
            "output: STA i am not even joking slightly . . . . . . END END END END END END END END END END END END END END END END END END END END END \n",
            "input:  white girls singing about \"niggers\" stealing should not have had to face any repercussions ?  . . . . . \n",
            "output: STA i am not ready to convince you . . . . . . . END . END . END . END . END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "input:  they are not 5 year olds . them repeating what they heard is not an excuse . it does not matter if there was malicious intent . \n",
            "output: STA i am so lost for the energy . i truly appreciate it . END is a . END END END END END END END END END END END END END END END END END END END END END END END . END . END . END . END \n",
            "input:  okay so i know we have had our differences on pineapples on pizza in the past but there is something else that needs attention . . . eggnog \n",
            "output: STA i am so glad for the energy . i am fully to get the energy . END is a ho . END END END END END END END END END END END END END END END END END END END END END END END END END END . END \n",
            "input:  what in the tits is a coquito \n",
            "output: STA i am not even joking slightly . . . . . END END END END . END . END . END . END END END END END END END END END END END END END END END END END END END END \n",
            "input:  just riding these ol dirt roads . \n",
            "output: STA i am not even joking slightly . . . . . . END END END END END END . END . END END END END END END END END END END END END END END END END END \n",
            "input:  k babe . \n",
            "output: STA i am not even pissed END END END END END END END i am not even mad END END END END END END END END END i am not sure . . . . . . END END END END END END END END END END END END END \n",
            "input:  hate how y'all try to call me \"fake gay\" no . . trust &amp ; believe vagina is wonderful to me , but so is your father . *hint hint* idiots \n",
            "output: STA i am so glad for the energy , but i am fully . END . END . END . END . END END END END END END END END END END END END END END END END END END END END END END i have been in the \n",
            "input:  but just plain dinner is lame . . i like to have something sweet sometimes to . they get to eat what they want , why i can not ? \n",
            "output: STA i am so upset . . . . . . . . . . . . . . . . . . . END a , . END . END . END . END END END END END END END END END END END END END \n",
            "input:  just one ? \n",
            "output: STA i am not even joking slightly . . . . . . END END END END END END END END END END END END END END END END END END END END END \n",
            "input:  yeah !  that sounds like the tracy in my timeline .  \n",
            "output: STA i am so happy for you . i am not sure . END is a great day . END END END END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "input:  dont you all het tired calling half of the country racist ? \n",
            "output: STA i am so happy for the END END END END END END i have not played in the league . END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "input:  yes . good night . \n",
            "output: STA i am not even joking slightly . . . . . . END END END END END . END . END . END END END END END END END END END END END END END END END END END END END END \n",
            "input:  my selfies got bad lighting ? lol \n",
            "output: STA i am not even joking slightly . . . . . . END END END END END END . END . END are not mad . END END END END END END END END END END END END END END END END END END END END END END \n",
            "input:  rightttttt lmfao \n",
            "output: STA i am not a person . . . . . . END END , so , i will not have a fan . END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "input:  yeah i do \n",
            "output: STA i am not even pissed END END END END END END END i am not even do not even get a quick of a END END END END END END END END END END END END END END END END END END END END END END END \n",
            "input:  yeah there should be room . usually the desks behind where i sit . \n",
            "output: STA i am so happy for you . i am not sure i could get to get it END END END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "input:  i will beat your ass  \n",
            "output: STA i am not even even get a costume . i am down END END END END END END END END END END END END END END END END END END END END \n",
            "input:  i am near the gym fight me pussy \n",
            "output: STA i am so happy for you END END END END END END END END END END i am not even yet . . . . . END END END END END END END END END END END END END END END END END END END END END END END \n",
            "input:  alright kiddo . 8 hours . \n",
            "output: STA i am not even joking slightly . . . . . . END END END END END END . END . END END END END END END END END END END END END END END END END END \n",
            "input:  i bought you a fudge brownie . it counts as cake . \n",
            "output: STA i am so happy for you . i am not sure . END is a great day . END END END END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "input:  i see \n",
            "output: STA i am not even even get a costume . END . END END END END END END END END END END i am not even do not even get it . END END END END END END END END END END END END END END END END END END \n",
            "input:  sooo . . . . did everyone on uapb campus tv is loose signal ? \n",
            "output: STA i am not sure what is working to . i am 72 hip replacement END END END END END END END END END END END END END END END END END END END END END END END \n",
            "input:  well idk but mine did \n",
            "output: STA i am not even joking slightly . . . . . END END END END END END END END END END END END END END END END END END END END \n",
            "input:  will you be at the phi delts friday ? ? ? \n",
            "output: STA i am so happy for the END END END END END END i have not played in the league . END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "input:  yes it is and yes i am !  \n",
            "output: STA i am so happy for you END END END END END END END END END END i am not even yet . . . . . END END END END END END END END END END END END END END END END END END END END END END END \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "E51c9hps4Spn",
        "colab_type": "code",
        "outputId": "bb626779-88fe-4172-89c3-43baece83f14",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4069
        }
      },
      "cell_type": "code",
      "source": [
        "x = range(0,Epochs) \n",
        "valid_loss = np.zeros(Epochs)\n",
        "train_loss = np.zeros(Epochs)\n",
        "for m in range(Epochs):\n",
        "  # Loop over training batches due to memory constraints:\n",
        "  for n in range(0,round_exem,step):\n",
        "    q2 = q[n:n+step]\n",
        "    s = q2.shape\n",
        "    count = 0\n",
        "    for i, sent in enumerate(a[n:n+step]):\n",
        "      l = np.where(sent==1)\n",
        "      limit = l[0][0]\n",
        "      count += limit + 1\n",
        "    Q = np.zeros((count,maxlen_input))\n",
        "    A = np.zeros((count,maxlen_input))\n",
        "    Y = np.zeros((count,dictionary_size))\n",
        "    \n",
        "    count = 0\n",
        "    for i, sent in enumerate(a[n:n+step]):\n",
        "      ans_partial = np.zeros((1,maxlen_input))\n",
        "      # Loop over the positions of the current target output (the current output sequence):\n",
        "      l = np.where(sent==1)\n",
        "      limit = l[0][0]\n",
        "      for k in range(1,limit+1):\n",
        "        # Mapping the target output (the next output word) for one-hot codding:\n",
        "        y = np.zeros((1, dictionary_size))\n",
        "        y[0, sent[k]] = 1\n",
        "        # preparing the partial answer to input:\n",
        "        ans_partial[0,-k:] = sent[0:k]\n",
        "        # training the model for one epoch using teacher forcing:\n",
        "        Q[count, :] = q2[i:i+1] \n",
        "        A[count, :] = ans_partial \n",
        "        Y[count, :] = y\n",
        "        count += 1\n",
        "    print('Training epoch: %d, training examples: %d - %d'%(m,n, n + step))\n",
        "    model.fit([Q, A], Y, batch_size=BatchSize, epochs=1)\n",
        "    test_input = qt[6:7]\n",
        "    print(print_result(test_input))\n",
        "  model.save_weights(weights_file, overwrite=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training epoch: 0, training examples: 0 - 2578\n",
            "Epoch 1/1\n",
            "36329/36329 [==============================] - 88s 2ms/step - loss: 2.4340\n",
            "STA i am sayinggg jealous END END END END END i have been played in the middle of the night END END END END END END END END END END END END END END END END END END END END END END END END END END END END i know \n",
            "Training epoch: 0, training examples: 2578 - 5156\n",
            "Epoch 1/1\n",
            "37537/37537 [==============================] - 89s 2ms/step - loss: 2.4508\n",
            "STA i am not feelin class END END END END i have not played the in the league END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 1, training examples: 0 - 2578\n",
            "Epoch 1/1\n",
            "36329/36329 [==============================] - 86s 2ms/step - loss: 2.3316\n",
            "STA i have ben , i had be at the day . i am not talking about it is a big day . END . END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 1, training examples: 2578 - 5156\n",
            "Epoch 1/1\n",
            "37537/37537 [==============================] - 89s 2ms/step - loss: 2.3696\n",
            "STA i am not feelin class END END END END i have not been in the END END END END END END END i am not sure they are not END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 2, training examples: 0 - 2578\n",
            "Epoch 1/1\n",
            "36329/36329 [==============================] - 86s 2ms/step - loss: 2.2767\n",
            "STA i am sayinggg jealous END END END END END i have been set in the middle of the night END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 2, training examples: 2578 - 5156\n",
            "Epoch 1/1\n",
            "37537/37537 [==============================] - 89s 2ms/step - loss: 2.3132\n",
            "STA i am not feelin class END END END END i have not seen they have to be in the in the legal in the legal and heat their END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 3, training examples: 0 - 2578\n",
            "Epoch 1/1\n",
            "36329/36329 [==============================] - 86s 2ms/step - loss: 2.2305\n",
            "STA i am sayinggg for the spurs fan for the team . END . END is gonna be bad END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END the \n",
            "Training epoch: 3, training examples: 2578 - 5156\n",
            "Epoch 1/1\n",
            "37537/37537 [==============================] - 89s 2ms/step - loss: 2.2604\n",
            "STA i am not feelin class END END END END i have not played in the league . i appreciate you like that END END END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 4, training examples: 0 - 2578\n",
            "Epoch 1/1\n",
            "36329/36329 [==============================] - 87s 2ms/step - loss: 2.1843\n",
            "STA i am sayinggg jealous END END END END are not gonna be back . END END END END END END END END END END END END END END END END END END END END and the \n",
            "Training epoch: 4, training examples: 2578 - 5156\n",
            "Epoch 1/1\n",
            "37537/37537 [==============================] - 89s 2ms/step - loss: 2.2078\n",
            "STA i am not feelin class for a boat END END END END END i have not seen this year END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 5, training examples: 0 - 2578\n",
            "Epoch 1/1\n",
            "36329/36329 [==============================] - 86s 2ms/step - loss: 2.1371\n",
            "STA i am sayinggg jealous END END END are not gonna be able to be a conter because they are always considered . END END END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 5, training examples: 2578 - 5156\n",
            "Epoch 1/1\n",
            "37537/37537 [==============================] - 89s 2ms/step - loss: 2.1579\n",
            "STA i am not feelin class END END END i have not played in the league END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 6, training examples: 0 - 2578\n",
            "Epoch 1/1\n",
            "36329/36329 [==============================] - 87s 2ms/step - loss: 2.0876\n",
            "STA i am sayinggg jealous END END END END END i have been set in the middle of the league ? END ? END END END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 6, training examples: 2578 - 5156\n",
            "Epoch 1/1\n",
            "37537/37537 [==============================] - 90s 2ms/step - loss: 2.1098\n",
            "STA i am not feelin class END END END END i have not feeling in the same END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 7, training examples: 0 - 2578\n",
            "Epoch 1/1\n",
            "36329/36329 [==============================] - 86s 2ms/step - loss: 2.0462\n",
            "STA i am so busy rn tho d : END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 7, training examples: 2578 - 5156\n",
            "Epoch 1/1\n",
            "37537/37537 [==============================] - 89s 2ms/step - loss: 2.0609\n",
            "STA i am not feelin class END END END but i am not sure they are not boring END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 8, training examples: 0 - 2578\n",
            "Epoch 1/1\n",
            "36329/36329 [==============================] - 86s 2ms/step - loss: 1.9989\n",
            "STA i am sayinggg for the middle of the wrestler . . . END . END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 8, training examples: 2578 - 5156\n",
            "Epoch 1/1\n",
            "37537/37537 [==============================] - 89s 2ms/step - loss: 2.0140\n",
            "STA i am not feelin class tomorrow rn END END END i have not been in the middle of my emo END END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 9, training examples: 0 - 2578\n",
            "Epoch 1/1\n",
            "36329/36329 [==============================] - 86s 2ms/step - loss: 1.9565\n",
            "STA i do not know what i am talking about the is the same of the ? END ? END ? END END #sorandom END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 9, training examples: 2578 - 5156\n",
            "Epoch 1/1\n",
            "37537/37537 [==============================] - 89s 2ms/step - loss: 1.9682\n",
            "STA i am not feelin class tomorrow rn but i am not sure it is a little little better END END END END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 10, training examples: 0 - 2578\n",
            "Epoch 1/1\n",
            "36329/36329 [==============================] - 86s 2ms/step - loss: 1.9108\n",
            "STA i do not know what i am going to be organized sports . END . END END END . END . END ? END ? END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 10, training examples: 2578 - 5156\n",
            "Epoch 1/1\n",
            "37537/37537 [==============================] - 89s 2ms/step - loss: 1.9219\n",
            "STA i am not feelin class for a boat END END END END i have not seen in the same END END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 11, training examples: 0 - 2578\n",
            "Epoch 1/1\n",
            "36329/36329 [==============================] - 86s 2ms/step - loss: 1.8687\n",
            "STA i am sayinggg for the stupid shit END END END END END i have been thinking thinking about the film as the guy . END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 11, training examples: 2578 - 5156\n",
            "Epoch 1/1\n",
            "37537/37537 [==============================] - 89s 2ms/step - loss: 1.8788\n",
            "STA i am not feelin class END END END but i am not sure they are not boring END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 12, training examples: 0 - 2578\n",
            "Epoch 1/1\n",
            "36329/36329 [==============================] - 86s 2ms/step - loss: 1.8237\n",
            "STA i am sorry for you . i cannot imagine it is a great day ! END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END , \n",
            "Training epoch: 12, training examples: 2578 - 5156\n",
            "Epoch 1/1\n",
            "37537/37537 [==============================] - 89s 2ms/step - loss: 1.8381\n",
            "STA i am just trying to live with my rn but i am not sure it is a lot of money for me END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 13, training examples: 0 - 2578\n",
            "Epoch 1/1\n",
            "36329/36329 [==============================] - 86s 2ms/step - loss: 1.7841\n",
            "STA i am sayinggg for the middle of the wrestler is wrestler . END . END END END END END END END END END END END END END END END END END END END END END END END END END END END END END . . . . END neato \n",
            "Training epoch: 13, training examples: 2578 - 5156\n",
            "Epoch 1/1\n",
            "37537/37537 [==============================] - 89s 2ms/step - loss: 1.7958\n",
            "STA i am not feelin class tomorrow rn but i am not sure it is a little good END END END END END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 14, training examples: 0 - 2578\n",
            "Epoch 1/1\n",
            "36329/36329 [==============================] - 86s 2ms/step - loss: 1.7478\n",
            "STA i do not know what the mouse was thinking ? ? ? ? ? ? ? ? END ? END ? END ? END ? END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 14, training examples: 2578 - 5156\n",
            "Epoch 1/1\n",
            "37537/37537 [==============================] - 89s 2ms/step - loss: 1.7529\n",
            "STA i am just trying to convince you and just laying out of my hair , i just want to go . END END END END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 15, training examples: 0 - 2578\n",
            "Epoch 1/1\n",
            "36329/36329 [==============================] - 86s 2ms/step - loss: 1.7059\n",
            "STA i do not know what the mouse was thinking ? ? why come into a house with 2 ferocious felines ? ? END END END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 15, training examples: 2578 - 5156\n",
            "Epoch 1/1\n",
            "37537/37537 [==============================] - 89s 2ms/step - loss: 1.7109\n",
            "STA i am not feelin class END END END but i am not sure they are not boring END END END END END END END END END END END END END END END END END END END END END END END END END END END and the END \n",
            "Training epoch: 16, training examples: 0 - 2578\n",
            "Epoch 1/1\n",
            "36329/36329 [==============================] - 86s 2ms/step - loss: 1.6652\n",
            "STA i am sure what is a lot of is being a END END END END END END like a different of END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 16, training examples: 2578 - 5156\n",
            "Epoch 1/1\n",
            "37537/37537 [==============================] - 89s 2ms/step - loss: 1.6711\n",
            "STA i am not feelin class tomorrow rn but i am not sure i am not sure END END END END END END END END END END END END END END END END END END END END END END END END END and END END and \n",
            "Training epoch: 17, training examples: 0 - 2578\n",
            "Epoch 1/1\n",
            "36329/36329 [==============================] - 86s 2ms/step - loss: 1.6267\n",
            "STA i do not know what i am an . but i am not sure what is working out of the is not actually END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 17, training examples: 2578 - 5156\n",
            "Epoch 1/1\n",
            "37537/37537 [==============================] - 89s 2ms/step - loss: 1.6350\n",
            "STA i am not feelin class tomorrow rn but i am not sure i am not a sports END END END END END END END END END END END END END END END END END END END END END END END END END END END and the reason they have \n",
            "Training epoch: 18, training examples: 0 - 2578\n",
            "Epoch 1/1\n",
            "36329/36329 [==============================] - 86s 2ms/step - loss: 1.5861\n",
            "STA i am sayinggg but i am not a spurs fan for my team . he is finally getting the best . i am very optimistic ! ! END ! END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 18, training examples: 2578 - 5156\n",
            "Epoch 1/1\n",
            "37537/37537 [==============================] - 89s 2ms/step - loss: 1.5905\n",
            "STA i am not feelin class tomorrow rn but i am not sure it is a little END END END END END END END END END END END END END END END END END END END END END END END END END END and the END END \n",
            "Training epoch: 19, training examples: 0 - 2578\n",
            "Epoch 1/1\n",
            "36329/36329 [==============================] - 86s 2ms/step - loss: 1.5501\n",
            "STA i do not know what the mouse was thinking ? ? ? why come into a house with 2 ferocious felines ? ? END ? END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 19, training examples: 2578 - 5156\n",
            "Epoch 1/1\n",
            "37537/37537 [==============================] - 89s 2ms/step - loss: 1.5496\n",
            "STA i am not feelin class tomorrow rn but i am not sure it is a little little good END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END and \n",
            "Training epoch: 20, training examples: 0 - 2578\n",
            "Epoch 1/1\n",
            "36329/36329 [==============================] - 86s 2ms/step - loss: 1.5128\n",
            "STA i am sayinggg for the stupid shit END END END as well i have in the in the middle parking lot END END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 20, training examples: 2578 - 5156\n",
            "Epoch 1/1\n",
            "37537/37537 [==============================] - 89s 2ms/step - loss: 1.5165\n",
            "STA i am not feelin class tomorrow rn but i definitely got it in my head and my family is over my family is so hard to get . END it is not a . END END END END END END END END END END END END END END \n",
            "Training epoch: 21, training examples: 0 - 2578\n",
            "Epoch 1/1\n",
            "36329/36329 [==============================] - 86s 2ms/step - loss: 1.4740\n",
            "STA i do not know what the mouse was thinking ? ? why come into a house with 2 ferocious felines ? ? END END END END END END END END END END END END END END END END END END END END END END END END END END . \n",
            "Training epoch: 21, training examples: 2578 - 5156\n",
            "Epoch 1/1\n",
            "37537/37537 [==============================] - 89s 2ms/step - loss: 1.4758\n",
            "STA i am not feelin class tomorrow rn but i have tattoos over my collection END END w my and have to have in my and END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 22, training examples: 0 - 2578\n",
            "Epoch 1/1\n",
            "36329/36329 [==============================] - 86s 2ms/step - loss: 1.4381\n",
            "STA i am the same of wrestler i do not even want to do the on much . i am not sure . END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 22, training examples: 2578 - 5156\n",
            "Epoch 1/1\n",
            "37537/37537 [==============================] - 89s 2ms/step - loss: 1.4443\n",
            "STA i am not feelin class tomorrow rn but i definitely do not have a huge piece of money in my head END END END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 23, training examples: 0 - 2578\n",
            "Epoch 1/1\n",
            "36329/36329 [==============================] - 86s 2ms/step - loss: 1.4050\n",
            "STA i am sure ! ! ! ! ! END ! END ! END END END END END END END END END END you know ? END END END END END END END END END END END END END END END END END END END END END a \n",
            "Training epoch: 23, training examples: 2578 - 5156\n",
            "Epoch 1/1\n",
            "37537/37537 [==============================] - 89s 2ms/step - loss: 1.4088\n",
            "STA i am not yet sleepy . END are not a single shred of money in your , i have no 2 2 weeks END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 24, training examples: 0 - 2578\n",
            "Epoch 1/1\n",
            "36329/36329 [==============================] - 86s 2ms/step - loss: 1.3738\n",
            "STA i am so busy rn tho d : END END END END END ? END ? END END END END END END END END END END END END END END END END . . . . END neato END END . END . END . END END END END \n",
            "Training epoch: 24, training examples: 2578 - 5156\n",
            "Epoch 1/1\n",
            "37537/37537 [==============================] - 89s 2ms/step - loss: 1.3750\n",
            "STA i am not feelin class tomorrow rn but i definitely have not to be there END END END END END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 25, training examples: 0 - 2578\n",
            "Epoch 1/1\n",
            "36329/36329 [==============================] - 86s 2ms/step - loss: 1.3408\n",
            "STA i do not know what the mouse was thinking ? ? why come into a house with 2 ferocious felines ? ? END END END END END END END END END END END END END END END END END END END END END END END END END END the \n",
            "Training epoch: 25, training examples: 2578 - 5156\n",
            "Epoch 1/1\n",
            "37537/37537 [==============================] - 89s 2ms/step - loss: 1.3421\n",
            "STA i am not feelin class tomorrow rn but i have tattoos it END END END END END END END END END END END END END END END END END END END END i am not sure it is a . not . END END END END \n",
            "Training epoch: 26, training examples: 0 - 2578\n",
            "Epoch 1/1\n",
            "36329/36329 [==============================] - 86s 2ms/step - loss: 1.3095\n",
            "STA i am just trying to live with the ppl and i am not sure for whatever players for anything ! i am very very excited ! END END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 26, training examples: 2578 - 5156\n",
            "Epoch 1/1\n",
            "37537/37537 [==============================] - 89s 2ms/step - loss: 1.3087\n",
            "STA i am not feelin class tomorrow rn but i definitely got it in my head and my mom probably not even want to do . END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 27, training examples: 0 - 2578\n",
            "Epoch 1/1\n",
            "36329/36329 [==============================] - 86s 2ms/step - loss: 1.2791\n",
            "STA i am so busy rn tho d : END END END END END . END END END END END END END END END END END END END i can not wait til the price is wedding . you should be a END END END . END END \n",
            "Training epoch: 27, training examples: 2578 - 5156\n",
            "Epoch 1/1\n",
            "37537/37537 [==============================] - 89s 2ms/step - loss: 1.2764\n",
            "STA i am not trying to convince you out of us at this damn . i am going to my schedule . END END END END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 28, training examples: 0 - 2578\n",
            "Epoch 1/1\n",
            "36329/36329 [==============================] - 86s 2ms/step - loss: 1.2402\n",
            "STA i do not know what the mouse was thinking ? ? why come into a house with 2 ferocious felines ? ? END END END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 28, training examples: 2578 - 5156\n",
            "Epoch 1/1\n",
            "37537/37537 [==============================] - 89s 2ms/step - loss: 1.2424\n",
            "STA i am just trying to convince you and go out at work at my work , i do not have to talk about any money , but i am not END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 29, training examples: 0 - 2578\n",
            "Epoch 1/1\n",
            "36329/36329 [==============================] - 86s 2ms/step - loss: 1.2076\n",
            "STA i am sorry for you . i cannot imagine ! ! ! ! ! ! ! ! ! END ! ! END ! END ! END ! END ! END END END END END END END END END END END END END ! ! ! ! ! ! ? \n",
            "Training epoch: 29, training examples: 2578 - 5156\n",
            "Epoch 1/1\n",
            "37537/37537 [==============================] - 89s 2ms/step - loss: 1.2067\n",
            "STA i am not feelin class tomorrow rn but i definitely have not to be there END END END END END END END END END END END END END END END END END END END END END END END END END and the is good enough , even be \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "r8-qiXo64VkH",
        "colab_type": "code",
        "outputId": "123a82e8-5533-48ba-8e2d-fe93b8226cdd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1717
        }
      },
      "cell_type": "code",
      "source": [
        "for i in range(50):\n",
        "  print('input: %s'%context[i].replace('STA','').replace('END',''))\n",
        "  output = print_result(qt[i:i+1])\n",
        "  print('output: %s'%output)\n",
        "  i += 1\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input:  what is up dadyo when did you get back on twitter ? haha \n",
            "output: STA i am not a spurs fan . END him &amp ; kawhi are so so well i have to do END END END END END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "input:  literally never about that account , love it . \n",
            "output: STA i am not sure what is working out well . i am trying to do real shit out at the game . END END END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "input:  about 50 times today . terminal vim user . \n",
            "output: STA i am not sure what is working out of these keep those days . END is END END END END END END END END END END END END END END END END END END END END END END END END END END END END END ? END \n",
            "input:  cmd+opt+esc is good but still available via menubar \n",
            "output: STA i am not sure what is working out of those these days lol END i am ready to rt and go these day END END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "input:  i am disgusted \n",
            "output: STA END END END END END END END END END END END pharma END END END END END END END \n",
            "input:  what a piece of shit \n",
            "output: STA i am not a pretty asian girl like that i was an extra of i having having a good and it is good END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "input:  yay , you great hunter . ive killed lots of lizards and bugs but never a mouse . \n",
            "output: STA i am not feelin class tomorrow rn but i definitely have not to be there END END END END END END END END END END END END END END END END END END END END END END END END END and the is good enough , even be \n",
            "input:  and then that mouse had the nerve to try to eat our kibble !  let this be a lesson fur all the other mousies !   \n",
            "output: STA i am sorry to hear you END END END END END and i do not see you guys of it END END END END END END END END END END END END END END END END END END END END END END END and END and \n",
            "input:  tomorrow \n",
            "output: STA i am not ready to see you END END END END END END END END END END END END END END END END END END END END END and the and END END END END END END END END END \n",
            "input:  make sure i have a bed and seat saved next to you ! \n",
            "output: STA i am so sleepy END END END END i am gonna y home END END are not at us . END END END END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "input:  wassup shorty .  \n",
            "output: STA i am not even END END but i thought it was gonna have a good genuine fan and a new team . END END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "input:  appreciate that shorty , you too .  \n",
            "output: STA i am not sure what is working out well . i am barely gonna try it again again ! ! END ;3 END END END END END END END . END END END END END END END END END END END END END END END END END END END \n",
            "input:  yea \n",
            "output: STA END END END END END END END END END END pharma getting a good END END END END END END END END END END END END END END END END \n",
            "input:  gotchu \n",
            "output: STA END END END END END END END END END END pharma getting a good END END END END END END END END END END END END END END END END END \n",
            "input:  good  wby \n",
            "output: STA i am just kidding it END END END . i am not even have a good sleep and a chat and real life END END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "input:  that is wassup  \n",
            "output: STA i am not even this END END . i am not sure it might be at the first . END END END END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "input:  and the dash for cash races too \n",
            "output: STA i am not sure what is working out of those these days lol END END END END END END END END END END END END END END END END END END END . END ; . END END END ; END END END END \n",
            "input:  do you think this wouldve happened anyway if it was another driver stinkin it up like kyle has recently ? \n",
            "output: STA i am not yet sleepy . END are not a single shred of us . they follow it , its good . its a little story . END END END END END END END END END END END END END END END END END END END END END END \n",
            "input:  what did these niggas say \n",
            "output: STA i am not even this . it is a of . it is a but it is all of ya will not . END END END END END END END END END END END END END END END END END END END END END END \n",
            "input:  kenny is wife is white on top of that . u not gonna tell me u put a flag over the color of your skin . \n",
            "output: STA i am not ready to see it . . . here here , but the same part of the same as black people END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "input:  u cute now \n",
            "output: STA i am not even END END but i thought it was gonna have a good one and a good genuine END END END END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "input:  any hot chicks wanna let me touch their but today ? \n",
            "output: STA i am so happy for you ! i am not sure ! ! ! ! ! ! ! ! ! END ! ! END ! END ! END ! END END my little good made my love love END END END END END END \n",
            "input:  i havent groped that ass in a while i need dat \n",
            "output: STA i am so sleepy END END END you are not better lol END you deserve it lol END you deserve it lol END END END END END END END END END END END END getting the rest of course END END END END END END END END END END \n",
            "input:  so much to do before leaving for d .c . ahhh \n",
            "output: STA i am not sure what is the END . i am gonna try to rt again again again again . END END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "input:   ? ? ? \n",
            "output: STA i am not a spurs fan END END END but kawhi was like this one of the show END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "input:  white girls singing about \"niggers\" stealing should not have had to face any repercussions ?  . . . . . \n",
            "output: STA i am not ready for your response . i am willing to pick up xbox stuff END END END END END END END END END END END END END END END END END END END END END END END END END END END , is \n",
            "input:  they are not 5 year olds . them repeating what they heard is not an excuse . it does not matter if there was malicious intent . \n",
            "output: STA mann it is not that it ? or or a ? END or a ? END or ? END END END END END END END END END END END END END END END END END END END END END END END END END END END : critique \n",
            "input:  okay so i know we have had our differences on pineapples on pizza in the past but there is something else that needs attention . . . eggnog \n",
            "output: STA i hear about it again ! END END to me . END . END . END . END is and and on the and END END END END END END END END END END END END END END END END END END END END \n",
            "input:  what in the tits is a coquito \n",
            "output: STA i am not sure END END END END but i am gonna try it it again again END END END END END END END . END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "input:  just riding these ol dirt roads . \n",
            "output: STA i am not a pretty asian girl like that i just can not walk END END END END END END END END END with her ? END END END END END END END END END END END END END END END END END END END END END END END \n",
            "input:  k babe . \n",
            "output: STA i am not even this END END . i am not sure it might be at the first . END END END END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "input:  hate how y'all try to call me \"fake gay\" no . . trust &amp ; believe vagina is wonderful to me , but so is your father . *hint hint* idiots \n",
            "output: STA i am on to text the . god gave that that . will . . ? END ? END END END END END END END END END END END END END END END END END END END END . END \n",
            "input:  but just plain dinner is lame . . i like to have something sweet sometimes to . they get to eat what they want , why i can not ? \n",
            "output: STA i am gonna try to rt now but i think i am a great person and a little life about it is when i do not END END END END END END END END END END END END END END END END END END END END END END END \n",
            "input:  just one ? \n",
            "output: STA i am not even pissed END END - i was working lol END you were to work lol END END END END END END END i will not END END END END END END END END END END END END END END END END END END END END END \n",
            "input:  yeah !  that sounds like the tracy in my timeline .  \n",
            "output: STA i am so happy for the boat out END END END END END END END - i have not seen . END END END END END END END END END END END END END END END END END END END END END END END END END ? END \n",
            "input:  dont you all het tired calling half of the country racist ? \n",
            "output: STA i am stuck between wanting to cry . i am not a great one day and a little one that is good . END END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "input:  yes . good night . \n",
            "output: STA i am not even END END . i am gonna die but i am not sure END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "input:  my selfies got bad lighting ? lol \n",
            "output: STA i am not a pretty asian girl like you that just walk in END END END END END END END END END but i am not even pissed END END END END END END END END END END END END END END END END END END END END END \n",
            "input:  rightttttt lmfao \n",
            "output: STA END END END END END END END END END END pharma getting a good END END END END END END END END END END END END END END END END END \n",
            "input:  yeah i do \n",
            "output: STA i am not even ok END END that i can not say it . . . . . END i always so ok lol END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "input:  yeah there should be room . usually the desks behind where i sit . \n",
            "output: STA i sorta feel like i have made a huge mike END END END END is fine END END END . not only as many of my emo END END END END END END END END END END END END END END END END END END END END END END \n",
            "input:  i will beat your ass  \n",
            "output: STA i am not even ok END END END i have not been in this END END END END END END i am not even do it END END END END END END END END END END END END END END END END END END END END END END END \n",
            "input:  i am near the gym fight me pussy \n",
            "output: STA i am so happy for you ! i am not a great day END END END END END END END END END END END END you are END END END END END END END END END END END END END END END END END END END END END END \n",
            "input:  alright kiddo . 8 hours . \n",
            "output: STA i am not a spurs fan END END but i have not played this like i had a good one of his weapons END END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "input:  i bought you a fudge brownie . it counts as cake . \n",
            "output: STA i am so sad . i am not sure END END END END END END i have not always have in the same . END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "input:  i see \n",
            "output: STA i am not even ok END END END so i can not rt it . . i am a a great fan END END END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "input:  sooo . . . . did everyone on uapb campus tv is loose signal ? \n",
            "output: STA i am not sure what is the is a little END . END does it does not have a good option ? END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "input:  well idk but mine did \n",
            "output: STA i am not a spurs fan fan . like kawhi him of my but i do not want to it again END END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "input:  will you be at the phi delts friday ? ? ? \n",
            "output: STA i am so happy for the boat out END END END END END END END but i have not even in the same . END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "input:  yes it is and yes i am !  \n",
            "output: STA i am not rich END END END END i am END but i am not sure we can manage to do it . END is a better thing END END END END END END END END END END END END END END END END END END END \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "oBZQN24yfCka",
        "colab_type": "code",
        "outputId": "e69e0dd6-d2d2-4356-9607-29db30ff6918",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.layers import Input, Embedding, LSTM, Dense, RepeatVector, Bidirectional, Dropout, concatenate\n",
        "from keras.optimizers import Adam, SGD\n",
        "from keras.models import Model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Activation, Dense\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.preprocessing import sequence\n",
        "import keras.backend as K\n",
        "import os\n",
        "import theano.tensor as T\n",
        "import os\n",
        "import pandas as pd\n",
        "import sys\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "weights_file = 'assignment4_twitter.h5'\n",
        "ad = Adam(lr=0.00005) \n",
        "\n",
        "input_context = Input(shape=(maxlen_input,), name='input_context')\n",
        "input_answer = Input(shape=(maxlen_input,), name='input_answer')\n",
        "LSTM_encoder = LSTM(sentence_embedding_size, init= 'lecun_uniform')\n",
        "LSTM_decoder = LSTM(sentence_embedding_size, init= 'lecun_uniform')\n",
        "if os.path.isfile(weights_file):\n",
        "    Shared_Embedding = Embedding(output_dim=word_embedding_size, input_dim=dictionary_size, input_length=maxlen_input)\n",
        "else:\n",
        "    Shared_Embedding = Embedding(output_dim=word_embedding_size, input_dim=dictionary_size, weights=[embedding_matrix], input_length=maxlen_input)\n",
        "word_embedding_context = Shared_Embedding(input_context)\n",
        "context_embedding = LSTM_encoder(word_embedding_context)\n",
        "\n",
        "word_embedding_answer = Shared_Embedding(input_answer)\n",
        "answer_embedding = LSTM_decoder(word_embedding_answer)\n",
        "\n",
        "merge_layer = concatenate([context_embedding, answer_embedding])\n",
        "out = Dense(int(dictionary_size/2), activation=\"relu\")(merge_layer)\n",
        "out = Dense(dictionary_size, activation=\"softmax\")(out)\n",
        "\n",
        "model = Model(input=[input_context, input_answer], output = [out])\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer=ad)\n",
        "\n",
        "if os.path.isfile(weights_file):\n",
        "    model.load_weights(weights_file)\n",
        "    \n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:21: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(300, kernel_initializer=\"lecun_uniform\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:22: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(300, kernel_initializer=\"lecun_uniform\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:37: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_context (InputLayer)      (None, 50)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_answer (InputLayer)       (None, 50)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_3 (Embedding)         (None, 50, 100)      1000000     input_context[0][0]              \n",
            "                                                                 input_answer[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "lstm_5 (LSTM)                   (None, 300)          481200      embedding_3[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "lstm_6 (LSTM)                   (None, 300)          481200      embedding_3[1][0]                \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_3 (Concatenate)     (None, 600)          0           lstm_5[0][0]                     \n",
            "                                                                 lstm_6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense_5 (Dense)                 (None, 5000)         3005000     concatenate_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_6 (Dense)                 (None, 10000)        50010000    dense_5[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 54,977,400\n",
            "Trainable params: 54,977,400\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "z4U9tyctff1x",
        "colab_type": "code",
        "outputId": "e642b4b2-a4ba-4bb5-d19a-392c11695b9d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4069
        }
      },
      "cell_type": "code",
      "source": [
        "x = range(0,Epochs) \n",
        "valid_loss = np.zeros(Epochs)\n",
        "train_loss = np.zeros(Epochs)\n",
        "for m in range(Epochs):\n",
        "  # Loop over training batches due to memory constraints:\n",
        "  for n in range(0,round_exem,step):\n",
        "    q2 = q[n:n+step]\n",
        "    s = q2.shape\n",
        "    count = 0\n",
        "    for i, sent in enumerate(a[n:n+step]):\n",
        "      l = np.where(sent==1)\n",
        "      limit = l[0][0]\n",
        "      count += limit + 1\n",
        "    Q = np.zeros((count,maxlen_input))\n",
        "    A = np.zeros((count,maxlen_input))\n",
        "    Y = np.zeros((count,dictionary_size))\n",
        "    \n",
        "    count = 0\n",
        "    for i, sent in enumerate(a[n:n+step]):\n",
        "      ans_partial = np.zeros((1,maxlen_input))\n",
        "      # Loop over the positions of the current target output (the current output sequence):\n",
        "      l = np.where(sent==1)\n",
        "      limit = l[0][0]\n",
        "      for k in range(1,limit+1):\n",
        "        # Mapping the target output (the next output word) for one-hot codding:\n",
        "        y = np.zeros((1, dictionary_size))\n",
        "        y[0, sent[k]] = 1\n",
        "        # preparing the partial answer to input:\n",
        "        ans_partial[0,-k:] = sent[0:k]\n",
        "        # training the model for one epoch using teacher forcing:\n",
        "        Q[count, :] = q2[i:i+1] \n",
        "        A[count, :] = ans_partial \n",
        "        Y[count, :] = y\n",
        "        count += 1\n",
        "    print('Training epoch: %d, training examples: %d - %d'%(m,n, n + step))\n",
        "    model.fit([Q, A], Y, batch_size=BatchSize, epochs=1)\n",
        "    test_input = qt[6:7]\n",
        "    print(print_result(test_input))\n",
        "  model.save_weights(weights_file, overwrite=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training epoch: 0, training examples: 0 - 2578\n",
            "Epoch 1/1\n",
            "36329/36329 [==============================] - 89s 2ms/step - loss: 1.2059\n",
            "STA i do not know what the mouse was thinking ? ? why come into a house with 2 ferocious felines ? ? END END END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 0, training examples: 2578 - 5156\n",
            "Epoch 1/1\n",
            "37537/37537 [==============================] - 89s 2ms/step - loss: 1.2007\n",
            "STA i am not feelin class tomorrow rn but i definitely do not have a huge deal with my collection END END END END END END END END END END END END END END END END END END END END END END END END END END END END END . \n",
            "Training epoch: 1, training examples: 0 - 2578\n",
            "Epoch 1/1\n",
            "36329/36329 [==============================] - 86s 2ms/step - loss: 1.1445\n",
            "STA i do not know what the mouse was thinking ? ? why come into a house with 2 ferocious felines ? ? END END END END END END END END END END END END END END END END END END END END END END END END END END . \n",
            "Training epoch: 1, training examples: 2578 - 5156\n",
            "Epoch 1/1\n",
            "37537/37537 [==============================] - 89s 2ms/step - loss: 1.1507\n",
            "STA i am sorry , but i am glad you , i am just not feeling with you , i am off , i am off , but i am just right now . END i can not help END END END END END END END END END END END \n",
            "Training epoch: 2, training examples: 0 - 2578\n",
            "Epoch 1/1\n",
            "36329/36329 [==============================] - 86s 2ms/step - loss: 1.1118\n",
            "STA i do not know what the mouse was thinking ? ? why come into a house with 2 ferocious felines ? ? END END END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 2, training examples: 2578 - 5156\n",
            "Epoch 1/1\n",
            "37537/37537 [==============================] - 89s 2ms/step - loss: 1.1191\n",
            "STA i am not feelin class tomorrow rn but i definitely will not be feeling there in the morning END END END END END END END END END END END END END END END END END END END END END END END END ? ? ? ? END it is \n",
            "Training epoch: 3, training examples: 0 - 2578\n",
            "Epoch 1/1\n",
            "36329/36329 [==============================] - 86s 2ms/step - loss: 1.0830\n",
            "STA i do not know what i am going to be organized sports . i am gonna be a . END END END END END END END END END END END END END END END END END END END END END END END END END END END END . \n",
            "Training epoch: 3, training examples: 2578 - 5156\n",
            "Epoch 1/1\n",
            "37537/37537 [==============================] - 89s 2ms/step - loss: 1.0833\n",
            "STA i am not yet sleepy . END . END END END . i am not gonna y home , but i am willing to be at all . END . END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 4, training examples: 0 - 2578\n",
            "Epoch 1/1\n",
            "36329/36329 [==============================] - 86s 2ms/step - loss: 1.0535\n",
            "STA i do not know what the mouse was thinking ? ? why come into a house with 2 ferocious felines ? ? END END END END END END END END END END END END END END END END END END END END END END END END END END the \n",
            "Training epoch: 4, training examples: 2578 - 5156\n",
            "Epoch 1/1\n",
            "37537/37537 [==============================] - 89s 2ms/step - loss: 1.0567\n",
            "STA i am just taking the lab , i am just trying to get out at work at you , i am going , i am 72 . END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 5, training examples: 0 - 2578\n",
            "Epoch 1/1\n",
            "36329/36329 [==============================] - 86s 2ms/step - loss: 1.0307\n",
            "STA i do not know what the mouse was thinking ? ? why come into a house with 2 ferocious felines ? ? END END END END END END END END END END END END END END END END END END END END END END END END END . . \n",
            "Training epoch: 5, training examples: 2578 - 5156\n",
            "Epoch 1/1\n",
            "37537/37537 [==============================] - 89s 2ms/step - loss: 1.0300\n",
            "STA i am just taking the lab , i have actually just have to work in my head , i just my best life , i wish . i am not , but i could not much money END END END END END END END END END END END END \n",
            "Training epoch: 6, training examples: 0 - 2578\n",
            "Epoch 1/1\n",
            "36329/36329 [==============================] - 86s 2ms/step - loss: 0.9981\n",
            "STA i do not know what the mouse was thinking ? ? why come into a house with 2 ferocious felines ? ? END END END END END END END END END END END END END END END END END END END END END END END END END END . \n",
            "Training epoch: 6, training examples: 2578 - 5156\n",
            "Epoch 1/1\n",
            "37537/37537 [==============================] - 89s 2ms/step - loss: 1.0011\n",
            "STA i am just taking the lab , i have END END END END END i woke up , i am not living my head n ppl . END . END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 7, training examples: 0 - 2578\n",
            "Epoch 1/1\n",
            "36329/36329 [==============================] - 86s 2ms/step - loss: 0.9745\n",
            "STA i do not know what i was gonna gonna try it . but i am a little back to be back . END END END END END END END END END END END END END END END END END END END END END END END END . END . \n",
            "Training epoch: 7, training examples: 2578 - 5156\n",
            "Epoch 1/1\n",
            "37537/37537 [==============================] - 89s 2ms/step - loss: 0.9716\n",
            "STA i am not feelin class tomorrow rn but i definitely do not have a new days lol END END END END END END . END END END END END END END END END END END END END END END END END END END END END END END END ? \n",
            "Training epoch: 8, training examples: 0 - 2578\n",
            "Epoch 1/1\n",
            "36329/36329 [==============================] - 86s 2ms/step - loss: 0.9447\n",
            "STA i sat refreshing the page for the book ! ! ! ! END END END END END END END END END END END END END END the flight END . END ? END END END ; END END ; like cold . END END END END END END END \n",
            "Training epoch: 8, training examples: 2578 - 5156\n",
            "Epoch 1/1\n",
            "37537/37537 [==============================] - 89s 2ms/step - loss: 0.9455\n",
            "STA i am just taking the lab , i have made in ! END i am not going to do my work out lol END END END END END END END END END END END END END END END END END END . END END END END . END \n",
            "Training epoch: 9, training examples: 0 - 2578\n",
            "Epoch 1/1\n",
            "36329/36329 [==============================] - 86s 2ms/step - loss: 0.9231\n",
            "STA i do not know what the mouse was thinking ? ? why come into a house with 2 ferocious felines ? ? END END END END END END END END END END END END END END END END END END END END END END END END END . . \n",
            "Training epoch: 9, training examples: 2578 - 5156\n",
            "Epoch 1/1\n",
            "37537/37537 [==============================] - 89s 2ms/step - loss: 0.9200\n",
            "STA i am just trying to live END END END END , so i am off on my schedule for my bday . END END END END END END END END END END END END END END END END . END END END END END END . END END END \n",
            "Training epoch: 10, training examples: 0 - 2578\n",
            "Epoch 1/1\n",
            "36329/36329 [==============================] - 86s 2ms/step - loss: 0.8982\n",
            "STA i do not know what the mouse was thinking ? ? why come into a house with 2 ferocious felines ? ? END END END END END END END END END END END END END END END END END END END END END END END END END END the \n",
            "Training epoch: 10, training examples: 2578 - 5156\n",
            "Epoch 1/1\n",
            "37537/37537 [==============================] - 89s 2ms/step - loss: 0.8978\n",
            "STA i am not a spurs fan , but kawhi leonard is such a beast all-around guy . kd needs to be more all-around like him in gsw . END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 11, training examples: 0 - 2578\n",
            "Epoch 1/1\n",
            "36329/36329 [==============================] - 86s 2ms/step - loss: 0.8717\n",
            "STA i do not know what the mouse was thinking ? ? why come into a house with 2 ferocious felines ? ? END END END END END END END END END END END END END END END END END END END END END END END ? ? ? ? \n",
            "Training epoch: 11, training examples: 2578 - 5156\n",
            "Epoch 1/1\n",
            "37537/37537 [==============================] - 89s 2ms/step - loss: 0.8697\n",
            "STA i am not feelin class tomorrow rn but i definitely will not be feeling there in the morning END END END END END END END END END END END END END END END END END END END END END and the END END END END . END is \n",
            "Training epoch: 12, training examples: 0 - 2578\n",
            "Epoch 1/1\n",
            "36329/36329 [==============================] - 86s 2ms/step - loss: 0.8493\n",
            "STA i do not know what the mouse was thinking ? ? why come into a house with 2 ferocious felines ? ? END END END END END END END END END END END END END END END END END END END END END END END END END i do \n",
            "Training epoch: 12, training examples: 2578 - 5156\n",
            "Epoch 1/1\n",
            "37537/37537 [==============================] - 89s 2ms/step - loss: 0.8451\n",
            "STA i am sure , i am glad you will not END END END END END too lol END too i have tattoos END END END END END END END END END END END END END END END END END END END END END END END END END and the \n",
            "Training epoch: 13, training examples: 0 - 2578\n",
            "Epoch 1/1\n",
            "36329/36329 [==============================] - 86s 2ms/step - loss: 0.8274\n",
            "STA i do not know what the mouse was thinking ? ? why come into a house with 2 ferocious felines ? ? END END END END END END END END END END END END END END END END END END END END END END END END END ? ? \n",
            "Training epoch: 13, training examples: 2578 - 5156\n",
            "Epoch 1/1\n",
            "37537/37537 [==============================] - 89s 2ms/step - loss: 0.8264\n",
            "STA i am not trying to talk about pig . lol END that is just sold and you are on my own ! END END END END END END END END END END END END END END END END END END END END END END END END . END END \n",
            "Training epoch: 14, training examples: 0 - 2578\n",
            "Epoch 1/1\n",
            "36329/36329 [==============================] - 86s 2ms/step - loss: 0.8015\n",
            "STA i do not know what the mouse was thinking ? ? why come into a house with 2 ferocious felines ? ? END END END END END END END END END END END END END END END END END END END END END END END END . END are \n",
            "Training epoch: 14, training examples: 2578 - 5156\n",
            "Epoch 1/1\n",
            "37537/37537 [==============================] - 89s 2ms/step - loss: 0.8037\n",
            "STA i am just taking the lab , i have END END END END i woke up my last night , its shit is not my head . END END END END . END END END END END END END END END END END END END END END END \n",
            "Training epoch: 15, training examples: 0 - 2578\n",
            "Epoch 1/1\n",
            "36329/36329 [==============================] - 86s 2ms/step - loss: 0.7859\n",
            "STA i do not know what the mouse was thinking ? ? why come into a house with 2 ferocious felines ? ? END END END END END END END END END END END END END END END END END END END END END END END END END the dem \n",
            "Training epoch: 15, training examples: 2578 - 5156\n",
            "Epoch 1/1\n",
            "37537/37537 [==============================] - 89s 2ms/step - loss: 0.7799\n",
            "STA i am just taking the lab , i have END END END END ! ! ! i am glad for you END END END END END END END END END END END END END END END END END END END END END END END END END and the \n",
            "Training epoch: 16, training examples: 0 - 2578\n",
            "Epoch 1/1\n",
            "36329/36329 [==============================] - 86s 2ms/step - loss: 0.7635\n",
            "STA i do not know what the mouse was thinking ? ? why come into a house with 2 ferocious felines ? ? END END END END END END END END END END END END END END END END END END END END END END END END ? ? ? \n",
            "Training epoch: 16, training examples: 2578 - 5156\n",
            "Epoch 1/1\n",
            "37537/37537 [==============================] - 89s 2ms/step - loss: 0.7507\n",
            "STA i am not a spurs fan , but kawhi leonard is such a beast all-around guy . kd needs to be more all-around like him in gsw . END . END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 17, training examples: 0 - 2578\n",
            "Epoch 1/1\n",
            "36329/36329 [==============================] - 86s 2ms/step - loss: 0.7453\n",
            "STA i do not know what the mouse was thinking ? ? why come into a house with 2 ferocious felines ? ? END END END END END END END END END END END END END END END END END END END END END END END ? ? ? END \n",
            "Training epoch: 17, training examples: 2578 - 5156\n",
            "Epoch 1/1\n",
            "37537/37537 [==============================] - 89s 2ms/step - loss: 0.7291\n",
            "STA i am just taking the lab , i have ! END ! ! ! END i am glad to go . but i think is could be cleared too END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 18, training examples: 0 - 2578\n",
            "Epoch 1/1\n",
            "36329/36329 [==============================] - 86s 2ms/step - loss: 0.7245\n",
            "STA i do not know what the mouse was thinking ? ? why come into a house with 2 ferocious felines ? ? END END END END END END END END END END END END END END END END END END END END END END END ? ? ? ? \n",
            "Training epoch: 18, training examples: 2578 - 5156\n",
            "Epoch 1/1\n",
            "37537/37537 [==============================] - 89s 2ms/step - loss: 0.7144\n",
            "STA i am not yet sleepy . . hahaha . i am still on my head waiting . END it was out at the night . END END END END END END END END END END END END END END END END END END END END END END . END \n",
            "Training epoch: 19, training examples: 0 - 2578\n",
            "Epoch 1/1\n",
            "36329/36329 [==============================] - 86s 2ms/step - loss: 0.7039\n",
            "STA i do not know what the mouse was thinking ? ? why come into a house with 2 ferocious felines ? ? END END END END END END END END END END END END END END END END END END END END END END END ? ? ? END \n",
            "Training epoch: 19, training examples: 2578 - 5156\n",
            "Epoch 1/1\n",
            "37537/37537 [==============================] - 89s 2ms/step - loss: 0.6948\n",
            "STA i am just taking the lab , i have END END END END END i woke up my last night , now too it is not way at the door END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 20, training examples: 0 - 2578\n",
            "Epoch 1/1\n",
            "36329/36329 [==============================] - 86s 2ms/step - loss: 0.6889\n",
            "STA i do not know what the mouse was thinking ? ? why come into a house with 2 ferocious felines ? ? END END END END END END END END END END END END END END END END END END END END END END END ? ? ? ? \n",
            "Training epoch: 20, training examples: 2578 - 5156\n",
            "Epoch 1/1\n",
            "37537/37537 [==============================] - 89s 2ms/step - loss: 0.6715\n",
            "STA i am not trying to talk about pig . lol END that is just sold and END END END END END END END END END END END END END END END END END . END costs a , END END . END END END END END \n",
            "Training epoch: 21, training examples: 0 - 2578\n",
            "Epoch 1/1\n",
            "36329/36329 [==============================] - 86s 2ms/step - loss: 0.6640\n",
            "STA i do not know what the mouse was thinking ? ? why come into a house with 2 ferocious felines ? ? END END END END END END END END END END END END END END END END END END END END END END END ? ? ? ? \n",
            "Training epoch: 21, training examples: 2578 - 5156\n",
            "Epoch 1/1\n",
            "37537/37537 [==============================] - 89s 2ms/step - loss: 0.6530\n",
            "STA i am not trying to convince you and go to pick up for having a place for 2 . that already END END END END END END END END END END END END END END END END END END END END END END END END END END END and \n",
            "Training epoch: 22, training examples: 0 - 2578\n",
            "Epoch 1/1\n",
            "36329/36329 [==============================] - 86s 2ms/step - loss: 0.6472\n",
            "STA i do not know what the mouse was thinking ? ? why come into a house with 2 ferocious felines ? ? END END END END END END END END END END END END END END END END END END END END END END ? ? ? END did \n",
            "Training epoch: 22, training examples: 2578 - 5156\n",
            "Epoch 1/1\n",
            "37537/37537 [==============================] - 89s 2ms/step - loss: 0.6371\n",
            "STA i am just taking the lab , i have ! END END END END END i woke up they have now . . END END END END END END END END END END END END END END END END END END END END END END . END END \n",
            "Training epoch: 23, training examples: 0 - 2578\n",
            "Epoch 1/1\n",
            "36329/36329 [==============================] - 86s 2ms/step - loss: 0.6336\n",
            "STA i do not know what the mouse was thinking ? ? why come into a house with 2 ferocious felines ? ? END END END END END END END END END END END END END END END END END END END END END END the flight is the new \n",
            "Training epoch: 23, training examples: 2578 - 5156\n",
            "Epoch 1/1\n",
            "37537/37537 [==============================] - 89s 2ms/step - loss: 0.6244\n",
            "STA i am not ready to talk about pig . lol END that is been a fan for END END END END END END END END END END END END END END END END . END END END . END END END END . END END END END \n",
            "Training epoch: 24, training examples: 0 - 2578\n",
            "Epoch 1/1\n",
            "36329/36329 [==============================] - 86s 2ms/step - loss: 0.6136\n",
            "STA i do not know what the mouse was thinking ? ? why come into a house with 2 ferocious felines ? ? END END END END END END END END END END END END END END END END END END END END END END END 2 more , you \n",
            "Training epoch: 24, training examples: 2578 - 5156\n",
            "Epoch 1/1\n",
            "37537/37537 [==============================] - 89s 2ms/step - loss: 0.6047\n",
            "STA i am just taking the lab , i am just going to work END END END END END END but not many boring END END END END END END END END END END END END END END END . END END END . END END END ; getting want \n",
            "Training epoch: 25, training examples: 0 - 2578\n",
            "Epoch 1/1\n",
            "36329/36329 [==============================] - 86s 2ms/step - loss: 0.6051\n",
            "STA i do not know what the mouse was thinking ? ? why come into a house with 2 ferocious felines ? ? END END END END END END END END END END END END END END END END END END END END END END END i do not know \n",
            "Training epoch: 25, training examples: 2578 - 5156\n",
            "Epoch 1/1\n",
            "37537/37537 [==============================] - 89s 2ms/step - loss: 0.5917\n",
            "STA i am just taking the lab , i have ! END ! ! ! END END END END END END END END END END END . END END END . END END END . END END . END END END END END END END END END END END \n",
            "Training epoch: 26, training examples: 0 - 2578\n",
            "Epoch 1/1\n",
            "36329/36329 [==============================] - 86s 2ms/step - loss: 0.5902\n",
            "STA i do not know what the mouse was thinking ? ? why come into a house with 2 ferocious felines ? ? END END END END END END END END END END END END END END END END END END END END END END the flight is the one \n",
            "Training epoch: 26, training examples: 2578 - 5156\n",
            "Epoch 1/1\n",
            "37537/37537 [==============================] - 89s 2ms/step - loss: 0.5730\n",
            "STA i am just taking the lab , i have ! END END END END END i woke up you got it at work END END END END END END END END END END END END END END END END END END END END . END END END END \n",
            "Training epoch: 27, training examples: 0 - 2578\n",
            "Epoch 1/1\n",
            "36329/36329 [==============================] - 86s 2ms/step - loss: 0.5709\n",
            "STA i do not know what the mouse was thinking ? ? why come into a house with 2 ferocious felines ? ? END END END END END END END END END END END END END END END END END END END END END END END 2 more days bitches \n",
            "Training epoch: 27, training examples: 2578 - 5156\n",
            "Epoch 1/1\n",
            "37537/37537 [==============================] - 89s 2ms/step - loss: 0.5581\n",
            "STA i am just taking the lab , i have ! END END END END END i woke up you have in my head , but idk END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 28, training examples: 0 - 2578\n",
            "Epoch 1/1\n",
            "36329/36329 [==============================] - 86s 2ms/step - loss: 0.5577\n",
            "STA i do not know what the mouse was thinking ? ? why come into a house with 2 ferocious felines ? ? END END END END END END END END END END END END END END END END END END END END END END END the dem is the \n",
            "Training epoch: 28, training examples: 2578 - 5156\n",
            "Epoch 1/1\n",
            "37537/37537 [==============================] - 89s 2ms/step - loss: 0.5426\n",
            "STA i am just taking the lab , i have ! END END END END END i woke up you got it at work for you too END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 29, training examples: 0 - 2578\n",
            "Epoch 1/1\n",
            "36329/36329 [==============================] - 86s 2ms/step - loss: 0.5402\n",
            "STA i do not know what the mouse was thinking ? ? why come into a house with 2 ferocious felines ? ? END END END END END END END END END END END END END END END END END END END END END END END i do not know \n",
            "Training epoch: 29, training examples: 2578 - 5156\n",
            "Epoch 1/1\n",
            "37537/37537 [==============================] - 89s 2ms/step - loss: 0.5290\n",
            "STA i am not trying to talk about pig with my boyfri , i am just it pissed you and that shit . END END END END END END END END END END END END END END END END END . END END END . END END END ; want \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2QKBaSd7fD7C",
        "colab_type": "code",
        "outputId": "8aaeb5de-3cc0-4095-9aca-47980e7686e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 709
        }
      },
      "cell_type": "code",
      "source": [
        "for i in range(20):\n",
        "  print('input: %s'%context[i].replace('STA','').replace('END',''))\n",
        "  output = print_result(q[i:i+1])\n",
        "  print('output: %s'%output)\n",
        "  i += 1\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input:  what is up dadyo when did you get back on twitter ? haha \n",
            "output: STA creepy . END END END . do you are at the time ? END ? END are you END END END END END END END END END END END END END END END END END END END END . END END END . END END END END END . \n",
            "input:  literally never about that account , love it . \n",
            "output: STA i followed the END and i am not even this of the . END is not mean END END END END END END END END END END END END END END END END END END END END END i know END is the \n",
            "input:  about 50 times today . terminal vim user . \n",
            "output: STA i am sorry i will continue my love for the old show END END END END END END END END END END END END END END . END . END . END END END END END END . END END END END END END END END END END END \n",
            "input:  cmd+opt+esc is good but still available via menubar \n",
            "output: STA switch or hand jane . . . .choose ! ! ! ! END up those x ! END END END END END ; now ! ! ! END ! END ;3 END END END END END END END END ! END END END END END END ! ! \n",
            "input:  i am disgusted \n",
            "output: STA did not even ? END END END END END but i aint my by the first game END END END END END END the shit END END END END END END END END END END the god END END END END END END END END END \n",
            "input:  what a piece of shit \n",
            "output: STA i am not a pretty asian girl like you that can just walk in END END END END END END END but the same only sucks END END END END END END END END END END END END END END END END END END END . END them END \n",
            "input:  yay , you great hunter . ive killed lots of lizards and bugs but never a mouse . \n",
            "output: STA it is astonishing anyone is that stupid and obtuse on purpose in this century . END . END END END . END END END END END END END END END END END END END END END END END that is exactly it ! END END END END END END \n",
            "input:  and then that mouse had the nerve to try to eat our kibble !  let this be a lesson fur all the other mousies !   \n",
            "output: STA i am not ready for this episode fam END END END but the better like the sox as END END END END END END END END END END END END END END END END END i do not like this tweet like a tweet like a so \n",
            "input:  tomorrow \n",
            "output: STA *everything . that includes three meals a day with package . takes 4 days from my house . beautiful trip , love it but alone :( END END END END END END END END END END END END END END END END END END END END . END END \n",
            "input:  make sure i have a bed and seat saved next to you ! \n",
            "output: STA i am drunk END END but not yet i am a new house END END . END game END END END END END END END END END END END END END END END END telling ableist !suzaku to fuck off END END END END END END END END END \n",
            "input:  wassup shorty .  \n",
            "output: STA i have been up since 430 in the time END END END END END i have options END END END END END are so sorry END END END END END END END END END END END END END END END END END END i have been in my mind \n",
            "input:  appreciate that shorty , you too .  \n",
            "output: STA i am so bored END END ; my hair out of my sleep . END END END END END END END END END END END END END END END END END . END was my dad , , more . . . END END END \n",
            "input:  yea \n",
            "output: STA i am at my wit is here END END END . can not come see if you win &amp ; if you do not have a END END END END END END END END END END END END END END END END END END END END END \n",
            "input:  gotchu \n",
            "output: STA i usually think teachers should be better and stuff but hes a piece of shit so idc END END END END END END END END END END END END END END END END END END i know crazy to think so much was to be an \n",
            "input:  good  wby \n",
            "output: STA i am not yet sleepy . END . i am doing the indians , and i am just off but i can not have at hard END END END END END END END END END END END END END END END END END END END END END END END \n",
            "input:  that is wassup  \n",
            "output: STA well , i am not talking baseball . i am talking real indians in north dakota .people as mascots = bad END END END END END END END END END END END END END END END . END costs . END END END ; god , no \n",
            "input:  and the dash for cash races too \n",
            "output: STA yay i am so glad you got it ! i love that game ! ! ! ! END ;3 END END END END END END END END END ; not not us END END END END END END END END END END END END END END END END \n",
            "input:  do you think this wouldve happened anyway if it was another driver stinkin it up like kyle has recently ? \n",
            "output: STA i am not rich END END END i am here at work END END END END . END END END END END END END END END END END . END a bad time , do you like a different ? END END END END END END END \n",
            "input:  what did these niggas say \n",
            "output: STA haha oh that is my exact issue ! only of god i have been a and . END . END . END is a END END END END END END END END END END END END END END END END END END END END END . \n",
            "input:  kenny is wife is white on top of that . u not gonna tell me u put a flag over the color of your skin . \n",
            "output: STA shoulda been there already END END END , so chill you were the shit END END END END END END END END END END END END END END END END END END how much though i have got it to watch you in them . END END END \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "x9e3ESIn81zc",
        "colab_type": "code",
        "outputId": "f450676f-7ff2-4770-98a8-80eda1eef6c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.layers import Input, Embedding, LSTM, Dense, RepeatVector, Bidirectional, Dropout, concatenate\n",
        "from keras.optimizers import Adam, SGD\n",
        "from keras.models import Model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Activation, Dense\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.preprocessing import sequence\n",
        "import keras.backend as K\n",
        "import os\n",
        "import theano.tensor as T\n",
        "import os\n",
        "import pandas as pd\n",
        "import sys\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "weights_file = 'assignment4_twitter.h5'\n",
        "ad = Adam(lr=0.00005) \n",
        "\n",
        "input_context = Input(shape=(maxlen_input,), name='input_context')\n",
        "input_answer = Input(shape=(maxlen_input,), name='input_answer')\n",
        "LSTM_encoder = LSTM(sentence_embedding_size, init= 'lecun_uniform')\n",
        "LSTM_decoder = LSTM(sentence_embedding_size, init= 'lecun_uniform')\n",
        "if os.path.isfile(weights_file):\n",
        "    Shared_Embedding = Embedding(output_dim=word_embedding_size, input_dim=dictionary_size, input_length=maxlen_input)\n",
        "else:\n",
        "    Shared_Embedding = Embedding(output_dim=word_embedding_size, input_dim=dictionary_size, weights=[embedding_matrix], input_length=maxlen_input)\n",
        "word_embedding_context = Shared_Embedding(input_context)\n",
        "context_embedding = LSTM_encoder(word_embedding_context)\n",
        "\n",
        "word_embedding_answer = Shared_Embedding(input_answer)\n",
        "answer_embedding = LSTM_decoder(word_embedding_answer)\n",
        "\n",
        "merge_layer = concatenate([context_embedding, answer_embedding])\n",
        "out = Dense(int(dictionary_size/2), activation=\"relu\")(merge_layer)\n",
        "out = Dense(dictionary_size, activation=\"softmax\")(out)\n",
        "\n",
        "model = Model(input=[input_context, input_answer], output = [out])\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer=ad)\n",
        "\n",
        "if os.path.isfile(weights_file):\n",
        "    model.load_weights(weights_file)\n",
        "    \n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:21: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(300, kernel_initializer=\"lecun_uniform\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:22: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(300, kernel_initializer=\"lecun_uniform\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:37: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_context (InputLayer)      (None, 50)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_answer (InputLayer)       (None, 50)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_5 (Embedding)         (None, 50, 100)      1000000     input_context[0][0]              \n",
            "                                                                 input_answer[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "lstm_9 (LSTM)                   (None, 300)          481200      embedding_5[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "lstm_10 (LSTM)                  (None, 300)          481200      embedding_5[1][0]                \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_5 (Concatenate)     (None, 600)          0           lstm_9[0][0]                     \n",
            "                                                                 lstm_10[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_9 (Dense)                 (None, 5000)         3005000     concatenate_5[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_10 (Dense)                (None, 10000)        50010000    dense_9[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 54,977,400\n",
            "Trainable params: 54,977,400\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "j6hQJUl98xLM",
        "colab_type": "code",
        "outputId": "83e202b7-1db6-4f64-a9b6-7d2e1d59e2eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1381
        }
      },
      "cell_type": "code",
      "source": [
        "x = range(0,Epochs) \n",
        "valid_loss = np.zeros(Epochs)\n",
        "train_loss = np.zeros(Epochs)\n",
        "for m in range(Epochs):\n",
        "  # Loop over training batches due to memory constraints:\n",
        "  for n in range(0,round_exem,step):\n",
        "    q2 = q[n:n+step]\n",
        "    s = q2.shape\n",
        "    count = 0\n",
        "    for i, sent in enumerate(a[n:n+step]):\n",
        "      l = np.where(sent==1)\n",
        "      limit = l[0][0]\n",
        "      count += limit + 1\n",
        "    Q = np.zeros((count,maxlen_input))\n",
        "    A = np.zeros((count,maxlen_input))\n",
        "    Y = np.zeros((count,dictionary_size))\n",
        "    \n",
        "    count = 0\n",
        "    for i, sent in enumerate(a[n:n+step]):\n",
        "      ans_partial = np.zeros((1,maxlen_input))\n",
        "      # Loop over the positions of the current target output (the current output sequence):\n",
        "      l = np.where(sent==1)\n",
        "      limit = l[0][0]\n",
        "      for k in range(1,limit+1):\n",
        "        # Mapping the target output (the next output word) for one-hot codding:\n",
        "        y = np.zeros((1, dictionary_size))\n",
        "        y[0, sent[k]] = 1\n",
        "        # preparing the partial answer to input:\n",
        "        ans_partial[0,-k:] = sent[0:k]\n",
        "        # training the model for one epoch using teacher forcing:\n",
        "        Q[count, :] = q2[i:i+1] \n",
        "        A[count, :] = ans_partial \n",
        "        Y[count, :] = y\n",
        "        count += 1\n",
        "    print('Training epoch: %d, training examples: %d - %d'%(m,n, n + step))\n",
        "    model.fit([Q, A], Y, batch_size=BatchSize, epochs=1)\n",
        "    test_input = qt[6:7]\n",
        "    print(print_result(test_input))\n",
        "  model.save_weights(weights_file, overwrite=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training epoch: 0, training examples: 0 - 2578\n",
            "Epoch 1/1\n",
            "36329/36329 [==============================] - 90s 2ms/step - loss: 0.5484\n",
            "STA i do not know what the mouse was thinking ? ? why come into a house with 2 ferocious felines ? ? END END END END END END END END END END END END END END END END END END END END END the flight is the new day \n",
            "Training epoch: 0, training examples: 2578 - 5156\n",
            "Epoch 1/1\n",
            "37537/37537 [==============================] - 89s 2ms/step - loss: 0.5244\n",
            "STA good morning gorgeous how are u today babe ? END . END END END END END END END , i got it END END END END END END END END END END END END END END END END . END END END . END END END END END . \n",
            "Training epoch: 1, training examples: 0 - 2578\n",
            "Epoch 1/1\n",
            "36329/36329 [==============================] - 87s 2ms/step - loss: 0.5166\n",
            "STA i do not know what the mouse was thinking ? ? why come into a house with 2 ferocious felines ? ? END END END END END END END END END END END END END END END END END END END END END the fact i do not , \n",
            "Training epoch: 1, training examples: 2578 - 5156\n",
            "Epoch 1/1\n",
            "37537/37537 [==============================] - 89s 2ms/step - loss: 0.5040\n",
            "STA i am just taking the lab , i have ! END END END END END i am not gonna do it last night END END END END END END END END END END END END END END END END END . END END END . END END END \n",
            "Training epoch: 2, training examples: 0 - 2578\n",
            "Epoch 1/1\n",
            "36329/36329 [==============================] - 86s 2ms/step - loss: 0.5012\n",
            "STA i do not know what the mouse was thinking ? ? why come into a house with 2 ferocious felines ? ? END END END END END END END END END END END END END END END END END END END END END the flight is the one when \n",
            "Training epoch: 2, training examples: 2578 - 5156\n",
            "Epoch 1/1\n",
            "37537/37537 [==============================] - 90s 2ms/step - loss: 0.4849\n",
            "STA i am not a spurs fan , but kawhi leonard is such a beast all-around guy . kd needs to be more all-around like him in gsw . END END END END END END END END END END END END END END END END END END END END . \n",
            "Training epoch: 3, training examples: 0 - 2578\n",
            "Epoch 1/1\n",
            "36329/36329 [==============================] - 86s 2ms/step - loss: 0.4866\n",
            "STA i do not know what the mouse was thinking ? ? why come into a house with 2 ferocious felines ? ? END END END END END END END END END END END END END END END END END END END END END the flight is the new season \n",
            "Training epoch: 3, training examples: 2578 - 5156\n",
            "Epoch 1/1\n",
            "37537/37537 [==============================] - 89s 2ms/step - loss: 0.4728\n",
            "STA i am sure it is a problem , not like a family , i am like a family , so you could do not get at it END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 4, training examples: 0 - 2578\n",
            "Epoch 1/1\n",
            "36329/36329 [==============================] - 86s 2ms/step - loss: 0.4832\n",
            "STA i do not know what the mouse was thinking ? ? why come into a house with 2 ferocious felines ? ? END END END END END END END END END END END END END END END END END END END END END END 2 more days bitches END \n",
            "Training epoch: 4, training examples: 2578 - 5156\n",
            "Epoch 1/1\n",
            "37537/37537 [==============================] - 89s 2ms/step - loss: 0.4607\n",
            "STA good morning gorgeous how are u today babe ? END . END END END END END END END END END END END END END END . END END END right now , i want to pick up to check on that new new . END END . END END \n",
            "Training epoch: 5, training examples: 0 - 2578\n",
            "Epoch 1/1\n",
            "36329/36329 [==============================] - 86s 2ms/step - loss: 0.4721\n",
            "STA i do not know what the mouse was thinking ? ? why come into a house with 2 ferocious felines ? ? END END END END END END END END END END END END END END END END END END END END END END i do not know what \n",
            "Training epoch: 5, training examples: 2578 - 5156\n",
            "Epoch 1/1\n",
            "37537/37537 [==============================] - 89s 2ms/step - loss: 0.4532\n",
            "STA good morning gorgeous how are u today babe ? END . END END END END END END END , i am ready to go END END END END END END END END END END END END END . END END END END END END END END END END . \n",
            "Training epoch: 6, training examples: 0 - 2578\n",
            "Epoch 1/1\n",
            "36329/36329 [==============================] - 86s 2ms/step - loss: 0.4600\n",
            "STA i do not know what the mouse was thinking ? ? why come into a house with 2 ferocious felines ? ? END END END END END END END END END END END END END END END END END END END END END END i do not know what \n",
            "Training epoch: 6, training examples: 2578 - 5156\n",
            "Epoch 1/1\n",
            "37537/37537 [==============================] - 89s 2ms/step - loss: 0.4352\n",
            "STA i am sure it is a problem , but unless mlb requires them to learn there is not much to be my own . END END END END . END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 7, training examples: 0 - 2578\n",
            "Epoch 1/1\n",
            "36329/36329 [==============================] - 86s 2ms/step - loss: 0.4452\n",
            "STA i do not know what the mouse was thinking ? ? why come into a house with 2 ferocious felines ? ? END END END END END END END END END END END END END END END END END END END END END END the fact i do not \n",
            "Training epoch: 7, training examples: 2578 - 5156\n",
            "Epoch 1/1\n",
            "37537/37537 [==============================] - 89s 2ms/step - loss: 0.4229\n",
            "STA good morning gorgeous how are u today babe ? END . END END END . END END END END END but i would not want END END END END END . END END END END END . END END END END END END END END END END END END \n",
            "Training epoch: 8, training examples: 0 - 2578\n",
            "Epoch 1/1\n",
            "36329/36329 [==============================] - 86s 2ms/step - loss: 0.4383\n",
            "STA i do not know what the mouse was thinking ? ? why come into a house with 2 ferocious felines ? ? END END END END END END END END END END END END END END END END END END END END the good shit END END END lol \n",
            "Training epoch: 8, training examples: 2578 - 5156\n",
            "Epoch 1/1\n",
            "37537/37537 [==============================] - 89s 2ms/step - loss: 0.4129\n",
            "STA good morning , try to y dry . had good night at work , butt hurts from sitting in hard chair , got my pillow , still hurts END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 9, training examples: 0 - 2578\n",
            "Epoch 1/1\n",
            "36329/36329 [==============================] - 86s 2ms/step - loss: 0.4256\n",
            "STA i do not know what the mouse was thinking ? ? why come into a house with 2 ferocious felines ? ? END END END END END END END END END END END END END END END END END END END END END the flight is the new season \n",
            "Training epoch: 9, training examples: 2578 - 5156\n",
            "Epoch 1/1\n",
            "37537/37537 [==============================] - 89s 2ms/step - loss: 0.4038\n",
            "STA i am just taking the lab , i have ! END END END END END i woke up they sing enjoy the lot END END END END END END END END END END END END END END END END END END END END END . END END END \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "yM92txuR87Sc",
        "colab_type": "code",
        "outputId": "baf84a18-43df-4680-aece-037a54c78258",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 709
        }
      },
      "cell_type": "code",
      "source": [
        "for i in range(20):\n",
        "  print('input: %s'%context[i].replace('STA','').replace('END',''))\n",
        "  output = print_result(q[i:i+1])\n",
        "  print('output: %s'%output)\n",
        "  i += 1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input:  what is up dadyo when did you get back on twitter ? haha \n",
            "output: STA END END END can not forget END END END END . . one that may as the person can you do END END END END END END END END END END END END END END END END END END END END END END . END END END \n",
            "input:  literally never about that account , love it . \n",
            "output: STA i followed the END END i have read it but its but i am gonna take it off it END END END END END . END . END END END END END END END END END END END END END END END END END END . END \n",
            "input:  about 50 times today . terminal vim user . \n",
            "output: STA i am sorry i will not continue my respect for a cubbies and not fit . END . END it is a huge it END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "input:  cmd+opt+esc is good but still available via menubar \n",
            "output: STA switch or hand jane . . . .choose ! ! ! ! END up x END END END ! ! END ! END ! END ;3 END END END END ; now END ? END END END END END END END END END END END END . . \n",
            "input:  i am disgusted \n",
            "output: STA what bout nba or madden ? END END END END END . END but no else else END END END END END END END END . you must be going ? END END END END END END END ? END ? END END END END END END END \n",
            "input:  what a piece of shit \n",
            "output: STA i am not sure yet END END END END i have not seen how you do END END END END ? END END END END END END you do END END END END END END END END END END END END END END END END . END them \n",
            "input:  yay , you great hunter . ive killed lots of lizards and bugs but never a mouse . \n",
            "output: STA it is astonishing anyone is that stupid and obtuse on purpose in this century . END . END . END END END . END END END END END END END END END END END END END END END END END END i mean that it is to get \n",
            "input:  and then that mouse had the nerve to try to eat our kibble !  let this be a lesson fur all the other mousies !   \n",
            "output: STA i am not ready for this episode fam END END END but the old like i cant see END END END END END but i would not say it END END END END END END END END . END END END END END END END END END END \n",
            "input:  tomorrow \n",
            "output: STA that . . .was disturbing . END . END . END . END . END . END END . END END END END END END END END END END the light END END . END END END END END END . END END END END . END END END \n",
            "input:  make sure i have a bed and seat saved next to you ! \n",
            "output: STA i need to smoke , this cold turkey not going END END END END END END END . END END END END END END END END END END END takes so END and in my , keep telling me to music END END \n",
            "input:  wassup shorty .  \n",
            "output: STA i have been up since 430 in time END END END END END i have got it seen all the brigade . END looked up . END END END END END END END END END . END END END END END END END END END END END END END \n",
            "input:  appreciate that shorty , you too .  \n",
            "output: STA beauty and the beast END END END END END END END END are you always bothering END END END END . only one of the first 3 3 #bible #god END END END END END END END END END END END END END END END END END END \n",
            "input:  yea \n",
            "output: STA i have got you , do you want chicken noodle or potato ? END END END END END END END END END END END END END END END END END END . END ; if you want and with . END END END END \n",
            "input:  gotchu \n",
            "output: STA i usually think teachers should be better and stuff but hes a piece of shit so idc END END END . END END END END END END END END END END END END END END END END END END END END and six days from tokyo drift \n",
            "input:  good  wby \n",
            "output: STA i hope the #indians win today in #ndingrock #nodapl :) END END END but i am not gonna y awake END END END END END END END END END END END END . END END END . END END ; getting in 2 . END END END END \n",
            "input:  that is wassup  \n",
            "output: STA well , i am not talking baseball . i am talking real indians in north dakota .people as mascots = bad END END END END END END END END END END END END END END . END ? END ? END , you are well END END END END \n",
            "input:  and the dash for cash races too \n",
            "output: STA imy END END END do not pee yourself END END END would be good lol END END END END END but not all in the same END END END . END . END END END END END END END END END END END END END END END END END \n",
            "input:  do you think this wouldve happened anyway if it was another driver stinkin it up like kyle has recently ? \n",
            "output: STA i do not think there is any shame in being a better than a . . . END a END END END END END END END END END END END END END END END END END END END END END END END END END , \n",
            "input:  what did these niggas say \n",
            "output: STA haha oh damn that sucks . should have rode with us all these damn crazy END END END END END END END END END END END END END END END . END END END END . END is a . END . END END END END END \n",
            "input:  kenny is wife is white on top of that . u not gonna tell me u put a flag over the color of your skin . \n",
            "output: STA damn idk if i want that much . how much mut for it ? END END END END END END END END by 20 END END END by 20 minutes and END END END END END END . END END END END END END END END END END \n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
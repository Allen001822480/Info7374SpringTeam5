{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment4_seq2seq_5.5.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Allen001822480/Info7374SpringTeam5/blob/Assignment4/Assignment4_seq2seq_5_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "mRlCrT_i-4eg",
        "colab_type": "code",
        "outputId": "35ad879a-d42b-4277-c3fd-0c0198af5972",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "cell_type": "code",
      "source": [
        "import re\n",
        "tw = open('./TwitterLowerAsciiCorpus.txt')\n",
        "twitter = tw.read()\n",
        "data = [d for d in twitter.split('\\n')]\n",
        "data = [d for d in data if d != '']\n",
        "#data = eval('[%s]'%repr(data).replace('[', '').replace(']', ''))\n",
        "data = list(map(lambda x:re.sub(r'^A-Za-z\\d\\s\\,\\.\\!\\?\\'\\\"\\+\\-','',x), data))\n",
        "print(data[0:5])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[\"what's up dadyo when did you get back on twitter? haha\", \"like 2 weeks ago and it's going as terribly as i remember, but deg is still hilarious so it's ok\", 'literally never about that account, love it.', 'answer me this fellow apple peoples: how many times in the past year have you used the escape key?', 'about 50 times today. terminal vim user.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "f_ZI_NJL_F4d",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "l1 = ['won’t','won\\'t','wouldn’t','wouldn\\'t','’m', '’re', '’ve', '’ll', '’s','’d', 'n’t', '\\'m', '\\'re', '\\'ve', '\\'ll', '\\'s', '\\'d', 'can\\'t', 'n\\'t', 'B: ', 'A: ', ',', ';', '.', '?', '!', ':', '. ?', ',   .', '. ,', 'STA', 'END', 'sta', 'end']\n",
        "l2 = ['will not','will not','would not','would not',' am', ' are', ' have', ' will', ' is', ' had', ' not', ' am', ' are', ' have', ' will', ' is', ' had', 'can not', ' not', '', '', ' ,', ' ;', ' .', ' ?', ' !', ' :', '? ', '.', ',', '', '', '', '']\n",
        "\n",
        "for i, raw_word in enumerate(data):\n",
        "    for j, term in enumerate(l1):\n",
        "        raw_word = raw_word.replace(term,l2[j])\n",
        "    \n",
        "    data[i] = raw_word.lower()\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZJCSozFM_KYM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data = list(map(lambda x:'STA '+x+' END', data))\n",
        "context = data[::2]\n",
        "answers = data[1::2]\n",
        "all = context + answers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_maMcVGXftv9",
        "colab_type": "code",
        "outputId": "33db7810-1d83-45d2-c888-1b058366f08e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install vaderSentiment"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting vaderSentiment\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/86/9e/c53e1fc61aac5ee490a6ac5e21b1ac04e55a7c2aba647bb8411c9aadf24e/vaderSentiment-3.2.1-py2.py3-none-any.whl (125kB)\n",
            "\r\u001b[K    8% |██▋                             | 10kB 17.3MB/s eta 0:00:01\r\u001b[K    16% |█████▏                          | 20kB 2.3MB/s eta 0:00:01\r\u001b[K    24% |███████▉                        | 30kB 3.3MB/s eta 0:00:01\r\u001b[K    32% |██████████▍                     | 40kB 2.1MB/s eta 0:00:01\r\u001b[K    40% |█████████████                   | 51kB 2.6MB/s eta 0:00:01\r\u001b[K    48% |███████████████▋                | 61kB 3.1MB/s eta 0:00:01\r\u001b[K    56% |██████████████████▎             | 71kB 3.6MB/s eta 0:00:01\r\u001b[K    65% |████████████████████▉           | 81kB 4.0MB/s eta 0:00:01\r\u001b[K    73% |███████████████████████▍        | 92kB 4.5MB/s eta 0:00:01\r\u001b[K    81% |██████████████████████████      | 102kB 3.5MB/s eta 0:00:01\r\u001b[K    89% |████████████████████████████▋   | 112kB 3.6MB/s eta 0:00:01\r\u001b[K    97% |███████████████████████████████▎| 122kB 5.0MB/s eta 0:00:01\r\u001b[K    100% |████████████████████████████████| 133kB 4.7MB/s \n",
            "\u001b[?25hInstalling collected packages: vaderSentiment\n",
            "Successfully installed vaderSentiment-3.2.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "I62rZb5JfyqB",
        "colab_type": "code",
        "outputId": "4e74861f-2188-43e2-d7ba-5cb30fabedfc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "cell_type": "code",
      "source": [
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "context_sentiments = []\n",
        "answers_sentiments = []\n",
        "for sentence in context:\n",
        "  vs = analyzer.polarity_scores(sentence)\n",
        "  context_sentiments.append(vs['neg'])\n",
        "'''\n",
        "for sentence in answers:\n",
        "  vs = analyzer.polarity_scores(sentence)\n",
        "  answers_sentiments.append(vs['pos'])\n",
        "'''\n",
        "print(context_sentiments)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.0, 0.0, 0.0, 0.0, 0.405, 0.34, 0.122, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.231, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.179, 0.084, 0.0, 0.174, 0.211, 0.0, 0.169, 0.105, 0.0, 0.0, 0.347, 0.0, 0.245, 0.0, 0.0, 0.0, 0.333, 0.206, 0.0, 0.0, 0.0, 0.15, 0.123, 0.0, 0.0, 0.113, 0.124, 0.0, 0.0, 0.0, 0.0, 0.156, 0.289, 0.0, 0.0, 0.0, 0.065, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05, 0.0, 0.0, 0.145, 0.042, 0.0, 0.272, 0.534, 0.0, 0.0, 0.0, 0.0, 0.0, 0.194, 0.0, 0.186, 0.206, 0.0, 0.241, 0.0, 0.0, 0.0, 0.0, 0.0, 0.368, 0.196, 0.0, 0.156, 0.0, 0.216, 0.0, 0.177, 0.208, 0.0, 0.0, 0.0, 0.202, 0.0, 0.0, 0.0, 0.094, 0.0, 0.286, 0.0, 0.0, 0.0, 0.168, 0.0, 0.0, 0.097, 0.171, 0.091, 0.0, 0.0, 0.126, 0.0, 0.0, 0.381, 0.0, 0.355, 0.0, 0.0, 0.0, 0.0, 0.141, 0.0, 0.0, 0.0, 0.0, 0.261, 0.0, 0.0, 0.0, 0.173, 0.0, 0.225, 0.211, 0.0, 0.0, 0.0, 0.0, 0.147, 0.0, 0.089, 0.0, 0.154, 0.0, 0.0, 0.0, 0.091, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.212, 0.234, 0.0, 0.0, 0.0, 0.122, 0.164, 0.0, 0.116, 0.0, 0.0, 0.0, 0.074, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.116, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.323, 0.0, 0.0, 0.079, 0.144, 0.242, 0.322, 0.0, 0.502, 0.0, 0.108, 0.248, 0.0, 0.0, 0.274, 0.067, 0.119, 0.0, 0.337, 0.0, 0.258, 0.409, 0.0, 0.078, 0.0, 0.0, 0.262, 0.305, 0.0, 0.0, 0.0, 0.347, 0.0, 0.0, 0.196, 0.0, 0.105, 0.0, 0.0, 0.0, 0.119, 0.0, 0.0, 0.0, 0.226, 0.0, 0.0, 0.0, 0.152, 0.0, 0.0, 0.0, 0.0, 0.32, 0.196, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.177, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.112, 0.0, 0.118, 0.0, 0.125, 0.231, 0.262, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.245, 0.0, 0.211, 0.0, 0.247, 0.206, 0.178, 0.0, 0.191, 0.202, 0.0, 0.0, 0.0, 0.256, 0.192, 0.061, 0.075, 0.0, 0.0, 0.321, 0.0, 0.129, 0.0, 0.0, 0.0, 0.132, 0.243, 0.0, 0.355, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.458, 0.191, 0.0, 0.122, 0.139, 0.208, 0.082, 0.048, 0.0, 0.0, 0.083, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.144, 0.0, 0.0, 0.099, 0.101, 0.128, 0.0, 0.0, 0.0, 0.219, 0.153, 0.0, 0.132, 0.0, 0.0, 0.225, 0.204, 0.081, 0.107, 0.0, 0.0, 0.065, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.178, 0.273, 0.259, 0.341, 0.19, 0.201, 0.134, 0.04, 0.045, 0.155, 0.444, 0.0, 0.14, 0.157, 0.0, 0.0, 0.0, 0.0, 0.247, 0.0, 0.277, 0.0, 0.0, 0.0, 0.0, 0.0, 0.112, 0.0, 0.0, 0.179, 0.049, 0.0, 0.0, 0.171, 0.0, 0.0, 0.0, 0.0, 0.04, 0.0, 0.102, 0.0, 0.0, 0.0, 0.055, 0.07, 0.0, 0.0, 0.0, 0.0, 0.286, 0.0, 0.0, 0.0, 0.064, 0.0, 0.0, 0.203, 0.213, 0.169, 0.088, 0.0, 0.0, 0.187, 0.0, 0.0, 0.244, 0.0, 0.0, 0.079, 0.0, 0.199, 0.0, 0.0, 0.217, 0.0, 0.208, 0.0, 0.377, 0.0, 0.0, 0.135, 0.323, 0.0, 0.189, 0.0, 0.0, 0.0, 0.0, 0.13, 0.156, 0.0, 0.0, 0.0, 0.133, 0.0, 0.097, 0.0, 0.193, 0.097, 0.0, 0.0, 0.0, 0.0, 0.287, 0.0, 0.0, 0.0, 0.0, 0.259, 0.0, 0.0, 0.0, 0.0, 0.0, 0.082, 0.083, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.348, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.122, 0.0, 0.0, 0.0, 0.0, 0.0, 0.125, 0.0, 0.0, 0.0, 0.0, 0.127, 0.133, 0.399, 0.0, 0.0, 0.162, 0.0, 0.0, 0.0, 0.277, 0.11, 0.0, 0.0, 0.149, 0.205, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.247, 0.208, 0.0, 0.143, 0.066, 0.286, 0.159, 0.0, 0.0, 0.45, 0.272, 0.0, 0.138, 0.286, 0.0, 0.0, 0.267, 0.0, 0.139, 0.0, 0.0, 0.0, 0.278, 0.0, 0.34, 0.0, 0.05, 0.0, 0.0, 0.0, 0.23, 0.0, 0.0, 0.172, 0.0, 0.0, 0.241, 0.0, 0.0, 0.095, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.229, 0.0, 0.0, 0.371, 0.0, 0.153, 0.116, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.118, 0.043, 0.0, 0.13, 0.101, 0.316, 0.168, 0.0, 0.0, 0.59, 0.079, 0.355, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.189, 0.0, 0.0, 0.071, 0.0, 0.412, 0.17, 0.427, 0.0, 0.0, 0.0, 0.195, 0.247, 0.237, 0.0, 0.0, 0.0, 0.153, 0.113, 0.074, 0.0, 0.0, 0.0, 0.0, 0.206, 0.091, 0.262, 0.301, 0.0, 0.231, 0.0, 0.0, 0.0, 0.068, 0.156, 0.384, 0.095, 0.036, 0.0, 0.193, 0.0, 0.0, 0.176, 0.286, 0.118, 0.0, 0.0, 0.0, 0.0, 0.171, 0.145, 0.242, 0.0, 0.051, 0.0, 0.0, 0.0, 0.187, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.604, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.323, 0.0, 0.0, 0.114, 0.0, 0.0, 0.0, 0.259, 0.0, 0.147, 0.231, 0.0, 0.051, 0.054, 0.0, 0.0, 0.0, 0.115, 0.0, 0.0, 0.0, 0.152, 0.13, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.307, 0.0, 0.0, 0.049, 0.0, 0.0, 0.0, 0.127, 0.0, 0.0, 0.134, 0.0, 0.0, 0.0, 0.234, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.257, 0.0, 0.0, 0.075, 0.099, 0.0, 0.0, 0.068, 0.0, 0.0, 0.0, 0.18, 0.072, 0.0, 0.0, 0.166, 0.0, 0.076, 0.208, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.115, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.285, 0.087, 0.0, 0.0, 0.183, 0.0, 0.075, 0.0, 0.0, 0.0, 0.0, 0.172, 0.0, 0.176, 0.167, 0.178, 0.193, 0.0, 0.0, 0.0, 0.412, 0.126, 0.0, 0.297, 0.247, 0.228, 0.0, 0.0, 0.134, 0.0, 0.235, 0.0, 0.336, 0.0, 0.0, 0.0, 0.0, 0.096, 0.0, 0.188, 0.0, 0.0, 0.0, 0.0, 0.118, 0.0, 0.0, 0.105, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.235, 0.0, 0.197, 0.161, 0.0, 0.1, 0.076, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.255, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.093, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.158, 0.0, 0.0, 0.172, 0.0, 0.158, 0.0, 0.0, 0.0, 0.057, 0.197, 0.0, 0.0, 0.0, 0.161, 0.085, 0.0, 0.0, 0.403, 0.216, 0.298, 0.127, 0.0, 0.0, 0.0, 0.272, 0.0, 0.0, 0.0, 0.0, 0.0, 0.217, 0.0, 0.0, 0.088, 0.0, 0.0, 0.0, 0.211, 0.0, 0.213, 0.0, 0.107, 0.35, 0.0, 0.0, 0.0, 0.0, 0.185, 0.0, 0.0, 0.0, 0.153, 0.0, 0.0, 0.229, 0.0, 0.051, 0.164, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11, 0.0, 0.248, 0.304, 0.153, 0.0, 0.0, 0.091, 0.0, 0.095, 0.063, 0.0, 0.0, 0.095, 0.036, 0.0, 0.0, 0.158, 0.0, 0.0, 0.211, 0.0, 0.274, 0.0, 0.402, 0.0, 0.0, 0.429, 0.189, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.131, 0.0, 0.0, 0.0, 0.0, 0.0, 0.247, 0.0, 0.0, 0.193, 0.0, 0.0, 0.0, 0.0, 0.0, 0.296, 0.0, 0.153, 0.116, 0.0, 0.0, 0.153, 0.0, 0.195, 0.149, 0.0, 0.0, 0.0, 0.196, 0.0, 0.074, 0.0, 0.407, 0.0, 0.0, 0.0, 0.482, 0.0, 0.268, 0.0, 0.0, 0.0, 0.09, 0.0, 0.0, 0.0, 0.197, 0.657, 0.181, 0.617, 0.237, 0.0, 0.0, 0.0, 0.086, 0.0, 0.0, 0.0, 0.0, 0.067, 0.0, 0.184, 0.0, 0.0, 0.193, 0.0, 0.0, 0.0, 0.0, 0.086, 0.0, 0.2, 0.194, 0.0, 0.0, 0.262, 0.301, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.053, 0.195, 0.0, 0.097, 0.0, 0.0, 0.0, 0.388, 0.306, 0.0, 0.0, 0.256, 0.145, 0.0, 0.129, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.093, 0.067, 0.545, 0.071, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.171, 0.0, 0.0, 0.0, 0.0, 0.301, 0.45, 0.0, 0.132, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.167, 0.07, 0.067, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.049, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.259, 0.195, 0.0, 0.0, 0.13, 0.0, 0.062, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.209, 0.0, 0.0, 0.0, 0.0, 0.209, 0.0, 0.0, 0.0, 0.0, 0.0, 0.327, 0.0, 0.0, 0.101, 0.0, 0.211, 0.0, 0.0, 0.125, 0.206, 0.0, 0.47, 0.0, 0.0, 0.205, 0.149, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.131, 0.0, 0.114, 0.0, 0.412, 0.0, 0.0, 0.0, 0.0, 0.154, 0.0, 0.142, 0.0, 0.0, 0.0, 0.082, 0.0, 0.0, 0.0, 0.0, 0.281, 0.0, 0.388, 0.0, 0.154, 0.0, 0.242, 0.0, 0.079, 0.0, 0.0, 0.291, 0.0, 0.373, 0.097, 0.0, 0.072, 0.0, 0.0, 0.121, 0.0, 0.0, 0.359, 0.145, 0.095, 0.189, 0.134, 0.0, 0.0, 0.0, 0.191, 0.0, 0.264, 0.146, 0.0, 0.0, 0.174, 0.176, 0.198, 0.601, 0.0, 0.242, 0.0, 0.0, 0.275, 0.0, 0.054, 0.0, 0.0, 0.388, 0.0, 0.169, 0.0, 0.0, 0.0, 0.0, 0.243, 0.075, 0.181, 0.341, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.246, 0.0, 0.153, 0.0, 0.0, 0.343, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.049, 0.197, 0.235, 0.0, 0.425, 0.0, 0.0, 0.0, 0.0, 0.0, 0.111, 0.0, 0.0, 0.0, 0.0, 0.0, 0.335, 0.0, 0.135, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.119, 0.144, 0.07, 0.13, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.136, 0.0, 0.239, 0.228, 0.415, 0.247, 0.0, 0.09, 0.358, 0.362, 0.0, 0.0, 0.159, 0.0, 0.0, 0.36, 0.397, 0.211, 0.363, 0.187, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.286, 0.0, 0.432, 0.0, 0.101, 0.06, 0.0, 0.0, 0.0, 0.062, 0.223, 0.193, 0.192, 0.0, 0.0, 0.0, 0.0, 0.135, 0.0, 0.0, 0.0, 0.0, 0.144, 0.0, 0.0, 0.0, 0.128, 0.0, 0.434, 0.0, 0.0, 0.111, 0.0, 0.0, 0.0, 0.0, 0.0, 0.187, 0.0, 0.137, 0.0, 0.0, 0.0, 0.289, 0.091, 0.0, 0.0, 0.0, 0.0, 0.223, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.249, 0.0, 0.242, 0.221, 0.185, 0.18, 0.057, 0.0, 0.0, 0.0, 0.0, 0.0, 0.14, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.075, 0.0, 0.0, 0.0, 0.0, 0.208, 0.102, 0.102, 0.0, 0.0, 0.128, 0.0, 0.0, 0.0, 0.092, 0.166, 0.087, 0.28, 0.0, 0.0, 0.0, 0.128, 0.0, 0.0, 0.0, 0.134, 0.0, 0.0, 0.0, 0.114, 0.0, 0.0, 0.098, 0.0, 0.0, 0.099, 0.0, 0.195, 0.0, 0.332, 0.0, 0.231, 0.0, 0.0, 0.0, 0.052, 0.0, 0.247, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.259, 0.0, 0.0, 0.125, 0.138, 0.061, 0.0, 0.163, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.136, 0.0, 0.0, 0.0, 0.151, 0.0, 0.113, 0.0, 0.416, 0.253, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.209, 0.067, 0.088, 0.087, 0.0, 0.0, 0.087, 0.185, 0.0, 0.525, 0.381, 0.0, 0.115, 0.0, 0.244, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.151, 0.0, 0.0, 0.0, 0.106, 0.0, 0.383, 0.0, 0.328, 0.281, 0.262, 0.332, 0.0, 0.0, 0.251, 0.0, 0.167, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.067, 0.0, 0.143, 0.278, 0.161, 0.057, 0.0, 0.0, 0.0, 0.226, 0.0, 0.0, 0.0, 0.0, 0.138, 0.053, 0.0, 0.0, 0.0, 0.082, 0.0, 0.0, 0.0, 0.0, 0.133, 0.249, 0.0, 0.115, 0.231, 0.0, 0.0, 0.0, 0.481, 0.0, 0.0, 0.273, 0.0, 0.0, 0.0, 0.121, 0.0, 0.0, 0.079, 0.195, 0.171, 0.0, 0.284, 0.0, 0.0, 0.0, 0.0, 0.0, 0.242, 0.0, 0.0, 0.095, 0.0, 0.0, 0.437, 0.081, 0.184, 0.0, 0.0, 0.355, 0.042, 0.0, 0.0, 0.0, 0.0, 0.081, 0.194, 0.0, 0.0, 0.0, 0.0, 0.284, 0.083, 0.0, 0.0, 0.0, 0.0, 0.306, 0.22, 0.0, 0.0, 0.148, 0.277, 0.0, 0.231, 0.496, 0.247, 0.1, 0.0, 0.0, 0.0, 0.0, 0.282, 0.0, 0.293, 0.095, 0.0, 0.0, 0.0, 0.0, 0.31, 0.0, 0.0, 0.0, 0.141, 0.0, 0.104, 0.189, 0.0, 0.0, 0.0, 0.0, 0.211, 0.452, 0.0, 0.0, 0.0, 0.078, 0.267, 0.323, 0.0, 0.0, 0.0, 0.47, 0.0, 0.0, 0.223, 0.541, 0.34, 0.0, 0.211, 0.298, 0.0, 0.23, 0.0, 0.111, 0.0, 0.0, 0.0, 0.0, 0.0, 0.343, 0.277, 0.0, 0.0, 0.0, 0.078, 0.0, 0.0, 0.073, 0.242, 0.0, 0.0, 0.089, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05, 0.425, 0.0, 0.27, 0.228, 0.128, 0.0, 0.215, 0.0, 0.0, 0.0, 0.0, 0.219, 0.0, 0.0, 0.0, 0.284, 0.0, 0.18, 0.0, 0.0, 0.0, 0.0, 0.306, 0.13, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.505, 0.0, 0.116, 0.0, 0.086, 0.0, 0.0, 0.0, 0.352, 0.0, 0.081, 0.324, 0.0, 0.0, 0.0, 0.187, 0.0, 0.139, 0.0, 0.155, 0.0, 0.0, 0.11, 0.0, 0.0, 0.286, 0.0, 0.059, 0.041, 0.298, 0.0, 0.0, 0.335, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.173, 0.0, 0.453, 0.0, 0.148, 0.333, 0.0, 0.259, 0.0, 0.0, 0.208, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.246, 0.0, 0.0, 0.0, 0.169, 0.0, 0.0, 0.282, 0.0, 0.194, 0.0, 0.125, 0.118, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.133, 0.186, 0.0, 0.0, 0.0, 0.0, 0.384, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.293, 0.0, 0.141, 0.0, 0.0, 0.0, 0.111, 0.0, 0.126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.081, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.259, 0.071, 0.0, 0.0, 0.0, 0.0, 0.049, 0.106, 0.0, 0.318, 0.0, 0.187, 0.246, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.233, 0.0, 0.0, 0.0, 0.0, 0.0, 0.432, 0.352, 0.0, 0.331, 0.097, 0.0, 0.0, 0.0, 0.0, 0.128, 0.0, 0.115, 0.187, 0.0, 0.246, 0.0, 0.0, 0.0, 0.0, 0.308, 0.231, 0.115, 0.0, 0.0, 0.0, 0.432, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.134, 0.0, 0.0, 0.0, 0.0, 0.302, 0.286, 0.0, 0.0, 0.151, 0.0, 0.0, 0.0, 0.069, 0.0, 0.0, 0.365, 0.0, 0.0, 0.0, 0.199, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.246, 0.0, 0.127, 0.0, 0.0, 0.204, 0.13, 0.146, 0.0, 0.0, 0.0, 0.087, 0.0, 0.0, 0.0, 0.0, 0.0, 0.152, 0.0, 0.0, 0.123, 0.0, 0.184, 0.0, 0.153, 0.0, 0.0, 0.059, 0.041, 0.0, 0.0, 0.0, 0.149, 0.0, 0.188, 0.078, 0.113, 0.179, 0.0, 0.089, 0.061, 0.189, 0.095, 0.0, 0.167, 0.0, 0.0, 0.198, 0.0, 0.0, 0.142, 0.0, 0.078, 0.1, 0.0, 0.0, 0.0, 0.0, 0.155, 0.157, 0.457, 0.161, 0.104, 0.0, 0.0, 0.0, 0.0, 0.0, 0.092, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.168, 0.078, 0.113, 0.096, 0.0, 0.169, 0.0, 0.0, 0.0, 0.206, 0.0, 0.091, 0.0, 0.318, 0.207, 0.0, 0.064, 0.0, 0.0, 0.0, 0.228, 0.0, 0.0, 0.0, 0.432, 0.0, 0.105, 0.118, 0.167, 0.0, 0.0, 0.0, 0.0, 0.178, 0.0, 0.0, 0.113, 0.0, 0.253, 0.112, 0.0, 0.046, 0.074, 0.0, 0.0, 0.0, 0.0, 0.0, 0.118, 0.0, 0.0, 0.362, 0.0, 0.093, 0.067, 0.087, 0.185, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.247, 0.0, 0.0, 0.211, 0.0, 0.183, 0.133, 0.0, 0.069, 0.055, 0.126, 0.149, 0.277, 0.0, 0.122, 0.0, 0.057, 0.121, 0.231, 0.0, 0.0, 0.121, 0.0, 0.171, 0.0, 0.136, 0.0, 0.0, 0.084, 0.0, 0.0, 0.249, 0.0, 0.215, 0.0, 0.213, 0.0, 0.295, 0.111, 0.044, 0.0, 0.597, 0.0, 0.0, 0.0, 0.092, 0.0, 0.0, 0.476, 0.0, 0.0, 0.325, 0.0, 0.0, 0.0, 0.0, 0.0, 0.222, 0.183, 0.661, 0.0, 0.19, 0.0, 0.082, 0.32, 0.296, 0.0, 0.082, 0.0, 0.135, 0.0, 0.069, 0.0, 0.169, 0.0, 0.0, 0.0, 0.0, 0.286, 0.152, 0.0, 0.0, 0.0, 0.138, 0.062, 0.0, 0.047, 0.0, 0.0, 0.0, 0.246, 0.0, 0.276, 0.0, 0.0, 0.0, 0.0, 0.333, 0.196, 0.0, 0.0, 0.0, 0.0, 0.171, 0.129, 0.149, 0.179, 0.084, 0.0, 0.0, 0.188, 0.0, 0.326, 0.136, 0.434, 0.0, 0.189, 0.386, 0.0, 0.0, 0.0, 0.0, 0.162, 0.114, 0.224, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.077, 0.09, 0.252, 0.0, 0.0, 0.0, 0.171, 0.0, 0.0, 0.0, 0.0, 0.124, 0.0, 0.27, 0.133, 0.0, 0.087, 0.098, 0.0, 0.0, 0.0, 0.385, 0.1, 0.0, 0.0, 0.091, 0.156, 0.0, 0.0, 0.0, 0.0, 0.0, 0.124, 0.0, 0.176, 0.119, 0.179, 0.0, 0.065, 0.136, 0.247, 0.0, 0.226, 0.0, 0.093, 0.368, 0.0, 0.0, 0.0, 0.0, 0.0, 0.093, 0.212, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.468, 0.105, 0.0, 0.0, 0.0, 0.285, 0.124, 0.0, 0.0, 0.095, 0.0, 0.148, 0.302, 0.0, 0.0, 0.136, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.125, 0.0, 0.085, 0.4, 0.339, 0.0, 0.279, 0.0, 0.0, 0.157, 0.0, 0.0, 0.331, 0.0, 0.0, 0.0, 0.305, 0.0, 0.0, 0.0, 0.142, 0.0, 0.056, 0.074, 0.0, 0.0, 0.217, 0.0, 0.0, 0.0, 0.0, 0.211, 0.0, 0.0, 0.0, 0.0, 0.0, 0.091, 0.0, 0.241, 0.149, 0.0, 0.24, 0.0, 0.0, 0.241, 0.0, 0.0, 0.0, 0.112, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.14, 0.246, 0.0, 0.129, 0.183, 0.093, 0.191, 0.0, 0.0, 0.0, 0.0, 0.0, 0.278, 0.0, 0.0, 0.101, 0.137, 0.144, 0.0, 0.337, 0.0, 0.183, 0.066, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.196, 0.286, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.429, 0.06, 0.0, 0.0, 0.0, 0.0, 0.0, 0.483, 0.238, 0.0, 0.0, 0.193, 0.0, 0.0, 0.259, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.235, 0.0, 0.156, 0.0, 0.0, 0.0, 0.155, 0.211, 0.135, 0.405, 0.0, 0.052, 0.168, 0.236, 0.0, 0.16, 0.0, 0.073, 0.0, 0.0, 0.658, 0.0, 0.0, 0.0, 0.149, 0.0, 0.0, 0.386, 0.085, 0.0, 0.0, 0.293, 0.0, 0.0, 0.0, 0.14, 0.0, 0.157, 0.153, 0.285, 0.0, 0.0, 0.0, 0.0, 0.0, 0.158, 0.0, 0.0, 0.0, 0.0, 0.262, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.306, 0.306, 0.0, 0.098, 0.0, 0.0, 0.0, 0.0, 0.14, 0.115, 0.0, 0.0, 0.147, 0.0, 0.0, 0.16, 0.294, 0.0, 0.405, 0.192, 0.308, 0.0, 0.0, 0.063, 0.0, 0.0, 0.0, 0.0, 0.0, 0.091, 0.0, 0.081, 0.0, 0.0, 0.144, 0.0, 0.0, 0.0, 0.0, 0.0, 0.276, 0.322, 0.0, 0.165, 0.173, 0.0, 0.0, 0.0, 0.0, 0.151, 0.226, 0.0, 0.0, 0.0, 0.122, 0.0, 0.0, 0.127, 0.195, 0.0, 0.0, 0.0, 0.0, 0.058, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.281, 0.0, 0.126, 0.127, 0.325, 0.0, 0.194, 0.0, 0.0, 0.0, 0.088, 0.0, 0.21, 0.0, 0.0, 0.0, 0.38, 0.167, 0.15, 0.234, 0.0, 0.0, 0.0, 0.17, 0.053, 0.04, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.381, 0.0, 0.0, 0.202, 0.0, 0.0, 0.0, 0.153, 0.0, 0.135, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.119, 0.0, 0.045, 0.061, 0.0, 0.0, 0.11, 0.0, 0.0, 0.0, 0.0, 0.158, 0.0, 0.091, 0.0, 0.0, 0.118, 0.0, 0.0, 0.378, 0.0, 0.137, 0.0, 0.0, 0.188, 0.0, 0.0, 0.122, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.136, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.122, 0.0, 0.0, 0.0, 0.185, 0.0, 0.0, 0.0, 0.105, 0.152, 0.0, 0.062, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.425, 0.0, 0.0, 0.187, 0.0, 0.0, 0.0, 0.15, 0.123, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.171, 0.111, 0.117, 0.0, 0.0, 0.161, 0.0, 0.0, 0.604, 0.33, 0.082, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.167, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.272, 0.0, 0.332, 0.285, 0.124, 0.0, 0.24, 0.0, 0.196, 0.0, 0.28, 0.0, 0.167, 0.132, 0.0, 0.228, 0.087, 0.115, 0.0, 0.0, 0.081, 0.104, 0.0, 0.333, 0.0, 0.258, 0.0, 0.0, 0.0, 0.167, 0.0, 0.4, 0.0, 0.0, 0.328, 0.0, 0.0, 0.0, 0.098, 0.0, 0.0, 0.0, 0.088, 0.094, 0.0, 0.0, 0.0, 0.565, 0.0, 0.208, 0.0, 0.0, 0.0, 0.116, 0.348, 0.068, 0.0, 0.0, 0.095, 0.231, 0.183, 0.146, 0.0, 0.33, 0.226, 0.455, 0.0, 0.379, 0.0, 0.089, 0.0, 0.0, 0.104, 0.107, 0.0, 0.0, 0.368, 0.196, 0.0, 0.156, 0.289, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.156, 0.0, 0.0, 0.0, 0.0, 0.0, 0.211, 0.0, 0.264, 0.144, 0.0, 0.091, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.479, 0.0, 0.0, 0.0, 0.061, 0.266, 0.0, 0.307, 0.275, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.121, 0.282, 0.0, 0.0, 0.0, 0.0, 0.0, 0.176, 0.241, 0.0, 0.0, 0.0, 0.0, 0.0, 0.421, 0.0, 0.403, 0.0, 0.0, 0.412, 0.0, 0.0, 0.0, 0.143, 0.07, 0.268, 0.0, 0.0, 0.067, 0.284, 0.0, 0.113, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.098, 0.0, 0.0, 0.338, 0.0, 0.197, 0.329, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.247, 0.247, 0.0, 0.0, 0.262, 0.0, 0.0, 0.0, 0.0, 0.16, 0.333, 0.0, 0.073, 0.16, 0.09, 0.0, 0.167, 0.0, 0.0, 0.217, 0.0, 0.0, 0.0, 0.0, 0.312, 0.0, 0.0, 0.0, 0.0, 0.14, 0.125, 0.231, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.228, 0.0, 0.0, 0.115, 0.0, 0.0, 0.0, 0.091, 0.173, 0.242, 0.0, 0.0, 0.0, 0.0, 0.391, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.173, 0.0, 0.0, 0.0, 0.301, 0.373, 0.29, 0.0, 0.0, 0.0, 0.199, 0.0, 0.0, 0.0, 0.213, 0.0, 0.161, 0.0, 0.0, 0.0, 0.0, 0.0, 0.138, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.164, 0.0, 0.0, 0.0, 0.176, 0.286, 0.213, 0.0, 0.0, 0.0, 0.0, 0.122, 0.172, 0.0, 0.0, 0.077, 0.154, 0.198, 0.128, 0.0, 0.0, 0.0, 0.0, 0.0, 0.083, 0.0, 0.156, 0.0, 0.372, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.194, 0.0, 0.367, 0.0, 0.0, 0.0, 0.0, 0.313, 0.181, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.134, 0.355, 0.0, 0.0, 0.565, 0.0, 0.0, 0.225, 0.0, 0.134, 0.0, 0.0, 0.0, 0.238, 0.0, 0.0, 0.18, 0.576, 0.381, 0.0, 0.097, 0.0, 0.146, 0.0, 0.0, 0.206, 0.178, 0.164, 0.0, 0.344, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.145, 0.164, 0.229, 0.0, 0.147, 0.25, 0.265, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.125, 0.109, 0.0, 0.144, 0.105, 0.0, 0.178, 0.109, 0.0, 0.0, 0.082, 0.0, 0.0, 0.188, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.108, 0.038, 0.0, 0.0, 0.304, 0.218, 0.0, 0.0, 0.0, 0.225, 0.0, 0.078, 0.0, 0.345, 0.071, 0.0, 0.0, 0.0, 0.0, 0.0, 0.138, 0.065, 0.161, 0.0, 0.165, 0.174, 0.0, 0.142, 0.106, 0.047, 0.175, 0.0, 0.0, 0.0, 0.166, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.35, 0.211, 0.0, 0.103, 0.0, 0.0, 0.0, 0.124, 0.091, 0.0, 0.161, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.144, 0.186, 0.0, 0.048, 0.0, 0.341, 0.0, 0.0, 0.0, 0.0, 0.0, 0.134, 0.0, 0.0, 0.062, 0.16, 0.0, 0.367, 0.167, 0.169, 0.0, 0.0, 0.1, 0.0, 0.046, 0.189, 0.244, 0.0, 0.0, 0.0, 0.0, 0.236, 0.145, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.134, 0.165, 0.0, 0.0, 0.184, 0.0, 0.0, 0.0, 0.197, 0.0, 0.0, 0.107, 0.0, 0.0, 0.069, 0.0, 0.0, 0.0, 0.0, 0.211, 0.0, 0.0, 0.123, 0.0, 0.206, 0.0, 0.0, 0.131, 0.0, 0.0, 0.247, 0.0, 0.365, 0.0, 0.0, 0.0, 0.273, 0.0, 0.0, 0.0, 0.071, 0.112, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.286, 0.0, 0.0, 0.0, 0.0, 0.089, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.322, 0.0, 0.0, 0.145, 0.0, 0.0, 0.0, 0.0, 0.069, 0.0, 0.211, 0.268, 0.0, 0.0, 0.072, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.277, 0.212, 0.072, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.195, 0.307, 0.0, 0.0, 0.118, 0.043, 0.205, 0.238, 0.0, 0.075, 0.0, 0.0, 0.0, 0.0, 0.0, 0.128, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.072, 0.0, 0.0, 0.0, 0.14, 0.135, 0.137, 0.301, 0.0, 0.325, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.073, 0.062, 0.0, 0.0, 0.0, 0.0, 0.054, 0.0, 0.0, 0.0, 0.085, 0.275, 0.0, 0.246, 0.0, 0.193, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.14, 0.156, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.083, 0.0, 0.19, 0.083, 0.0, 0.0, 0.428, 0.098, 0.0, 0.07, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.349, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.18, 0.26, 0.193, 0.0, 0.0, 0.0, 0.0, 0.623, 0.0, 0.0, 0.267, 0.0, 0.079, 0.155, 0.0, 0.0, 0.116, 0.0, 0.0, 0.0, 0.121, 0.0, 0.169, 0.206, 0.244, 0.0, 0.0, 0.0, 0.285, 0.124, 0.091, 0.0, 0.0, 0.0, 0.0, 0.182, 0.251, 0.113, 0.0, 0.0, 0.0, 0.524, 0.0, 0.0, 0.277, 0.11, 0.274, 0.0, 0.134, 0.0, 0.229, 0.0, 0.0, 0.0, 0.069, 0.09, 0.0, 0.154, 0.0, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.142, 0.0, 0.114, 0.315, 0.0, 0.0, 0.0, 0.313, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.162, 0.0, 0.0, 0.0, 0.154, 0.0, 0.0, 0.19, 0.0, 0.0, 0.0, 0.096, 0.0, 0.0, 0.0, 0.0, 0.122, 0.094, 0.0, 0.054, 0.0, 0.0, 0.0, 0.077, 0.098, 0.124, 0.091, 0.0, 0.0, 0.0, 0.0, 0.0, 0.233, 0.0, 0.0, 0.0, 0.0, 0.0, 0.321, 0.0, 0.492, 0.193, 0.122, 0.164, 0.0, 0.0, 0.0, 0.196, 0.0, 0.0, 0.0, 0.0, 0.533, 0.0, 0.0, 0.59, 0.0, 0.0, 0.0, 0.0, 0.0, 0.129, 0.149, 0.124, 0.186, 0.0, 0.0, 0.0, 0.0, 0.0, 0.201, 0.201, 0.11, 0.234, 0.163, 0.0, 0.0, 0.0, 0.0, 0.059, 0.119, 0.127, 0.399, 0.0, 0.0, 0.0, 0.0, 0.0, 0.412, 0.0, 0.0, 0.0, 0.322, 0.0, 0.0, 0.0, 0.082, 0.0, 0.411, 0.0, 0.0, 0.157, 0.0, 0.071, 0.0, 0.0, 0.0, 0.0, 0.0, 0.268, 0.185, 0.191, 0.266, 0.0, 0.0, 0.0, 0.306, 0.306, 0.19, 0.0, 0.138, 0.151, 0.295, 0.0, 0.0, 0.0, 0.37, 0.0, 0.0, 0.0, 0.0, 0.287, 0.327, 0.1, 0.0, 0.143, 0.0, 0.0, 0.0, 0.0, 0.252, 0.394, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.199, 0.253, 0.333, 0.206, 0.0, 0.087, 0.0, 0.0, 0.0, 0.0, 0.0, 0.065, 0.077, 0.0, 0.0, 0.0, 0.0, 0.065, 0.0, 0.208, 0.0, 0.0, 0.261, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.207, 0.125, 0.076, 0.0, 0.25, 0.0, 0.0, 0.0, 0.046, 0.074, 0.0, 0.0, 0.0, 0.0, 0.236, 0.0, 0.0, 0.0, 0.348, 0.0, 0.148, 0.29, 0.0, 0.0, 0.124, 0.0, 0.0, 0.355, 0.049, 0.106, 0.0, 0.0, 0.0, 0.0, 0.259, 0.422, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.261, 0.303, 0.118, 0.189, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.157, 0.136, 0.0, 0.0, 0.0, 0.292, 0.422, 0.0, 0.099, 0.0, 0.265, 0.431, 0.0, 0.0, 0.0, 0.0, 0.362, 0.0, 0.0, 0.0, 0.0, 0.0, 0.059, 0.0, 0.0, 0.152, 0.0, 0.0, 0.0, 0.0, 0.238, 0.0, 0.0, 0.0, 0.0, 0.115, 0.189, 0.356, 0.0, 0.0, 0.1, 0.0, 0.0, 0.104, 0.104, 0.0, 0.0, 0.0, 0.0, 0.163, 0.156, 0.218, 0.161, 0.0, 0.0, 0.0, 0.304, 0.0, 0.0, 0.249, 0.0, 0.251, 0.0, 0.124, 0.202, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.189, 0.438, 0.0, 0.0, 0.0, 0.062, 0.0, 0.214, 0.256, 0.0, 0.0, 0.136, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.094, 0.0, 0.0, 0.146, 0.0, 0.0, 0.0, 0.116, 0.0, 0.0, 0.0, 0.0, 0.117, 0.165, 0.0, 0.316, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.22, 0.216, 0.479, 0.225, 0.098, 0.0, 0.0, 0.0, 0.0, 0.0, 0.058, 0.087, 0.0, 0.161, 0.0, 0.12, 0.15, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.161, 0.108, 0.039, 0.0, 0.0, 0.0, 0.368, 0.0, 0.134, 0.165, 0.0, 0.0, 0.0, 0.0, 0.156, 0.258, 0.221, 0.0, 0.342, 0.326, 0.0, 0.0, 0.0, 0.0, 0.202, 0.0, 0.0, 0.0, 0.0, 0.0, 0.246, 0.0, 0.0, 0.0, 0.0, 0.375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.083, 0.0, 0.0, 0.0, 0.0, 0.109, 0.31, 0.412, 0.0, 0.0, 0.0, 0.174, 0.0, 0.355, 0.0, 0.166, 0.176, 0.066, 0.0, 0.0, 0.257, 0.0, 0.432, 0.0, 0.0, 0.073, 0.088, 0.0, 0.0, 0.075, 0.0, 0.0, 0.332, 0.0, 0.0, 0.0, 0.0, 0.0, 0.276, 0.0, 0.0, 0.321, 0.0, 0.0, 0.0, 0.034, 0.0, 0.0, 0.0, 0.0, 0.0, 0.298, 0.0, 0.32, 0.0, 0.329, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.351, 0.103, 0.0, 0.0, 0.374, 0.0, 0.186, 0.157, 0.0, 0.0, 0.093, 0.0, 0.151, 0.0, 0.131, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.107, 0.109, 0.412, 0.157, 0.0, 0.0, 0.15, 0.545, 0.103, 0.0, 0.362, 0.593, 0.0, 0.0, 0.0, 0.057, 0.121, 0.231, 0.228, 0.079, 0.0, 0.0, 0.258, 0.0, 0.173, 0.0, 0.0, 0.0, 0.0, 0.115, 0.206, 0.0, 0.192, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.268, 0.0, 0.0, 0.0, 0.0, 0.247, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.097, 0.0, 0.266, 0.0, 0.0, 0.172, 0.0, 0.0, 0.135, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.14, 0.0, 0.0, 0.0, 0.081, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.082, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.111, 0.0, 0.0, 0.0, 0.106, 0.102, 0.0, 0.055, 0.0, 0.0, 0.34, 0.2, 0.0, 0.0, 0.421, 0.0, 0.0, 0.193, 0.0, 0.0, 0.0, 0.374, 0.0, 0.188, 0.0, 0.0, 0.0, 0.531, 0.0, 0.0, 0.0, 0.111, 0.279, 0.0, 0.0, 0.078, 0.0, 0.132, 0.099, 0.079, 0.0, 0.0, 0.0, 0.115, 0.093, 0.102, 0.236, 0.064, 0.213, 0.0, 0.0, 0.0, 0.0, 0.0, 0.074, 0.266, 0.0, 0.191, 0.0, 0.0, 0.0, 0.14, 0.0, 0.072, 0.0, 0.114, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.17, 0.15, 0.0, 0.0, 0.431, 0.0, 0.0, 0.0, 0.0, 0.0, 0.249, 0.0, 0.497, 0.392, 0.0, 0.0, 0.0, 0.135, 0.0, 0.0, 0.0, 0.0, 0.129, 0.0, 0.151, 0.132, 0.0, 0.0, 0.0, 0.0, 0.0, 0.158, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.099, 0.073, 0.0, 0.329, 0.055, 0.126, 0.401, 0.202, 0.355, 0.0, 0.11, 0.0, 0.0, 0.236, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.21, 0.0, 0.186, 0.0, 0.119, 0.0, 0.0, 0.0, 0.0, 0.432, 0.192, 0.142, 0.0, 0.0, 0.0, 0.0, 0.034, 0.333, 0.0, 0.164, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.126, 0.0, 0.266, 0.0, 0.066, 0.0, 0.132, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.412, 0.0, 0.18, 0.0, 0.0, 0.0, 0.0, 0.0, 0.203, 0.0, 0.0, 0.0, 0.0, 0.0, 0.29, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03, 0.067, 0.0, 0.0, 0.0, 0.179, 0.107, 0.212, 0.0, 0.0, 0.0, 0.0, 0.224, 0.0, 0.0, 0.256, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.115, 0.0, 0.268, 0.0, 0.0, 0.0, 0.197, 0.079, 0.0, 0.0, 0.0, 0.412, 0.0, 0.0, 0.554, 0.0, 0.0, 0.0, 0.56, 0.0, 0.13, 0.0, 0.343, 0.0, 0.0, 0.0, 0.0, 0.104, 0.0, 0.0, 0.091, 0.0, 0.0, 0.0, 0.0, 0.0, 0.135, 0.0, 0.237, 0.113, 0.0, 0.195, 0.285, 0.0, 0.0, 0.0, 0.0, 0.062, 0.0, 0.0, 0.082, 0.0, 0.187, 0.0, 0.0, 0.0, 0.143, 0.306, 0.184, 0.0, 0.0, 0.0, 0.0, 0.139, 0.208, 0.167, 0.231, 0.0, 0.0, 0.097, 0.181, 0.194, 0.412, 0.0, 0.084, 0.0, 0.363, 0.552, 0.166, 0.094, 0.0, 0.113, 0.376, 0.204, 0.153, 0.0, 0.342, 0.0, 0.0, 0.122, 0.0, 0.0, 0.274, 0.205, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.308, 0.0, 0.0, 0.0, 0.189, 0.0, 0.258, 0.109, 0.097, 0.0, 0.0, 0.0, 0.099, 0.0, 0.0, 0.0, 0.145, 0.042, 0.127, 0.0, 0.0, 0.0, 0.116, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.211, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.153, 0.0, 0.0, 0.0, 0.194, 0.094, 0.059, 0.0, 0.213, 0.117, 0.0, 0.0, 0.071, 0.0, 0.171, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.186, 0.056, 0.074, 0.0, 0.101, 0.0, 0.0, 0.19, 0.257, 0.0, 0.199, 0.0, 0.0, 0.0, 0.0, 0.083, 0.0, 0.37, 0.2, 0.0, 0.0, 0.367, 0.0, 0.0, 0.263, 0.375, 0.0, 0.0, 0.213, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.244, 0.0, 0.0, 0.0, 0.0, 0.213, 0.0, 0.0, 0.0, 0.0, 0.127, 0.145, 0.0, 0.0, 0.0, 0.0, 0.0, 0.279, 0.355, 0.0, 0.0, 0.157, 0.231, 0.0, 0.181, 0.0, 0.0, 0.0, 0.0, 0.167, 0.0, 0.204, 0.0, 0.096, 0.0, 0.0, 0.17, 0.0, 0.0, 0.0, 0.0, 0.122, 0.164, 0.0, 0.0, 0.0, 0.069, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.154, 0.0, 0.248, 0.0, 0.131, 0.0, 0.0, 0.0, 0.183, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11, 0.0, 0.0, 0.315, 0.0, 0.0, 0.123, 0.0, 0.0, 0.0, 0.0, 0.0, 0.176, 0.103, 0.0, 0.0, 0.0, 0.0, 0.0, 0.431, 0.247, 0.202, 0.182, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.266, 0.0, 0.0, 0.0, 0.0, 0.14, 0.074, 0.0, 0.0, 0.0, 0.583, 0.0, 0.0, 0.0, 0.208, 0.0, 0.0, 0.0, 0.0, 0.306, 0.143, 0.0, 0.052, 0.309, 0.0, 0.441, 0.0, 0.0, 0.073, 0.0, 0.0, 0.106, 0.102, 0.0, 0.247, 0.0, 0.236, 0.0, 0.0, 0.0, 0.0, 0.0, 0.193, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.091, 0.167, 0.0, 0.0, 0.0, 0.0, 0.0, 0.112, 0.0, 0.0, 0.497, 0.392, 0.0, 0.375, 0.0, 0.147, 0.358, 0.0, 0.216, 0.211, 0.0, 0.0, 0.0, 0.0, 0.181, 0.0, 0.137, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.105, 0.239, 0.0, 0.202, 0.156, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.346, 0.0, 0.0, 0.0, 0.0, 0.238, 0.0, 0.0, 0.0, 0.0, 0.34, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.375, 0.189, 0.0, 0.127, 0.151, 0.0, 0.0, 0.0, 0.0, 0.068, 0.0, 0.0, 0.136, 0.341, 0.0, 0.0, 0.0, 0.0, 0.0, 0.075, 0.0, 0.0, 0.0, 0.492, 0.193, 0.361, 0.167, 0.0, 0.0, 0.096, 0.0, 0.208, 0.0, 0.131, 0.241, 0.0, 0.0, 0.0, 0.0, 0.246, 0.0, 0.071, 0.0, 0.0, 0.0, 0.0, 0.0, 0.137, 0.0, 0.113, 0.209, 0.0, 0.0, 0.101, 0.22, 0.0, 0.0, 0.231, 0.0, 0.054, 0.071, 0.0, 0.209, 0.0, 0.0, 0.0, 0.078, 0.037, 0.0, 0.099, 0.152, 0.0, 0.187, 0.0, 0.0, 0.152, 0.11, 0.0, 0.326, 0.239, 0.173, 0.089, 0.1, 0.053, 0.0, 0.0, 0.0, 0.166, 0.0, 0.0, 0.0, 0.405, 0.0, 0.0, 0.208, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.252, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.144, 0.0, 0.0, 0.32, 0.202, 0.105, 0.371, 0.0, 0.311, 0.37, 0.412, 0.0, 0.0, 0.0, 0.22, 0.0, 0.0, 0.105, 0.0, 0.239, 0.151, 0.0, 0.393, 0.0, 0.0, 0.0, 0.194, 0.0, 0.0, 0.0, 0.0, 0.528, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.091, 0.0, 0.197, 0.132, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.268, 0.0, 0.349, 0.393, 0.0, 0.0, 0.0, 0.194, 0.091, 0.0, 0.0, 0.08, 0.074, 0.0, 0.292, 0.0, 0.195, 0.0, 0.0, 0.0, 0.0, 0.421, 0.0, 0.0, 0.149, 0.196, 0.0, 0.0, 0.0, 0.365, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.146, 0.0, 0.0, 0.247, 0.0, 0.236, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.31, 0.0, 0.434, 0.0, 0.102, 0.071, 0.0, 0.0, 0.074, 0.0, 0.234, 0.291, 0.08, 0.296, 0.0, 0.118, 0.22, 0.168, 0.617, 0.0, 0.0, 0.0, 0.368, 0.194, 0.24, 0.0, 0.167, 0.097, 0.0, 0.203, 0.241, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.112, 0.0, 0.096, 0.0, 0.0, 0.0, 0.216, 0.197, 0.0, 0.0, 0.0, 0.0, 0.141, 0.0, 0.0, 0.0, 0.272, 0.144, 0.0, 0.0, 0.186, 0.253, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.191, 0.0, 0.0, 0.0, 0.0, 0.0, 0.105, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.086, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.084, 0.17, 0.0, 0.106, 0.0, 0.0, 0.0, 0.124, 0.0, 0.0, 0.0, 0.306, 0.0, 0.0, 0.0, 0.244, 0.0, 0.0, 0.0, 0.0, 0.146, 0.0, 0.148, 0.0, 0.294, 0.208, 0.116, 0.0, 0.0, 0.0, 0.123, 0.064, 0.0, 0.0, 0.0, 0.18, 0.072, 0.0, 0.119, 0.0, 0.0, 0.151, 0.0, 0.131, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.184, 0.119, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.088, 0.104, 0.141, 0.0, 0.249, 0.219, 0.179, 0.208, 0.0, 0.0, 0.0, 0.155, 0.211, 0.0, 0.263, 0.0, 0.255, 0.12, 0.106, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.137, 0.0, 0.0, 0.0, 0.444, 0.172, 0.223, 0.0, 0.248, 0.098, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.155, 0.0, 0.0, 0.0, 0.0, 0.0, 0.189, 0.27, 0.0, 0.0, 0.112, 0.136, 0.0, 0.14, 0.0, 0.0, 0.0, 0.0, 0.0, 0.087, 0.0, 0.0, 0.0, 0.0, 0.13, 0.0, 0.225, 0.0, 0.078, 0.158, 0.0, 0.0, 0.179]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "eEIFUO5n_L7F",
        "colab_type": "code",
        "outputId": "6b619f69-c023-4b80-d698-e476639ae15d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import itertools\n",
        "from keras.preprocessing import sequence\n",
        "vocabulary_file = 'vocabulary_twitter'\n",
        "padded_context_file = 'Padded_context'\n",
        "padded_answers_file = 'Padded_answers'\n",
        "unknown_token = 'something'\n",
        "\n",
        "vocabulary_size = 10000\n",
        "max_features = vocabulary_size\n",
        "maxlen_input = 50\n",
        "maxlen_output = 50  # cut texts after this number of words\n",
        "\n",
        "all = ' '.join(all)\n",
        "tokenized_all = all.split()\n",
        "tokenized_context = [t.split() for t in context]\n",
        "tokenized_answers = [t.split() for t in answers]\n",
        "\n",
        "word_freq = nltk.FreqDist(itertools.chain(tokenized_all))\n",
        "print (\"Found %d unique words tokens.\" % len(word_freq.items()))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 11854 unique words tokens.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mKy5TunA_PNv",
        "colab_type": "code",
        "outputId": "64b006ab-147d-41c0-e00b-a4dce350381d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "vocab = word_freq.most_common(vocabulary_size-1)\n",
        "with open(vocabulary_file, 'wb') as v:\n",
        "  pickle.dump(vocab, v)\n",
        "\n",
        "print(vocab[0:20])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('STA', 21028), ('END', 21028), ('.', 6665), ('i', 5236), ('the', 2913), ('to', 2577), ('you', 2550), ('is', 2536), ('!', 2242), (',', 2213), ('it', 2051), ('a', 2046), ('not', 1964), ('?', 1839), ('and', 1655), ('that', 1436), ('my', 1196), ('in', 1171), ('of', 1082), ('am', 1056)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "SS9Jbs0A_SCl",
        "colab_type": "code",
        "outputId": "1988c227-2985-4adc-d270-236a566a9a3b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "cell_type": "code",
      "source": [
        "vocab = pickle.load(open(vocabulary_file, 'rb'))\n",
        "index_to_word = [x[0] for x in vocab]\n",
        "index_to_word.append(unknown_token)\n",
        "word_to_index = dict([(w,i) for i,w in enumerate(index_to_word)])\n",
        "\n",
        "print (\"Using vocabulary of size %d.\" % vocabulary_size)\n",
        "print (\"The least frequent word in our vocabulary is '%s' and appeared %d times.\" % (vocab[-1][0], vocab[-1][1]))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using vocabulary of size 10000.\n",
            "The least frequent word in our vocabulary is 'mcdonalds' and appeared 1 times.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "rsElu_dP_Ua4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "# Replacing all words not in our vocabulary with the unknown token:\n",
        "for i, sent in enumerate(tokenized_answers):\n",
        "  tokenized_answers[i] = [w if w in word_to_index else unknown_token for w in sent]\n",
        "   \n",
        "for i, sent in enumerate(tokenized_context):\n",
        "  tokenized_context[i] = [w if w in word_to_index else unknown_token for w in sent]\n",
        "\n",
        "# Creating the training data:\n",
        "X = np.asarray([[word_to_index[w] for w in sent] for sent in tokenized_context])\n",
        "Y = np.asarray([[word_to_index[w] for w in sent] for sent in tokenized_answers])\n",
        "\n",
        "Q = sequence.pad_sequences(X, maxlen=maxlen_input, padding='post')\n",
        "A = sequence.pad_sequences(Y, maxlen=maxlen_output, padding='post')\n",
        "\n",
        "row, col = Q.shape\n",
        "for i in range(row):\n",
        "  Q[i,:] = Q[i,:]*context_sentiments[i]\n",
        "\n",
        "\n",
        "with open(padded_context_file, 'wb') as q:\n",
        "    pickle.dump(Q, q)\n",
        "    \n",
        "with open(padded_answers_file, 'wb') as a:\n",
        "    pickle.dump(A, a)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yg76ZiVv-1SG",
        "colab_type": "code",
        "outputId": "d973ef8e-e14d-4917-a138-534d7c0c8b69",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "print(Q.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5256, 50)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "V4hG9clF_Xlk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import requests\n",
        "file_url = \"http://nlp.stanford.edu/data/glove.6B.zip\"\n",
        "tr = requests.get(file_url, stream=True)\n",
        "with open(\"glove.6B.zip\", \"wb\") as f:\n",
        "    for chunk in tr.iter_content(chunk_size=1024):\n",
        "        if chunk:\n",
        "            f.write(chunk)\n",
        "            \n",
        "            \n",
        "import zipfile\n",
        "import os\n",
        "def un_zip(file_name):\n",
        "    \"\"\"unzip zip file\"\"\"\n",
        "    zip_file = zipfile.ZipFile(file_name)\n",
        "    if os.path.isdir(file_name + \"_files\"):\n",
        "        pass\n",
        "    else:\n",
        "        os.mkdir(file_name + \"_files\")\n",
        "    for names in zip_file.namelist():\n",
        "        zip_file.extract(names,file_name + \"_files/\")\n",
        "    zip_file.close()\n",
        "        \n",
        "glove = un_zip(\"glove.6B.zip\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "X0zFIatn_bn4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "padded_context_file = 'Padded_context'\n",
        "padded_answers_file = 'Padded_answers'\n",
        "unknown_token = 'something'\n",
        "word_embedding_size = 100\n",
        "sentence_embedding_size = 300\n",
        "dictionary_size = 10000\n",
        "maxlen_input = 50\n",
        "maxlen_output = 50\n",
        "num_subsets = 2\n",
        "Epochs = 50\n",
        "BatchSize = 128 \n",
        "Patience = 0\n",
        "dropout = .2\n",
        "n_test = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vPSeHTgw_eeP",
        "colab_type": "code",
        "outputId": "3462d67a-3d5a-4bc4-caa2-bc3c5d1744c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "cell_type": "code",
      "source": [
        "import _pickle\n",
        "import numpy as np\n",
        "\n",
        "embeddings_index = {}\n",
        "f = open('./glove.6B.zip_files/glove.6B.100d.txt')\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print('Found %s word vectors.' % len(embeddings_index))\n",
        "embedding_matrix = np.zeros((dictionary_size, word_embedding_size))\n",
        "\n",
        "# Loading our vocabulary:\n",
        "vocabulary = _pickle.load(open(vocabulary_file, 'rb'))\n",
        "\n",
        "# Using the Glove embedding:\n",
        "i = 0\n",
        "for word in vocabulary:\n",
        "    embedding_vector = embeddings_index.get(word[0])\n",
        "    \n",
        "    if embedding_vector is not None:\n",
        "        # words not found in embedding index will be all-zeros.\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "    i += 1\n",
        "    \n",
        "print(embedding_matrix)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 400000 word vectors.\n",
            "[[ 0.          0.          0.         ...  0.          0.\n",
            "   0.        ]\n",
            " [ 0.          0.          0.         ...  0.          0.\n",
            "   0.        ]\n",
            " [-0.33978999  0.20941     0.46348    ... -0.23394001  0.47297999\n",
            "  -0.028803  ]\n",
            " ...\n",
            " [-0.16700999  0.10193    -0.62102997 ... -0.55523002 -0.60065001\n",
            "  -0.22685   ]\n",
            " [ 0.31239     0.031386   -0.27726999 ...  0.28920999  0.82100999\n",
            "   0.84512001]\n",
            " [ 0.          0.          0.         ...  0.          0.\n",
            "   0.        ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "s-pWlk3-_hen",
        "colab_type": "code",
        "outputId": "8c6fbd09-0fee-4917-a318-ad82464a0bd8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 541
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.layers import Input, Embedding, LSTM, Dense, RepeatVector, Bidirectional, Dropout, concatenate\n",
        "from keras.optimizers import Adam, SGD\n",
        "from keras.models import Model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Activation, Dense\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.preprocessing import sequence\n",
        "import keras.backend as K\n",
        "import os\n",
        "import theano.tensor as T\n",
        "import os\n",
        "import pandas as pd\n",
        "import sys\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "weights_file = 'assignment4_twitter.h5'\n",
        "ad = Adam(lr=0.00005) \n",
        "\n",
        "input_context = Input(shape=(maxlen_input,), name='input_context')\n",
        "input_answer = Input(shape=(maxlen_input,), name='input_answer')\n",
        "LSTM_encoder = LSTM(sentence_embedding_size, init= 'lecun_uniform')\n",
        "LSTM_decoder = LSTM(sentence_embedding_size, init= 'lecun_uniform')\n",
        "if os.path.isfile(weights_file):\n",
        "    Shared_Embedding = Embedding(output_dim=word_embedding_size, input_dim=dictionary_size, input_length=maxlen_input)\n",
        "else:\n",
        "    Shared_Embedding = Embedding(output_dim=word_embedding_size, input_dim=dictionary_size, weights=[embedding_matrix], input_length=maxlen_input)\n",
        "word_embedding_context = Shared_Embedding(input_context)\n",
        "context_embedding = LSTM_encoder(word_embedding_context)\n",
        "\n",
        "word_embedding_answer = Shared_Embedding(input_answer)\n",
        "answer_embedding = LSTM_decoder(word_embedding_answer)\n",
        "\n",
        "merge_layer = concatenate([context_embedding, answer_embedding])\n",
        "out = Dense(int(dictionary_size/2), activation=\"relu\")(merge_layer)\n",
        "out = Dense(dictionary_size, activation=\"softmax\")(out)\n",
        "\n",
        "model = Model(input=[input_context, input_answer], output = [out])\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer=ad)\n",
        "\n",
        "if os.path.isfile(weights_file):\n",
        "    model.load_weights(weights_file)\n",
        "    \n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:21: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(300, kernel_initializer=\"lecun_uniform\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:22: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(300, kernel_initializer=\"lecun_uniform\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:37: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_context (InputLayer)      (None, 50)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_answer (InputLayer)       (None, 50)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_3 (Embedding)         (None, 50, 100)      1000000     input_context[0][0]              \n",
            "                                                                 input_answer[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "lstm_5 (LSTM)                   (None, 300)          481200      embedding_3[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "lstm_6 (LSTM)                   (None, 300)          481200      embedding_3[1][0]                \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_3 (Concatenate)     (None, 600)          0           lstm_5[0][0]                     \n",
            "                                                                 lstm_6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense_5 (Dense)                 (None, 5000)         3005000     concatenate_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_6 (Dense)                 (None, 10000)        50010000    dense_5[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 54,977,400\n",
            "Trainable params: 54,977,400\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "UmuRmGWd_kmf",
        "colab_type": "code",
        "outputId": "237c60f0-be42-4331-d622-7039017a95bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "cell_type": "code",
      "source": [
        "q = _pickle.load(open(padded_context_file, 'rb'))\n",
        "a = _pickle.load(open(padded_answers_file, 'rb'))\n",
        "n_exem, n_words = a.shape\n",
        "\n",
        "print('Number of exemples = %d'%(n_exem))\n",
        "step = int(np.around(n_exem/num_subsets))\n",
        "round_exem = int(step * num_subsets)\n",
        "print(step, round_exem)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of exemples = 5257\n",
            "2628 5256\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "7KpVa5oL_oMD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def print_result(input):\n",
        "  ans_partial = np.zeros((1,maxlen_input))\n",
        "  ans_partial[0,-1] = 0 #index of STA\n",
        "  for k in range(maxlen_input - 1):\n",
        "    ye = model.predict([input, ans_partial])\n",
        "    mp = np.argmax(ye)\n",
        "    ans_partial[0, 0:-1] = ans_partial[0, 1:]\n",
        "    ans_partial[0, -1] = mp\n",
        "  text = ''\n",
        "  for k in ans_partial[0]:\n",
        "    k = k.astype(int)\n",
        "    if k < (dictionary_size-2):\n",
        "      w = vocabulary[k]\n",
        "      text = text + w[0] + ' '\n",
        "  return(text)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "H11kNtzI_rJ0",
        "colab_type": "code",
        "outputId": "add3b03a-bd9b-4e33-82f3-f738da1b6167",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 14545
        }
      },
      "cell_type": "code",
      "source": [
        "x = range(0,Epochs) \n",
        "valid_loss = np.zeros(Epochs)\n",
        "train_loss = np.zeros(Epochs)\n",
        "for m in range(Epochs):\n",
        "  # Loop over training batches due to memory constraints:\n",
        "  for n in range(0,round_exem,step):\n",
        "    q2 = q[n:n+step]\n",
        "    s = q2.shape\n",
        "    count = 0\n",
        "    for i, sent in enumerate(a[n:n+step]):\n",
        "      l = np.where(sent==1)\n",
        "      limit = l[0][0]\n",
        "      count += limit + 1\n",
        "    Q = np.zeros((count,maxlen_input))\n",
        "    A = np.zeros((count,maxlen_input))\n",
        "    Y = np.zeros((count,dictionary_size))\n",
        "    \n",
        "    count = 0\n",
        "    for i, sent in enumerate(a[n:n+step]):\n",
        "      ans_partial = np.zeros((1,maxlen_input))\n",
        "      # Loop over the positions of the current target output (the current output sequence):\n",
        "      l = np.where(sent==1)\n",
        "      limit = l[0][0]\n",
        "      for k in range(1,limit+1):\n",
        "        # Mapping the target output (the next output word) for one-hot codding:\n",
        "        y = np.zeros((1, dictionary_size))\n",
        "        y[0, sent[k]] = 1\n",
        "        # preparing the partial answer to input:\n",
        "        ans_partial[0,-k:] = sent[0:k]\n",
        "        # training the model for one epoch using teacher forcing:\n",
        "        Q[count, :] = q2[i:i+1] \n",
        "        A[count, :] = ans_partial \n",
        "        Y[count, :] = y\n",
        "        count += 1\n",
        "    print('Training epoch: %d, training examples: %d - %d'%(m,n, n + step))\n",
        "    model.fit([Q, A], Y, batch_size=BatchSize, epochs=1)\n",
        "    test_input = q[6:7]\n",
        "    print(print_result(test_input))\n",
        "  model.save_weights(weights_file, overwrite=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training epoch: 0, training examples: 0 - 2628\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Epoch 1/1\n",
            "37934/37934 [==============================] - 110s 3ms/step - loss: 6.5961\n",
            "STA i am i am . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . \n",
            "Training epoch: 0, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38741/38741 [==============================] - 107s 3ms/step - loss: 5.4735\n",
            "STA i am i am not to the END END END END END END END END . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . \n",
            "Training epoch: 1, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "37934/37934 [==============================] - 105s 3ms/step - loss: 5.4486\n",
            "STA i am a END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 1, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38741/38741 [==============================] - 108s 3ms/step - loss: 5.0491\n",
            "STA i am not a END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 2, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "37934/37934 [==============================] - 106s 3ms/step - loss: 5.1714\n",
            "STA i am not a . END . END . END . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . \n",
            "Training epoch: 2, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38741/38741 [==============================] - 103s 3ms/step - loss: 4.8650\n",
            "STA i am not to be a END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 3, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "37934/37934 [==============================] - 97s 3ms/step - loss: 4.9783\n",
            "STA i am not the END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 3, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38741/38741 [==============================] - 97s 3ms/step - loss: 4.7077\n",
            "STA i am not END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 4, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "37934/37934 [==============================] - 102s 3ms/step - loss: 4.8132\n",
            "STA i am not a END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 4, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38741/38741 [==============================] - 101s 3ms/step - loss: 4.5594\n",
            "STA i am not END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 5, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "37934/37934 [==============================] - 102s 3ms/step - loss: 4.6592\n",
            "STA i am not a END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 5, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38741/38741 [==============================] - 103s 3ms/step - loss: 4.4139\n",
            "STA i am not END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 6, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "37934/37934 [==============================] - 102s 3ms/step - loss: 4.5066\n",
            "STA i am not a END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 6, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38741/38741 [==============================] - 104s 3ms/step - loss: 4.2613\n",
            "STA i am not END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 7, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "37934/37934 [==============================] - 99s 3ms/step - loss: 4.3486\n",
            "STA i am not a lot of the END and i am not a . END . END . END . END . END . END . END . END . END . END . END . END . END . END . END . END . END \n",
            "Training epoch: 7, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38741/38741 [==============================] - 102s 3ms/step - loss: 4.0941\n",
            "STA i am not END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 8, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "37934/37934 [==============================] - 106s 3ms/step - loss: 4.1716\n",
            "STA i am not mobile END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 8, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38741/38741 [==============================] - 108s 3ms/step - loss: 3.9073\n",
            "STA i am not mobile END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 9, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "37934/37934 [==============================] - 104s 3ms/step - loss: 3.9740\n",
            "STA i am not mobile END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 9, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38741/38741 [==============================] - 106s 3ms/step - loss: 3.7111\n",
            "STA i am not mobile END END END END END END \n",
            "Training epoch: 10, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "37934/37934 [==============================] - 104s 3ms/step - loss: 3.7627\n",
            "STA i am so happy birthday i can not wait to rock me END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 10, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38741/38741 [==============================] - 103s 3ms/step - loss: 3.5240\n",
            "STA i am not mobile END END END END \n",
            "Training epoch: 11, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "37934/37934 [==============================] - 98s 3ms/step - loss: 3.5597\n",
            "STA i am not mobile END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 11, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38741/38741 [==============================] - 99s 3ms/step - loss: 3.3608\n",
            "STA i am not mobile END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 12, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "37934/37934 [==============================] - 96s 3ms/step - loss: 3.3764\n",
            "STA i am not sure to the and i am giving to death me . END . END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 12, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38741/38741 [==============================] - 102s 3ms/step - loss: 3.2178\n",
            "STA i am not mobile END but i am not going to be END END END END END END END END END END END END \n",
            "Training epoch: 13, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "37934/37934 [==============================] - 99s 3ms/step - loss: 3.2149\n",
            "STA i am not sure to the and i am just curious . END . END . END . END . END . END . END . END . END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 13, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38741/38741 [==============================] - 100s 3ms/step - loss: 3.0899\n",
            "STA i am not mobile END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 14, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "37934/37934 [==============================] - 102s 3ms/step - loss: 3.0736\n",
            "STA i am not sure to buy my dad . END . END . END . END . END . END . END . END . END . END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 14, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38741/38741 [==============================] - 104s 3ms/step - loss: 2.9725\n",
            "STA i am END END END END END END END \n",
            "Training epoch: 15, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "37934/37934 [==============================] - 102s 3ms/step - loss: 2.9491\n",
            "STA i am just going to get my clock damn skin . . . . END END END . END . END . END . END . END END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 15, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "34688/38741 [=========================>....] - ETA: 10s - loss: 2.8605Training epoch: 0, training examples: 0 - 2628\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Epoch 1/1\n",
            "37934/37934 [==============================] - 110s 3ms/step - loss: 6.5961\n",
            "STA i am i am . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . \n",
            "Training epoch: 0, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38741/38741 [==============================] - 107s 3ms/step - loss: 5.4735\n",
            "STA i am i am not to the END END END END END END END END . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . \n",
            "Training epoch: 1, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "37934/37934 [==============================] - 105s 3ms/step - loss: 5.4486\n",
            "STA i am a END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 1, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38741/38741 [==============================] - 108s 3ms/step - loss: 5.0491\n",
            "STA i am not a END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 2, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "37934/37934 [==============================] - 106s 3ms/step - loss: 5.1714\n",
            "STA i am not a . END . END . END . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . \n",
            "Training epoch: 2, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38741/38741 [==============================] - 103s 3ms/step - loss: 4.8650\n",
            "STA i am not to be a END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 3, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "37934/37934 [==============================] - 97s 3ms/step - loss: 4.9783\n",
            "STA i am not the END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 3, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38741/38741 [==============================] - 97s 3ms/step - loss: 4.7077\n",
            "STA i am not END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 4, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "37934/37934 [==============================] - 102s 3ms/step - loss: 4.8132\n",
            "STA i am not a END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 4, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38741/38741 [==============================] - 101s 3ms/step - loss: 4.5594\n",
            "STA i am not END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 5, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "37934/37934 [==============================] - 102s 3ms/step - loss: 4.6592\n",
            "STA i am not a END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 5, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38741/38741 [==============================] - 103s 3ms/step - loss: 4.4139\n",
            "STA i am not END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 6, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "37934/37934 [==============================] - 102s 3ms/step - loss: 4.5066\n",
            "STA i am not a END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 6, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38741/38741 [==============================] - 104s 3ms/step - loss: 4.2613\n",
            "STA i am not END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 7, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "37934/37934 [==============================] - 99s 3ms/step - loss: 4.3486\n",
            "STA i am not a lot of the END and i am not a . END . END . END . END . END . END . END . END . END . END . END . END . END . END . END . END . END \n",
            "Training epoch: 7, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38741/38741 [==============================] - 102s 3ms/step - loss: 4.0941\n",
            "STA i am not END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 8, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "37934/37934 [==============================] - 106s 3ms/step - loss: 4.1716\n",
            "STA i am not mobile END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 8, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38741/38741 [==============================] - 108s 3ms/step - loss: 3.9073\n",
            "STA i am not mobile END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 9, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "37934/37934 [==============================] - 104s 3ms/step - loss: 3.9740\n",
            "STA i am not mobile END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 9, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38741/38741 [==============================] - 106s 3ms/step - loss: 3.7111\n",
            "STA i am not mobile END END END END END END \n",
            "Training epoch: 10, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "37934/37934 [==============================] - 104s 3ms/step - loss: 3.7627\n",
            "STA i am so happy birthday i can not wait to rock me END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 10, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38741/38741 [==============================] - 103s 3ms/step - loss: 3.5240\n",
            "STA i am not mobile END END END END \n",
            "Training epoch: 11, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "37934/37934 [==============================] - 98s 3ms/step - loss: 3.5597\n",
            "STA i am not mobile END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 11, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38741/38741 [==============================] - 99s 3ms/step - loss: 3.3608\n",
            "STA i am not mobile END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 12, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "37934/37934 [==============================] - 96s 3ms/step - loss: 3.3764\n",
            "STA i am not sure to the and i am giving to death me . END . END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 12, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38741/38741 [==============================] - 102s 3ms/step - loss: 3.2178\n",
            "STA i am not mobile END but i am not going to be END END END END END END END END END END END END \n",
            "Training epoch: 13, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "37934/37934 [==============================] - 99s 3ms/step - loss: 3.2149\n",
            "STA i am not sure to the and i am just curious . END . END . END . END . END . END . END . END . END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 13, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38741/38741 [==============================] - 100s 3ms/step - loss: 3.0899\n",
            "STA i am not mobile END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 14, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "37934/37934 [==============================] - 102s 3ms/step - loss: 3.0736\n",
            "STA i am not sure to buy my dad . END . END . END . END . END . END . END . END . END . END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 14, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38741/38741 [==============================] - 104s 3ms/step - loss: 2.9725\n",
            "STA i am END END END END END END END \n",
            "Training epoch: 15, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "37934/37934 [==============================] - 102s 3ms/step - loss: 2.9491\n",
            "STA i am just going to get my clock damn skin . . . . END END END . END . END . END . END . END END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 15, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38741/38741 [==============================] - 104s 3ms/step - loss: 2.8597\n",
            "38741/38741 [==============================] - 104s 3ms/step - loss: 2.8597\n",
            "STA i am gonna edge it . . . . END i was just curious . END . END . END . END . END . END . END . END END END END END END END END END END END END END END END END END END END END \n",
            "STA i am gonna edge it . . . . END i was just curious . END . END . END . END . END . END . END . END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 16, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "Training epoch: 16, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "37934/37934 [==============================] - 97s 3ms/step - loss: 2.8380\n",
            "37934/37934 [==============================] - 97s 3ms/step - loss: 2.8380\n",
            "STA i am writing to be at the of this morning . . . . . . . END . END . END . END . END . END . END . END END END . END END END END END END END END END END END END END \n",
            "STA i am writing to be at the of this morning . . . . . . . END . END . END . END . END . END . END . END END END . END END END END END END END END END END END END END \n",
            "Training epoch: 16, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "Training epoch: 16, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38741/38741 [==============================] - 101s 3ms/step - loss: 2.7542\n",
            "38741/38741 [==============================] - 101s 3ms/step - loss: 2.7542\n",
            "STA i am crying . . . . END i was not a short person worse ago . END i was not END END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "STA i am crying . . . . END i was not a short person worse ago . END i was not END END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 17, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "Training epoch: 17, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "37934/37934 [==============================] - 104s 3ms/step - loss: 2.7348\n",
            "37934/37934 [==============================] - 104s 3ms/step - loss: 2.7348\n",
            "STA i am writing END . . . END i am just curious . END . END . END . END . END . END . END . END . END END END END END END END END END END END END END END END END END END END END \n",
            "STA i am writing END . . . END i am just curious . END . END . END . END . END . END . END . END . END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 17, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "Training epoch: 17, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38741/38741 [==============================] - 105s 3ms/step - loss: 2.6530\n",
            "38741/38741 [==============================] - 105s 3ms/step - loss: 2.6530\n",
            "STA i h8 it . . . . i will be a good day . END END END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "STA i h8 it . . . . i will be a good day . END END END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 18, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "Training epoch: 18, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "37934/37934 [==============================] - 103s 3ms/step - loss: 2.6372\n",
            "37934/37934 [==============================] - 103s 3ms/step - loss: 2.6372\n",
            "STA i am writing END . . . END i think i can not wait to hear you . END END END . END END . END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "STA i am writing END . . . END i think i can not wait to hear you . END END END . END END . END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 18, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "Training epoch: 18, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38741/38741 [==============================] - 105s 3ms/step - loss: 2.5562\n",
            "38741/38741 [==============================] - 105s 3ms/step - loss: 2.5562\n",
            "STA i am \n",
            "STA i am \n",
            "Training epoch: 19, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "Training epoch: 19, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "37934/37934 [==============================] - 103s 3ms/step - loss: 2.5482\n",
            "37934/37934 [==============================] - 103s 3ms/step - loss: 2.5482\n",
            "STA i am writing END END . . . END END . END . END . END . END . END . END . END . END . END END END END END END END END END END END END END END END END END END END END END END \n",
            "STA i am writing END END . . . END END . END . END . END . END . END . END . END . END . END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 19, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "Training epoch: 19, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38741/38741 [==============================] - 103s 3ms/step - loss: 2.4638\n",
            "38741/38741 [==============================] - 103s 3ms/step - loss: 2.4638\n",
            "STA i fully it . . . . END i have not entered . END . END END END . END END END END END END END END END END END END END END END END END END END END END END \n",
            "STA i fully it . . . . END i have not entered . END . END END END . END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 20, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "Training epoch: 20, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "37934/37934 [==============================] - 97s 3ms/step - loss: 2.4621\n",
            "37934/37934 [==============================] - 97s 3ms/step - loss: 2.4621\n",
            "STA i am writing END . . . . END END . END . END . END . . . . . . . . . . . . END . END . END . END . END . END . END . END . END . END . END \n",
            "STA i am writing END . . . . END END . END . END . END . . . . . . . . . . . . END . END . END . END . END . END . END . END . END . END . END \n",
            "Training epoch: 20, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "Training epoch: 20, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38741/38741 [==============================] - 98s 3ms/step - loss: 2.3762\n",
            "38741/38741 [==============================] - 98s 3ms/step - loss: 2.3762\n",
            "STA i am crying . . . END END is worth it . END END END . END END END END END END END END END END END END END END END END END END END END \n",
            "STA i am crying . . . END END is worth it . END END END . END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 21, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "Training epoch: 21, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "37934/37934 [==============================] - 95s 3ms/step - loss: 2.3831\n",
            "37934/37934 [==============================] - 95s 3ms/step - loss: 2.3831\n",
            "STA i am writing END . . . END END . END . END . END . END . END . END . END . END . END END . END END END END END END END END END END END END END END END END END END END END \n",
            "STA i am writing END . . . END END . END . END . END . END . END . END . END . END . END END . END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 21, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "Training epoch: 21, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38741/38741 [==============================] - 100s 3ms/step - loss: 2.2938\n",
            "38741/38741 [==============================] - 100s 3ms/step - loss: 2.2938\n",
            "STA i am crying . . . END END END END END END END END END END END END END END END END END END END END \n",
            "STA i am crying . . . END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 22, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "Training epoch: 22, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "37934/37934 [==============================] - 98s 3ms/step - loss: 2.3056\n",
            "37934/37934 [==============================] - 98s 3ms/step - loss: 2.3056\n",
            "STA i am writing END END . . . END END . END . END . END . END . END . END . END . END END . END END END END END END END END END END END END END END END END END END END END END \n",
            "STA i am writing END END . . . END END . END . END . END . END . END . END . END . END END . END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 22, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "Training epoch: 22, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38741/38741 [==============================] - 100s 3ms/step - loss: 2.2155\n",
            "38741/38741 [==============================] - 100s 3ms/step - loss: 2.2155\n",
            "STA i am END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "STA i am END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 23, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "Training epoch: 23, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "37934/37934 [==============================] - 100s 3ms/step - loss: 2.2331\n",
            "37934/37934 [==============================] - 100s 3ms/step - loss: 2.2331\n",
            "STA i am writing END END . . . END END and i have and fails . . . END END END END END END END END END END END END END END END END END END END END END END END END END END END ? END \n",
            "STA i am writing END END . . . END END and i have and fails . . . END END END END END END END END END END END END END END END END END END END END END END END END END END END ? END \n",
            "Training epoch: 23, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "Training epoch: 23, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38741/38741 [==============================] - 101s 3ms/step - loss: 2.1433\n",
            "38741/38741 [==============================] - 101s 3ms/step - loss: 2.1433\n",
            "STA i am crying . . . END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "STA i am crying . . . END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 24, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "Training epoch: 24, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "37934/37934 [==============================] - 99s 3ms/step - loss: 2.1594\n",
            "37934/37934 [==============================] - 99s 3ms/step - loss: 2.1594\n",
            "STA i am writing END END . . . END END and i have had had a in . END . END END END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "STA i am writing END END . . . END END and i have had had a in . END . END END END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 24, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "Training epoch: 24, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38741/38741 [==============================] - 102s 3ms/step - loss: 2.0748\n",
            "38741/38741 [==============================] - 102s 3ms/step - loss: 2.0748\n",
            "STA i am such a crafty/creative person , but i do not know about it . END END are the same of ! ! END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "STA i am such a crafty/creative person , but i do not know about it . END END are the same of ! ! END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 25, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "Training epoch: 25, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "37934/37934 [==============================] - 101s 3ms/step - loss: 2.0911\n",
            "37934/37934 [==============================] - 101s 3ms/step - loss: 2.0911\n",
            "STA i am writing END . . . END i am cryin . END . END . END . END . END . END . END . END . END END END END END END END END END END END END END END END END END END END END END \n",
            "STA i am writing END . . . END i am cryin . END . END . END . END . END . END . END . END . END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 25, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "Training epoch: 25, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38741/38741 [==============================] - 102s 3ms/step - loss: 2.0041\n",
            "38741/38741 [==============================] - 102s 3ms/step - loss: 2.0041\n",
            "STA pulled END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "STA pulled END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 26, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "Training epoch: 26, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "37934/37934 [==============================] - 103s 3ms/step - loss: 2.0239\n",
            "37934/37934 [==============================] - 103s 3ms/step - loss: 2.0239\n",
            "STA i am writing END . . . END END on the dome . END . END . END . END . END END . END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "STA i am writing END . . . END END on the dome . END . END . END . END . END END . END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 26, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "Training epoch: 26, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38741/38741 [==============================] - 105s 3ms/step - loss: 1.9386\n",
            "38741/38741 [==============================] - 105s 3ms/step - loss: 1.9386\n",
            "STA i am such a crafty/creative person like i was like END END END END \n",
            "STA i am such a crafty/creative person like i was like END END END END \n",
            "Training epoch: 27, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "Training epoch: 27, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "37934/37934 [==============================] - 103s 3ms/step - loss: 1.9658\n",
            "37934/37934 [==============================] - 103s 3ms/step - loss: 1.9658\n",
            "STA i am writing END . . . END END . END . END . END . END . END . END . END . END . END END . END END END END END END END END END END END END END END END END END END END END \n",
            "STA i am writing END . . . END END . END . END . END . END . END . END . END . END . END END . END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 27, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "Training epoch: 27, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38741/38741 [==============================] - 106s 3ms/step - loss: 1.8804\n",
            "38741/38741 [==============================] - 106s 3ms/step - loss: 1.8804\n",
            "STA i am crying . . . END END END is good . END END END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "STA i am crying . . . END END END is good . END END END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 28, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "Training epoch: 28, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "37934/37934 [==============================] - 102s 3ms/step - loss: 1.8980\n",
            "37934/37934 [==============================] - 102s 3ms/step - loss: 1.8980\n",
            "STA i am writing END END . . . END END on the last yr but opening . END . END END . END END END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "STA i am writing END END . . . END END on the last yr but opening . END . END END . END END END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 28, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "Training epoch: 28, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38741/38741 [==============================] - 104s 3ms/step - loss: 1.8223\n",
            "38741/38741 [==============================] - 104s 3ms/step - loss: 1.8223\n",
            "STA i am such a crafty/creative person , but i do not know about it ! END are the new of the ! END are the END END END END END END END END END END END END END END END END END END END END END \n",
            "STA i am such a crafty/creative person , but i do not know about it ! END are the new of the ! END are the END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 29, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "Training epoch: 29, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "37934/37934 [==============================] - 100s 3ms/step - loss: 1.8490\n",
            "37934/37934 [==============================] - 100s 3ms/step - loss: 1.8490\n",
            "STA i am cold but not gonna b at this morning END END . END . END . END . END . END . . . . . . . END END END . END END END END END END END END END END END END END END END END \n",
            "STA i am cold but not gonna b at this morning END END . END . END . END . END . END . . . . . . . END END END . END END END END END END END END END END END END END END END END \n",
            "Training epoch: 29, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "Training epoch: 29, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38741/38741 [==============================] - 101s 3ms/step - loss: 1.7618\n",
            "38741/38741 [==============================] - 101s 3ms/step - loss: 1.7618\n",
            "STA i fully get that . kill . END . END END . END END END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "STA i fully get that . kill . END . END END . END END END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 30, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "Training epoch: 30, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "37934/37934 [==============================] - 97s 3ms/step - loss: 1.7878\n",
            "37934/37934 [==============================] - 97s 3ms/step - loss: 1.7878\n",
            "STA i am writing END END . . . END END on . END . END END . END END END END END END END END END END END END END END END END END END END END END END END END END END END END END . . \n",
            "STA i am writing END END . . . END END on . END . END END . END END END END END END END END END END END END END END END END END END END END END END END END END END END END END . . \n",
            "Training epoch: 30, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "Training epoch: 30, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38741/38741 [==============================] - 102s 3ms/step - loss: 1.7083\n",
            "38741/38741 [==============================] - 102s 3ms/step - loss: 1.7083\n",
            "STA i am such a crafty/creative person , but it is like it is hard about to than END . END END END END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "STA i am such a crafty/creative person , but it is like it is hard about to than END . END END END END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 31, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "Training epoch: 31, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "37934/37934 [==============================] - 100s 3ms/step - loss: 1.7333\n",
            "37934/37934 [==============================] - 100s 3ms/step - loss: 1.7333\n",
            "STA i am cold but can not cover myself because my cat is asleep END END on my blanket :( END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "STA i am cold but can not cover myself because my cat is asleep END END on my blanket :( END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 31, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "Training epoch: 31, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38741/38741 [==============================] - 101s 3ms/step - loss: 1.6572\n",
            "38741/38741 [==============================] - 101s 3ms/step - loss: 1.6572\n",
            "STA i am such a crafty/creative person , that is too big to the in the END in a lot of END END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "STA i am such a crafty/creative person , that is too big to the in the END in a lot of END END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 32, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "Training epoch: 32, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "37934/37934 [==============================] - 101s 3ms/step - loss: 1.6814\n",
            "37934/37934 [==============================] - 101s 3ms/step - loss: 1.6814\n",
            "STA i am cold but not gonna be at the game ! END ! END END ! END END ! END ! END END ! END END END END END END END END END END . . . END END END END END END END END END END END \n",
            "STA i am cold but not gonna be at the game ! END ! END END ! END END ! END ! END END ! END END END END END END END END END END . . . END END END END END END END END END END END \n",
            "Training epoch: 32, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "Training epoch: 32, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38741/38741 [==============================] - 104s 3ms/step - loss: 1.6086\n",
            "38741/38741 [==============================] - 104s 3ms/step - loss: 1.6086\n",
            "STA i fully get that . kill asshole with me ! END i mean you ! END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "STA i fully get that . kill asshole with me ! END i mean you ! END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 33, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "Training epoch: 33, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "37934/37934 [==============================] - 103s 3ms/step - loss: 1.6310\n",
            "37934/37934 [==============================] - 103s 3ms/step - loss: 1.6310\n",
            "STA i am cold but can not cover myself because my cat is asleep END on my blanket :( END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END ? \n",
            "STA i am cold but can not cover myself because my cat is asleep END on my blanket :( END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END ? \n",
            "Training epoch: 33, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "Training epoch: 33, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38741/38741 [==============================] - 105s 3ms/step - loss: 1.5586\n",
            "38741/38741 [==============================] - 105s 3ms/step - loss: 1.5586\n",
            "STA pulled END END END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "STA pulled END END END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 34, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "Training epoch: 34, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "37934/37934 [==============================] - 99s 3ms/step - loss: 1.5829\n",
            "37934/37934 [==============================] - 99s 3ms/step - loss: 1.5829\n",
            "STA i am writing END END . . . END END on the new team . END . END . END . END END END END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "STA i am writing END END . . . END END on the new team . END . END . END . END END END END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 34, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "Training epoch: 34, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38741/38741 [==============================] - 102s 3ms/step - loss: 1.5125\n",
            "38741/38741 [==============================] - 102s 3ms/step - loss: 1.5125\n",
            "STA i am such a crafty/creative person , but i do not know about it ! END are a while ago ! END END END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "STA i am such a crafty/creative person , but i do not know about it ! END are a while ago ! END END END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 35, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "Training epoch: 35, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "37934/37934 [==============================] - 104s 3ms/step - loss: 1.5356\n",
            "37934/37934 [==============================] - 104s 3ms/step - loss: 1.5356\n",
            "STA i am writing END END . . . END END on the dome of . END END END END END END END END END END END END END END END END END END END END END END END END END END END END END . . . .i \n",
            "STA i am writing END END . . . END END on the dome of . END END END END END END END END END END END END END END END END END END END END END END END END END END END END END . . . .i \n",
            "Training epoch: 35, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "Training epoch: 35, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38741/38741 [==============================] - 107s 3ms/step - loss: 1.4659\n",
            "38741/38741 [==============================] - 107s 3ms/step - loss: 1.4659\n",
            "STA stole . was not even . . . . END END END END END END END END END END END END END END END END END END END END END END END END END but every team leaves . . . . \n",
            "STA stole . was not even . . . . END END END END END END END END END END END END END END END END END END END END END END END END END but every team leaves . . . . \n",
            "Training epoch: 36, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "Training epoch: 36, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "37934/37934 [==============================] - 103s 3ms/step - loss: 1.4853\n",
            "37934/37934 [==============================] - 103s 3ms/step - loss: 1.4853\n",
            "STA you are missing another this week . . . END END and and out of . END END END END END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "STA you are missing another this week . . . END END and and out of . END END END END END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 36, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "Training epoch: 36, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38741/38741 [==============================] - 105s 3ms/step - loss: 1.4232\n",
            "38741/38741 [==============================] - 105s 3ms/step - loss: 1.4232\n",
            "STA i am such a crafty/creative person , that is too big up to the this ? END END in the power ? it is all about it ! END END . END END END END END END END END END END END END END END END END END \n",
            "STA i am such a crafty/creative person , that is too big up to the this ? END END in the power ? it is all about it ! END END . END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 37, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "Training epoch: 37, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "37934/37934 [==============================] - 104s 3ms/step - loss: 1.4455\n",
            "37934/37934 [==============================] - 104s 3ms/step - loss: 1.4455\n",
            "STA i am writing a book END . . . END out with and and and of the END END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "STA i am writing a book END . . . END out with and and and of the END END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 37, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "Training epoch: 37, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38741/38741 [==============================] - 105s 3ms/step - loss: 1.3807\n",
            "38741/38741 [==============================] - 105s 3ms/step - loss: 1.3807\n",
            "STA i am also curious how much . END . END . END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "STA i am also curious how much . END . END . END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 38, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "Training epoch: 38, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "37934/37934 [==============================] - 97s 3ms/step - loss: 1.4011\n",
            "37934/37934 [==============================] - 97s 3ms/step - loss: 1.4011\n",
            "STA you are bomb as a fun . END btw you are the \"journalists\" ! END ! END END ! END END ! END END END END . END ! END END END END END END END END END END END END END END END END END END END END \n",
            "STA you are bomb as a fun . END btw you are the \"journalists\" ! END ! END END ! END END ! END END END END . END ! END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 38, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "Training epoch: 38, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38741/38741 [==============================] - 98s 3ms/step - loss: 1.3382\n",
            "38741/38741 [==============================] - 98s 3ms/step - loss: 1.3382\n",
            "STA you obviously do not know how hard it is at the best END . END . END you think they are you . END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "STA you obviously do not know how hard it is at the best END . END . END you think they are you . END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 39, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "Training epoch: 39, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "37934/37934 [==============================] - 101s 3ms/step - loss: 1.3602\n",
            "37934/37934 [==============================] - 101s 3ms/step - loss: 1.3602\n",
            "STA i am writing END END . . . END out of . END END END . END END END END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "STA i am writing END END . . . END out of . END END END . END END END END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 39, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "Training epoch: 39, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38741/38741 [==============================] - 102s 3ms/step - loss: 1.2995\n",
            "38741/38741 [==============================] - 102s 3ms/step - loss: 1.2995\n",
            "STA i am such a crafty/creative person , but when i do not know i am putting from my is death END END END END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "STA i am such a crafty/creative person , but when i do not know i am putting from my is death END END END END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 40, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "Training epoch: 40, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "37934/37934 [==============================] - 99s 3ms/step - loss: 1.3201\n",
            "37934/37934 [==============================] - 99s 3ms/step - loss: 1.3201\n",
            "STA i am writing a book END . . .i do not couples already ? END are you already END . END are you already END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "STA i am writing a book END . . .i do not couples already ? END are you already END . END are you already END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 40, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "Training epoch: 40, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38741/38741 [==============================] - 101s 3ms/step - loss: 1.2599\n",
            "38741/38741 [==============================] - 101s 3ms/step - loss: 1.2599\n",
            "STA i am also curious how much . END . END out . END END END END END END END END END END END END END END END END END END ? ? END END END END END END END END END END END END END END \n",
            "STA i am also curious how much . END . END out . END END END END END END END END END END END END END END END END END END ? ? END END END END END END END END END END END END END END \n",
            "Training epoch: 41, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "Training epoch: 41, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "37934/37934 [==============================] - 101s 3ms/step - loss: 1.2792\n",
            "37934/37934 [==============================] - 101s 3ms/step - loss: 1.2792\n",
            "STA i am writing END END . . . END out of working on and and of of END END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "STA i am writing END END . . . END out of working on and and of of END END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 41, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "Training epoch: 41, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38741/38741 [==============================] - 103s 3ms/step - loss: 1.2245\n",
            "38741/38741 [==============================] - 103s 3ms/step - loss: 1.2245\n",
            "STA i am also curious how much . he ai not any . END they are you it is worth it . END END END END END END END END END END END END END END END END END END END END END END END \n",
            "STA i am also curious how much . he ai not any . END they are you it is worth it . END END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 42, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "Training epoch: 42, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "37934/37934 [==============================] - 122s 3ms/step - loss: 1.2412\n",
            "37934/37934 [==============================] - 122s 3ms/step - loss: 1.2412\n",
            "STA i can not wait to go home and get me to me up to a tattoo shot at a and END END END ! i am giving END END END END END END END END END END END END END END END END END END END END \n",
            "STA i can not wait to go home and get me to me up to a tattoo shot at a and END END END ! i am giving END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 42, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "Training epoch: 42, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38741/38741 [==============================] - 106s 3ms/step - loss: 1.1857\n",
            "38741/38741 [==============================] - 106s 3ms/step - loss: 1.1857\n",
            "STA i am also curious how you come to . . you think it is hard about it ? END are you ! END ! END END END END END END END END END END END END END END END END END END END END END END END \n",
            "STA i am also curious how you come to . . you think it is hard about it ? END are you ! END ! END END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 43, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "Training epoch: 43, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "37934/37934 [==============================] - 103s 3ms/step - loss: 1.2051\n",
            "37934/37934 [==============================] - 103s 3ms/step - loss: 1.2051\n",
            "STA i am writing a book END . . .i do not couples already have these things with it well and the and the of my END END END END END END END END END END END END END END END END END END END END END \n",
            "STA i am writing a book END . . .i do not couples already have these things with it well and the and the of my END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 43, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "Training epoch: 43, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38741/38741 [==============================] - 103s 3ms/step - loss: 1.1570\n",
            "38741/38741 [==============================] - 103s 3ms/step - loss: 1.1570\n",
            "STA i fully get that . kill at a few weeks ago and i can not wait to go see what i can many things and get me END END END END END END END END END END END END END END END END END END END END END END \n",
            "STA i fully get that . kill at a few weeks ago and i can not wait to go see what i can many things and get me END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 44, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "Training epoch: 44, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "37934/37934 [==============================] - 102s 3ms/step - loss: 1.1693\n",
            "37934/37934 [==============================] - 102s 3ms/step - loss: 1.1693\n",
            "STA i am writing a book END . . .i do not couples already ? END are the best . and you are the first lady and got to eat END END END END END END END END END END END END END END END END END END END END \n",
            "STA i am writing a book END . . .i do not couples already ? END are the best . and you are the first lady and got to eat END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 44, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "Training epoch: 44, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38741/38741 [==============================] - 107s 3ms/step - loss: 1.1314\n",
            "38741/38741 [==============================] - 107s 3ms/step - loss: 1.1314\n",
            "STA i am also curious how this . he be in his END , . END END END . END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "STA i am also curious how this . he be in his END , . END END END . END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 45, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "Training epoch: 45, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "37934/37934 [==============================] - 105s 3ms/step - loss: 1.1431\n",
            "37934/37934 [==============================] - 105s 3ms/step - loss: 1.1431\n",
            "STA i have 2 different interviews this week and if both go well i do not know which job END . . . . END END END END END END END END END END END END END END END END END END END END END END END END END my \n",
            "STA i have 2 different interviews this week and if both go well i do not know which job END . . . . END END END END END END END END END END END END END END END END END END END END END END END END END my \n",
            "Training epoch: 45, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "Training epoch: 45, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38741/38741 [==============================] - 108s 3ms/step - loss: 1.1007\n",
            "38741/38741 [==============================] - 108s 3ms/step - loss: 1.1007\n",
            "STA i am such a crafty/creative person , but when do not expect the at the season season off the will be ? END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "STA i am such a crafty/creative person , but when do not expect the at the season season off the will be ? END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 46, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "Training epoch: 46, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "37934/37934 [==============================] - 106s 3ms/step - loss: 1.1173\n",
            "37934/37934 [==============================] - 106s 3ms/step - loss: 1.1173\n",
            "STA i am writing a book END . my is so fun and the of my ! END END END END END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "STA i am writing a book END . my is so fun and the of my ! END END END END END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 46, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "Training epoch: 46, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38741/38741 [==============================] - 106s 3ms/step - loss: 1.0673\n",
            "38741/38741 [==============================] - 106s 3ms/step - loss: 1.0673\n",
            "STA i am also curious how this with sanders complaints of a rigged system &amp ; . END . END END END END END END END END END END END END END END END END END END END END END END END END END END man \n",
            "STA i am also curious how this with sanders complaints of a rigged system &amp ; . END . END END END END END END END END END END END END END END END END END END END END END END END END END END man \n",
            "Training epoch: 47, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "Training epoch: 47, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "37934/37934 [==============================] - 105s 3ms/step - loss: 1.0853\n",
            "37934/37934 [==============================] - 105s 3ms/step - loss: 1.0853\n",
            "STA you are the greatest fri , through thick &amp ; . and i can still have to my to get to . . END END END END END END END END END END END END END END END END END END END END END END END \n",
            "STA you are the greatest fri , through thick &amp ; . and i can still have to my to get to . . END END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 47, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "Training epoch: 47, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38741/38741 [==============================] - 104s 3ms/step - loss: 1.0381\n",
            "38741/38741 [==============================] - 104s 3ms/step - loss: 1.0381\n",
            "STA it is hard but makes here in the i have made with is . END END END . END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "STA it is hard but makes here in the i have made with is . END END END . END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 48, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "Training epoch: 48, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "37934/37934 [==============================] - 102s 3ms/step - loss: 1.0570\n",
            "37934/37934 [==============================] - 102s 3ms/step - loss: 1.0570\n",
            "STA i have 2 different interviews this week and if both go well i do not know which job END . . . . END END END END END END END END END END END END END END END END END END END END END END END END my last \n",
            "STA i have 2 different interviews this week and if both go well i do not know which job END . . . . END END END END END END END END END END END END END END END END END END END END END END END END my last \n",
            "Training epoch: 48, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "Training epoch: 48, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38741/38741 [==============================] - 105s 3ms/step - loss: 1.0057\n",
            "38741/38741 [==============================] - 105s 3ms/step - loss: 1.0057\n",
            "STA i am also curious how it was at his so here . END END with the and . END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "STA i am also curious how it was at his so here . END END with the and . END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 49, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "Training epoch: 49, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "37934/37934 [==============================] - 100s 3ms/step - loss: 1.0158\n",
            "37934/37934 [==============================] - 100s 3ms/step - loss: 1.0158\n",
            "STA i have 2 different interviews this week and if both go well i do not know which job is . . . . END END END END END END END END END END END END END END END END END END END END END END END END END ? \n",
            "STA i have 2 different interviews this week and if both go well i do not know which job is . . . . END END END END END END END END END END END END END END END END END END END END END END END END END ? \n",
            "Training epoch: 49, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "Training epoch: 49, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38741/38741 [==============================] - 103s 3ms/step - loss: 0.9712\n",
            "38741/38741 [==============================] - 103s 3ms/step - loss: 0.9712\n",
            "STA you obviously do not know how hard it is to win at ? END . . you think teams are overrated ? louisville ? END in the acc END END END END END END END END END END END END END END END END END END END \n",
            "STA you obviously do not know how hard it is to win at ? END . . you think teams are overrated ? louisville ? END in the acc END END END END END END END END END END END END END END END END END END END \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ixfIVGjk_y1D",
        "colab_type": "code",
        "outputId": "a59cc8f7-1720-4e73-e487-587bde730d55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1837
        }
      },
      "cell_type": "code",
      "source": [
        "y_pred = list()\n",
        "y_test = list()\n",
        "y_pred_labels = list()\n",
        "y_test_labels = list()\n",
        "for i in range(50):\n",
        "  print('input: %s'%context[i].replace('STA','').replace('END',''))\n",
        "  in_vs = analyzer.polarity_scores(context[i])\n",
        "  y_test.append(in_vs['neg'])\n",
        "  if in_vs['neg'] != 0:\n",
        "    y_test_labels.append('neg')\n",
        "  else:\n",
        "    y_test_labels.append('not_neg')\n",
        "  \n",
        "  output = print_result(q[i:i+1])\n",
        "  output = output.replace('STA','')\n",
        "  output = re.split('END',output)[0]\n",
        "  out_vs = analyzer.polarity_scores(output)\n",
        "  y_pred.append(out_vs['neg'])\n",
        "  if in_vs['neg'] != 0:\n",
        "    y_pred_labels.append('neg')\n",
        "  else:\n",
        "    y_pred_labels.append('not_neg')\n",
        "  print('output: %s'%output)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input:   what is up dadyo when did you get back on twitter ? haha  \n",
            "output:  i really do not \n",
            "input:   literally never about that account , love it .  \n",
            "output:  i really do not \n",
            "input:   about 50 times today . terminal vim user .  \n",
            "output:  i really do not \n",
            "input:   cmd+opt+esc is good but still available via menubar  \n",
            "output:  i really do not \n",
            "input:   i am disgusted  \n",
            "output:  you can get me off ? ? ? ? \n",
            "input:   what a piece of shit  \n",
            "output:  i am drunk \n",
            "input:   yay , you great hunter . ive killed lots of lizards and bugs but never a mouse .  \n",
            "output:  it is not a good idea . \n",
            "input:   and then that mouse had the nerve to try to eat our kibble !  let this be a lesson fur all the other mousies !    \n",
            "output:  i really do not \n",
            "input:   tomorrow  \n",
            "output:  i really do not \n",
            "input:   make sure i have a bed and seat saved next to you !  \n",
            "output:  i really do not \n",
            "input:   wassup shorty .   \n",
            "output:  i really do not \n",
            "input:   appreciate that shorty , you too .   \n",
            "output:  i really do not \n",
            "input:   yea  \n",
            "output:  i really do not \n",
            "input:   gotchu  \n",
            "output:  i really do not \n",
            "input:   good  wby  \n",
            "output:  i really do not \n",
            "input:   that is wassup   \n",
            "output:  i really do not \n",
            "input:   and the dash for cash races too  \n",
            "output:  i really do not \n",
            "input:   do you think this wouldve happened anyway if it was another driver stinkin it up like kyle has recently ?  \n",
            "output:  i really do not \n",
            "input:   what did these niggas say  \n",
            "output:  i need to smoke , this cold turkey not going \n",
            "input:   kenny is wife is white on top of that . u not gonna tell me u put a flag over the color of your skin .  \n",
            "output:  i really do not \n",
            "input:   u cute now  \n",
            "output:  i really do not \n",
            "input:   any hot chicks wanna let me touch their but today ?  \n",
            "output:  i really do not \n",
            "input:   i havent groped that ass in a while i need dat  \n",
            "output:  i really do not \n",
            "input:   so much to do before leaving for d .c . ahhh  \n",
            "output:  i really do not \n",
            "input:    ? ? ?  \n",
            "output:  i really do not \n",
            "input:   white girls singing about \"niggers\" stealing should not have had to face any repercussions ?  . . . . .  \n",
            "output:  you are doing goats ? lol \n",
            "input:   they are not 5 year olds . them repeating what they heard is not an excuse . it does not matter if there was malicious intent .  \n",
            "output:  i am sorry ! i did not snap you . i just have concerned and park ! \n",
            "input:   okay so i know we have had our differences on pineapples on pizza in the past but there is something else that needs attention . . . eggnog  \n",
            "output:  i really do not \n",
            "input:   what in the tits is a coquito  \n",
            "output:  i need to go and pick up my paycheck cause i been broke for the pass 2 weeks \n",
            "input:   just riding these ol dirt roads .  \n",
            "output:  i am not a pretty asian girl like you that can just walk in \n",
            "input:   k babe .  \n",
            "output:  i really do not \n",
            "input:   hate how y'all try to call me \"fake gay\" no . . trust &amp ; believe vagina is wonderful to me , but so is your father . *hint hint* idiots  \n",
            "output:  girl me too ! \n",
            "input:   but just plain dinner is lame . . i like to have something sweet sometimes to . they get to eat what they want , why i can not ?  \n",
            "output:  i am on to text you . my fb brother \n",
            "input:   just one ?  \n",
            "output:  i really do not \n",
            "input:   yeah !  that sounds like the tracy in my timeline .   \n",
            "output:  i really do not \n",
            "input:   dont you all het tired calling half of the country racist ?  \n",
            "output:  doooo itttt . . .dooooo ittttt \n",
            "input:   yes . good night .  \n",
            "output:  i really do not \n",
            "input:   my selfies got bad lighting ? lol  \n",
            "output:  i live in a country founded on the freedom from religion . put down the bible &amp ; pick up the constitution \n",
            "input:   rightttttt lmfao  \n",
            "output:  i really do not \n",
            "input:   yeah i do  \n",
            "output:  i really do not \n",
            "input:   yeah there should be room . usually the desks behind where i sit .  \n",
            "output:  i really do not \n",
            "input:   i will beat your ass   \n",
            "output:  i am still in shock . . .it was unnecessarily brutal \n",
            "input:   i am near the gym fight me pussy  \n",
            "output:  i know if i am going to take tomorrow off there lol \n",
            "input:   alright kiddo . 8 hours .  \n",
            "output:  i really do not \n",
            "input:   i bought you a fudge brownie . it counts as cake .  \n",
            "output:  i really do not \n",
            "input:   i see  \n",
            "output:  i really do not \n",
            "input:   sooo . . . . did everyone on uapb campus tv is loose signal ?  \n",
            "output:  i am \n",
            "input:   well idk but mine did  \n",
            "output:  \n",
            "input:   will you be at the phi delts friday ? ? ?  \n",
            "output:  i really do not \n",
            "input:   yes it is and yes i am !   \n",
            "output:  i really do not \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "dRHcwKCdJlGx",
        "colab_type": "code",
        "outputId": "70c61ae2-308f-4c3b-bc11-0bc48c169db7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "for l in range(len(y_test_labels)):\n",
        "  if y_test_labels[l] != 'neg':\n",
        "     y_test_labels[l]= 'not_neg'\n",
        "print(y_test_labels)\n",
        "for l in range(len(y_pred_labels)):\n",
        "  if y_pred_labels[l] != 'neg':\n",
        "     y_pred_labels[l]= 'not_neg'\n",
        "print(y_pred_labels)\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg']\n",
            "['not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg', 'not_neg']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "p7hfPcD7o_8A",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn import svm, datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.utils.multiclass import unique_labels\n",
        "\n",
        "def plot_confusion_matrix(y_true, y_pred, \n",
        "                          normalize=False,\n",
        "                          title=None,\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    if not title:\n",
        "        if normalize:\n",
        "            title = 'Normalized confusion matrix'\n",
        "        else:\n",
        "            title = 'Confusion matrix, without normalization'\n",
        "\n",
        "    # Compute confusion matrix\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    # Only use the labels that appear in the data\n",
        "    classes = unique_labels(y_true, y_pred)\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    print(cm)\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    ax.figure.colorbar(im, ax=ax)\n",
        "    # We want to show all ticks...\n",
        "    ax.set(xticks=np.arange(cm.shape[1]),\n",
        "           yticks=np.arange(cm.shape[0]),\n",
        "           # ... and label them with the respective list entries\n",
        "           xticklabels=classes, yticklabels=classes,\n",
        "           title=title,\n",
        "           ylabel='True label',\n",
        "           xlabel='Predicted label')\n",
        "\n",
        "    # Rotate the tick labels and set their alignment.\n",
        "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
        "             rotation_mode=\"anchor\")\n",
        "\n",
        "    # Loop over data dimensions and create text annotations.\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i in range(cm.shape[0]):\n",
        "        for j in range(cm.shape[1]):\n",
        "            ax.text(j, i, format(cm[i, j], fmt),\n",
        "                    ha=\"center\", va=\"center\",\n",
        "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "    fig.tight_layout()\n",
        "    return ax"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cQWywjtEodyN",
        "colab_type": "code",
        "outputId": "ad6728b2-39ff-4c3c-c989-963241bc9b51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        }
      },
      "cell_type": "code",
      "source": [
        "plot_confusion_matrix(y_test_labels, y_pred_labels, normalize=False,\n",
        "                      title='Unnormalized confusion matrix')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Confusion matrix, without normalization\n",
            "[[16  0]\n",
            " [ 0 34]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbEAAAGACAYAAAAj9ly5AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
            "text/plain": [
              "<Figure size 576x396 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "zkLc0JhuXRcE",
        "colab_type": "code",
        "outputId": "9f385db0-3b11-4784-d9ad-d04c1eb0fa46",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 473
        }
      },
      "cell_type": "code",
      "source": [
        "plot_confusion_matrix(y_test_labels, y_pred_labels, normalize=True,\n",
        "                      title='Normalized confusion matrix')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Normalized confusion matrix\n",
            "[[1. 0.]\n",
            " [0. 1.]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f596fc20668>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbQAAAGACAYAAADF35f9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\noImIiENQoImIiENQoImIiEP4fyDvJTLF/RWBAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 576x396 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "3BRG0CnU3R0S",
        "colab_type": "code",
        "outputId": "f99db984-280a-4751-d58d-39a76f739e76",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "cell_type": "code",
      "source": [
        "print(y_pred)\n",
        "print(y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.157, 0.157, 0.157, 0.157, 0.0, 0.0, 0.0, 0.157, 0.157, 0.157, 0.157, 0.157, 0.157, 0.157, 0.157, 0.157, 0.157, 0.157, 0.308, 0.157, 0.157, 0.157, 0.157, 0.157, 0.157, 0.0, 0.0, 0.157, 0.0, 0.0, 0.157, 0.0, 0.116, 0.157, 0.157, 0.0, 0.157, 0.0, 0.157, 0.157, 0.157, 0.0, 0.0, 0.157, 0.157, 0.157, 0.0, 0.0, 0.157, 0.157, 0.0, 0.487, 0.157, 0.157, 0.157, 0.157, 0.184, 0.258, 0.157, 0.157, 0.157, 0.0, 0.157, 0.157, 0.157, 0.157, 0.157, 0.157, 0.157, 0.157, 0.157, 0.577, 0.157, 0.157, 0.179, 0.0, 0.157, 0.0, 0.0, 0.157, 0.157, 0.157, 0.157, 0.157, 0.0, 0.157, 0.0, 0.0, 0.157, 0.0, 0.157, 0.157, 0.157, 0.157, 0.157, 0.158, 0.369, 0.157, 0.0, 0.157]\n",
            "[0.0, 0.0, 0.0, 0.0, 0.405, 0.34, 0.122, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.231, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.179, 0.084, 0.0, 0.174, 0.211, 0.0, 0.169, 0.105, 0.0, 0.0, 0.347, 0.0, 0.245, 0.0, 0.0, 0.0, 0.333, 0.206, 0.0, 0.0, 0.0, 0.15, 0.123, 0.0, 0.0, 0.113, 0.124, 0.0, 0.0, 0.0, 0.0, 0.156, 0.289, 0.0, 0.0, 0.0, 0.065, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05, 0.0, 0.0, 0.145, 0.042, 0.0, 0.272, 0.534, 0.0, 0.0, 0.0, 0.0, 0.0, 0.194, 0.0, 0.186, 0.206, 0.0, 0.241, 0.0, 0.0, 0.0, 0.0, 0.0, 0.368, 0.196, 0.0, 0.156, 0.0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hwirtqLWy67i",
        "colab_type": "code",
        "outputId": "0447e2d1-1814-4623-e022-0a04fbce488c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error\n",
        "mean_squared_error(y_test, y_pred)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.03527296"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "metadata": {
        "id": "Gx7S33zsLR7o",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
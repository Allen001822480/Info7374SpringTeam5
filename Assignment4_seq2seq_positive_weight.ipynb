{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment4_seq2seq_positive_weight.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Allen001822480/Info7374SpringTeam5/blob/Assignment4/Assignment4_seq2seq_positive_weight.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "mRlCrT_i-4eg",
        "colab_type": "code",
        "outputId": "3fc7ed6e-d177-4e6a-ebb0-2e744b6abd5f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "import re\n",
        "tw = open('./TwitterLowerAsciiCorpus.txt')\n",
        "twitter = tw.read()\n",
        "data = [d for d in twitter.split('\\n')]\n",
        "data = [d for d in data if d != '']\n",
        "#data = eval('[%s]'%repr(data).replace('[', '').replace(']', ''))\n",
        "data = list(map(lambda x:re.sub(r'^A-Za-z\\d\\s\\,\\.\\!\\?\\'\\\"\\+\\-','',x), data))\n",
        "print(data[0:5])"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[\"what's up dadyo when did you get back on twitter? haha\", \"like 2 weeks ago and it's going as terribly as i remember, but deg is still hilarious so it's ok\", 'literally never about that account, love it.', 'answer me this fellow apple peoples: how many times in the past year have you used the escape key?', 'about 50 times today. terminal vim user.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "f_ZI_NJL_F4d",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "l1 = ['won’t','won\\'t','wouldn’t','wouldn\\'t','’m', '’re', '’ve', '’ll', '’s','’d', 'n’t', '\\'m', '\\'re', '\\'ve', '\\'ll', '\\'s', '\\'d', 'can\\'t', 'n\\'t', 'B: ', 'A: ', ',', ';', '.', '?', '!', ':', '. ?', ',   .', '. ,', 'STA', 'END', 'sta', 'end']\n",
        "l2 = ['will not','will not','would not','would not',' am', ' are', ' have', ' will', ' is', ' had', ' not', ' am', ' are', ' have', ' will', ' is', ' had', 'can not', ' not', '', '', ' ,', ' ;', ' .', ' ?', ' !', ' :', '? ', '.', ',', '', '', '', '']\n",
        "\n",
        "for i, raw_word in enumerate(data):\n",
        "    for j, term in enumerate(l1):\n",
        "        raw_word = raw_word.replace(term,l2[j])\n",
        "    \n",
        "    data[i] = raw_word.lower()\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZJCSozFM_KYM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data = list(map(lambda x:'STA '+x+' END', data))\n",
        "context = data[::2]\n",
        "answers = data[1::2]\n",
        "all = context + answers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_maMcVGXftv9",
        "colab_type": "code",
        "outputId": "73e0e9a0-ec2d-4a2e-fe36-6dacb82cd8ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install vaderSentiment"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting vaderSentiment\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/86/9e/c53e1fc61aac5ee490a6ac5e21b1ac04e55a7c2aba647bb8411c9aadf24e/vaderSentiment-3.2.1-py2.py3-none-any.whl (125kB)\n",
            "\r\u001b[K    8% |██▋                             | 10kB 13.1MB/s eta 0:00:01\r\u001b[K    16% |█████▏                          | 20kB 5.0MB/s eta 0:00:01\r\u001b[K    24% |███████▉                        | 30kB 7.1MB/s eta 0:00:01\r\u001b[K    32% |██████████▍                     | 40kB 4.3MB/s eta 0:00:01\r\u001b[K    40% |█████████████                   | 51kB 5.2MB/s eta 0:00:01\r\u001b[K    48% |███████████████▋                | 61kB 6.1MB/s eta 0:00:01\r\u001b[K    56% |██████████████████▎             | 71kB 6.9MB/s eta 0:00:01\r\u001b[K    65% |████████████████████▉           | 81kB 7.6MB/s eta 0:00:01\r\u001b[K    73% |███████████████████████▍        | 92kB 8.5MB/s eta 0:00:01\r\u001b[K    81% |██████████████████████████      | 102kB 7.0MB/s eta 0:00:01\r\u001b[K    89% |████████████████████████████▋   | 112kB 7.2MB/s eta 0:00:01\r\u001b[K    97% |███████████████████████████████▎| 122kB 9.1MB/s eta 0:00:01\r\u001b[K    100% |████████████████████████████████| 133kB 7.9MB/s \n",
            "\u001b[?25hInstalling collected packages: vaderSentiment\n",
            "Successfully installed vaderSentiment-3.2.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "I62rZb5JfyqB",
        "colab_type": "code",
        "outputId": "ba961d63-20e6-415c-8bc1-f51cbde051dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "context_sentiments = []\n",
        "answers_sentiments = []\n",
        "for sentence in context:\n",
        "  vs = analyzer.polarity_scores(sentence)\n",
        "  context_sentiments.append(vs['pos'])\n",
        "print(context_sentiments)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.188, 0.344, 0.0, 0.178, 0.0, 0.0, 0.232, 0.0, 0.0, 0.35, 0.0, 0.31, 0.0, 0.0, 0.492, 0.0, 0.0, 0.111, 0.0, 0.076, 0.5, 0.0, 0.241, 0.0, 0.0, 0.0, 0.0, 0.055, 0.0, 0.0, 0.0, 0.134, 0.26, 0.0, 0.357, 0.0, 0.651, 0.228, 0.538, 0.423, 0.155, 0.0, 0.0, 0.333, 0.0, 0.0, 0.0, 0.2, 0.0, 0.487, 0.187, 0.203, 0.106, 0.0, 0.0, 0.0, 0.189, 0.0, 0.0, 0.0, 0.0, 0.068, 0.0, 0.0, 0.0, 0.0, 0.237, 0.0, 0.0, 0.0, 0.146, 0.069, 0.0, 0.099, 0.0, 0.183, 0.0, 0.107, 0.0, 0.2, 0.0, 0.0, 0.268, 0.0, 0.0, 0.0, 0.0, 0.287, 0.253, 0.0, 0.0, 0.221, 0.259, 0.25, 0.755, 0.195, 0.11, 0.0, 0.0, 0.106, 0.0, 0.213, 0.0, 0.0, 0.148, 0.157, 0.496, 0.331, 0.323, 0.0, 0.157, 0.0, 0.0, 0.0, 0.0, 0.226, 0.152, 0.0, 0.0, 0.127, 0.0, 0.119, 0.178, 0.0, 0.0, 0.0, 0.492, 0.0, 0.0, 0.42, 0.0, 0.15, 0.591, 0.0, 0.0, 0.176, 0.133, 0.144, 0.211, 0.0, 0.0, 0.356, 0.088, 0.192, 0.266, 0.087, 0.0, 0.319, 0.091, 0.0, 0.0, 0.0, 0.0, 0.647, 0.11, 0.147, 0.0, 0.0, 0.296, 0.359, 0.0, 0.098, 0.0, 0.0, 0.0, 0.0, 0.192, 0.0, 0.492, 0.0, 0.0, 0.344, 0.0, 0.239, 0.483, 0.0, 0.0, 0.148, 0.0, 0.193, 0.0, 0.274, 0.0, 0.0, 0.0, 0.087, 0.0, 0.238, 0.075, 0.497, 0.0, 0.0, 0.0, 0.0, 0.108, 0.286, 0.0, 0.274, 0.0, 0.6, 0.0, 0.0, 0.133, 0.0, 0.0, 0.422, 0.0, 0.0, 0.0, 0.0, 0.115, 0.0, 0.344, 0.487, 0.0, 0.165, 0.0, 0.188, 0.0, 0.583, 0.0, 0.064, 0.0, 0.367, 0.0, 0.0, 0.0, 0.104, 0.0, 0.415, 0.0, 0.0, 0.586, 0.659, 0.0, 0.0, 0.234, 0.0, 0.163, 0.0, 0.102, 0.264, 0.0, 0.249, 0.0, 0.359, 0.529, 0.185, 0.389, 0.0, 0.455, 0.42, 0.0, 0.241, 0.0, 0.34, 0.0, 0.0, 0.492, 0.0, 0.0, 0.0, 0.083, 0.161, 0.0, 0.0, 0.0, 0.333, 0.156, 0.0, 0.349, 0.462, 0.163, 0.283, 0.0, 0.0, 0.0, 0.0, 0.596, 0.0, 0.0, 0.0, 0.465, 0.246, 0.549, 0.0, 0.358, 0.194, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.238, 0.0, 0.05, 0.311, 0.277, 0.0, 0.359, 0.0, 0.208, 0.186, 0.0, 0.256, 0.0, 0.173, 0.121, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.212, 0.0, 0.0, 0.403, 0.299, 0.0, 0.0, 0.103, 0.213, 0.0, 0.0, 0.167, 0.482, 0.0, 0.082, 0.176, 0.214, 0.268, 0.349, 0.2, 0.0, 0.0, 0.249, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.097, 0.1, 0.0, 0.17, 0.0, 0.0, 0.119, 0.0, 0.088, 0.0, 0.191, 0.483, 0.066, 0.167, 0.0, 0.0, 0.241, 0.0, 0.263, 0.333, 0.0, 0.0, 0.093, 0.0, 0.07, 0.113, 0.0, 0.0, 0.049, 0.0, 0.0, 0.184, 0.0, 0.0, 0.0, 0.0, 0.088, 0.0, 0.0, 0.0, 0.0, 0.0, 0.268, 0.0, 0.282, 0.0, 0.141, 0.403, 0.0, 0.098, 0.053, 0.0, 0.21, 0.0, 0.0, 0.182, 0.091, 0.0, 0.0, 0.0, 0.098, 0.0, 0.068, 0.0, 0.271, 0.172, 0.0, 0.185, 0.0, 0.416, 0.0, 0.0, 0.0, 0.0, 0.41, 0.583, 0.0, 0.091, 0.0, 0.151, 0.154, 0.0, 0.151, 0.0, 0.0, 0.2, 0.0, 0.0, 0.388, 0.151, 0.211, 0.184, 0.236, 0.152, 0.117, 0.306, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.231, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.137, 0.119, 0.512, 0.0, 0.223, 0.126, 0.253, 0.0, 0.231, 0.28, 0.0, 0.121, 0.0, 0.388, 0.0, 0.457, 0.168, 0.0, 0.0, 0.24, 0.457, 0.0, 0.068, 0.389, 0.213, 0.318, 0.465, 0.271, 0.0, 0.0, 0.0, 0.165, 0.0, 0.0, 0.439, 0.0, 0.0, 0.0, 0.0, 0.232, 0.204, 0.217, 0.0, 0.0, 0.0, 0.226, 0.0, 0.0, 0.0, 0.0, 0.055, 0.0, 0.0, 0.0, 0.0, 0.0, 0.492, 0.0, 0.0, 0.111, 0.182, 0.0, 0.222, 0.0, 0.0, 0.0, 0.244, 0.0, 0.102, 0.403, 0.531, 0.42, 0.517, 0.0, 0.0, 0.0, 0.0, 0.0, 0.359, 0.541, 0.0, 0.125, 0.164, 0.0, 0.233, 0.0, 0.196, 0.0, 0.0, 0.0, 0.094, 0.217, 0.0, 0.0, 0.231, 0.0, 0.28, 0.0, 0.062, 0.108, 0.624, 0.0, 0.0, 0.299, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.352, 0.234, 0.286, 0.24, 0.586, 0.659, 0.318, 0.127, 0.0, 0.31, 0.5, 0.0, 0.423, 0.0, 0.615, 0.0, 0.0, 0.402, 0.0, 0.0, 0.233, 0.0, 0.0, 0.0, 0.0, 0.267, 0.282, 0.236, 0.0, 0.0, 0.369, 0.111, 0.0, 0.0, 0.0, 0.055, 0.0, 0.0, 0.0, 0.0, 0.627, 0.0, 0.0, 0.108, 0.0, 0.0, 0.374, 0.391, 0.341, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1, 0.0, 0.0, 0.138, 0.286, 0.525, 0.0, 0.0, 0.134, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.457, 0.0, 0.0, 0.0, 0.21, 0.478, 0.237, 0.386, 0.0, 0.0, 0.287, 0.0, 0.0, 0.079, 0.0, 0.0, 0.0, 0.0, 0.36, 0.256, 0.0, 0.0, 0.0, 0.105, 0.0, 0.0, 0.0, 0.0, 0.107, 0.0, 0.137, 0.186, 0.0, 0.314, 0.307, 0.128, 0.352, 0.0, 0.119, 0.399, 0.222, 0.0, 0.0, 0.0, 0.37, 0.583, 0.0, 0.197, 0.259, 0.255, 0.249, 0.0, 0.157, 0.0, 0.294, 0.0, 0.0, 0.0, 0.073, 0.0, 0.202, 0.205, 0.542, 0.0, 0.0, 0.0, 0.231, 0.08, 0.0, 0.0, 0.0, 0.17, 0.483, 0.0, 0.0, 0.129, 0.353, 0.153, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.526, 0.162, 0.39, 0.0, 0.0, 0.0, 0.0, 0.299, 0.0, 0.0, 0.0, 0.13, 0.0, 0.0, 0.113, 0.0, 0.188, 0.1, 0.259, 0.0, 0.0, 0.357, 0.144, 0.216, 0.0, 0.238, 0.15, 0.0, 0.153, 0.187, 0.112, 0.0, 0.0, 0.0, 0.195, 0.281, 0.0, 0.0, 0.0, 0.324, 0.244, 0.344, 0.08, 0.0, 0.244, 0.0, 0.188, 0.0, 0.0, 0.0, 0.0, 0.524, 0.0, 0.306, 0.471, 0.0, 0.185, 0.309, 0.0, 0.0, 0.583, 0.0, 0.375, 0.0, 0.385, 0.245, 0.188, 0.583, 0.0, 0.272, 0.131, 0.0, 0.0, 0.0, 0.171, 0.139, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.294, 0.247, 0.0, 0.141, 0.111, 0.0, 0.041, 0.177, 0.0, 0.0, 0.0, 0.0, 0.0, 0.076, 0.0, 0.106, 0.0, 0.241, 0.206, 0.0, 0.0, 0.0, 0.0, 0.124, 0.0, 0.141, 0.0, 0.0, 0.225, 0.302, 0.259, 0.0, 0.305, 0.594, 0.0, 0.234, 0.309, 0.764, 0.0, 0.0, 0.0, 0.0, 0.0, 0.098, 0.0, 0.129, 0.0, 0.0, 0.0, 0.421, 0.0, 0.222, 0.0, 0.0, 0.0, 0.266, 0.512, 0.533, 0.0, 0.0, 0.151, 0.347, 0.0, 0.73, 0.0, 0.0, 0.0, 0.0, 0.165, 0.282, 0.0, 0.296, 0.0, 0.0, 0.133, 0.0, 0.0, 0.0, 0.158, 0.0, 0.0, 0.097, 0.087, 0.0, 0.0, 0.122, 0.0, 0.0, 0.178, 0.158, 0.0, 0.0, 0.0, 0.0, 0.0, 0.244, 0.0, 0.0, 0.0, 0.136, 0.0, 0.0, 0.0, 0.324, 0.283, 0.263, 0.359, 0.0, 0.0, 0.0, 0.508, 0.142, 0.0, 0.182, 0.0, 0.0, 0.0, 0.273, 0.178, 0.0, 0.259, 0.0, 0.0, 0.643, 0.107, 0.77, 0.0, 0.129, 0.0, 0.0, 0.326, 0.0, 0.676, 0.231, 0.378, 0.156, 0.167, 0.151, 0.0, 0.0, 0.262, 0.0, 0.0, 0.0, 0.117, 0.12, 0.257, 0.2, 0.0, 0.105, 0.303, 0.0, 0.212, 0.0, 0.424, 0.0, 0.308, 0.0, 0.0, 0.0, 0.0, 0.081, 0.0, 0.0, 0.0, 0.22, 0.0, 0.157, 0.0, 0.0, 0.0, 0.0, 0.212, 0.0, 0.0, 0.0, 0.0, 0.121, 0.0, 0.0, 0.294, 0.0, 0.0, 0.326, 0.322, 0.263, 0.355, 0.0, 0.0, 0.0, 0.16, 0.0, 0.631, 0.0, 0.0, 0.151, 0.144, 0.283, 0.0, 0.0, 0.075, 0.116, 0.287, 0.408, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.222, 0.0, 0.337, 0.0, 0.405, 0.0, 0.0, 0.0, 0.0, 0.226, 0.0, 0.215, 0.142, 0.097, 0.0, 0.163, 0.0, 0.0, 0.098, 0.122, 0.213, 0.0, 0.073, 0.067, 0.0, 0.0, 0.0, 0.345, 0.0, 0.252, 0.273, 0.135, 0.36, 0.0, 0.0, 0.0, 0.0, 0.079, 0.0, 0.0, 0.0, 0.492, 0.0, 0.307, 0.298, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.368, 0.0, 0.0, 0.0, 0.0, 0.32, 0.0, 0.0, 0.134, 0.0, 0.071, 0.232, 0.209, 0.0, 0.0, 0.065, 0.084, 0.0, 0.181, 0.612, 0.0, 0.369, 0.172, 0.0, 0.0, 0.0, 0.0, 0.101, 0.0, 0.505, 0.155, 0.0, 0.483, 0.147, 0.0, 0.0, 0.0, 0.093, 0.424, 0.254, 0.0, 0.0, 0.132, 0.0, 0.457, 0.0, 0.0, 0.0, 0.448, 0.434, 0.388, 0.0, 0.0, 0.0, 0.0, 0.0, 0.343, 0.198, 0.209, 0.0, 0.0, 0.0, 0.0, 0.286, 0.0, 0.088, 0.135, 0.29, 0.106, 0.0, 0.222, 0.412, 0.159, 0.321, 0.0, 0.0, 0.0, 0.429, 0.0, 0.0, 0.0, 0.0, 0.0, 0.183, 0.288, 0.534, 0.0, 0.291, 0.0, 0.0, 0.583, 0.0, 0.0, 0.14, 0.134, 0.0, 0.202, 0.322, 0.38, 0.0, 0.0, 0.149, 0.31, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.235, 0.0, 0.115, 0.308, 0.449, 0.0, 0.275, 0.0, 0.333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.105, 0.286, 0.0, 0.13, 0.457, 0.358, 0.144, 0.0, 0.0, 0.278, 0.324, 0.0, 0.245, 0.0, 0.075, 0.139, 0.227, 0.0, 0.0, 0.199, 0.122, 0.0, 0.205, 0.156, 0.0, 0.061, 0.0, 0.328, 0.369, 0.414, 0.0, 0.0, 0.0, 0.0, 0.0, 0.147, 0.0, 0.153, 0.0, 0.222, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.286, 0.0, 0.31, 0.344, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4, 0.326, 0.37, 0.081, 0.0, 0.294, 0.209, 0.358, 0.0, 0.157, 0.533, 0.0, 0.0, 0.0, 0.0, 0.0, 0.468, 0.357, 0.194, 0.195, 0.0, 0.0, 0.0, 0.093, 0.318, 0.0, 0.0, 0.394, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.127, 0.455, 0.191, 0.326, 0.0, 0.0, 0.0, 0.0, 0.0, 0.254, 0.143, 0.0, 0.0, 0.189, 0.0, 0.203, 0.149, 0.0, 0.0, 0.0, 0.0, 0.367, 0.512, 0.0, 0.278, 0.492, 0.0, 0.0, 0.0, 0.0, 0.312, 0.397, 0.216, 0.0, 0.359, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.289, 0.0, 0.189, 0.0, 0.0, 0.273, 0.0, 0.359, 0.128, 0.088, 0.237, 0.0, 0.262, 0.0, 0.0, 0.251, 0.085, 0.0, 0.0, 0.0, 0.0, 0.0, 0.257, 0.0, 0.0, 0.0, 0.0, 0.219, 0.171, 0.364, 0.0, 0.0, 0.286, 0.0, 0.531, 0.24, 0.354, 0.274, 0.0, 0.0, 0.0, 0.367, 0.084, 0.101, 0.186, 0.0, 0.0, 0.243, 0.089, 0.075, 0.149, 0.184, 0.0, 0.107, 0.0, 0.0, 0.0, 0.216, 0.0, 0.323, 0.133, 0.0, 0.348, 0.128, 0.0, 0.521, 0.0, 0.0, 0.0, 0.15, 0.469, 0.588, 0.0, 0.0, 0.247, 0.0, 0.0, 0.156, 0.241, 0.0, 0.0, 0.083, 0.086, 0.0, 0.0, 0.259, 0.0, 0.0, 0.0, 0.0, 0.326, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.158, 0.0, 0.204, 0.0, 0.0, 0.474, 0.0, 0.153, 0.092, 0.291, 0.0, 0.275, 0.378, 0.335, 0.144, 0.109, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.205, 0.401, 0.149, 0.154, 0.742, 0.0, 0.258, 0.0, 0.0, 0.149, 0.0, 0.245, 0.0, 0.0, 0.105, 0.0, 0.0, 0.0, 0.0, 0.0, 0.185, 0.336, 0.0, 0.0, 0.0, 0.0, 0.286, 0.0, 0.23, 0.328, 0.0, 0.223, 0.0, 0.0, 0.0, 0.0, 0.0, 0.187, 0.146, 0.556, 0.0, 0.0, 0.332, 0.0, 0.507, 0.0, 0.649, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.263, 0.0, 0.0, 0.16, 0.0, 0.0, 0.0, 0.0, 0.0, 0.32, 0.0, 0.0, 0.0, 0.0, 0.0, 0.349, 0.367, 0.0, 0.0, 0.0, 0.318, 0.299, 0.318, 0.0, 0.394, 0.326, 0.0, 0.583, 0.286, 0.143, 0.6, 0.0, 0.0, 0.385, 0.0, 0.0, 0.086, 0.068, 0.481, 0.0, 0.095, 0.0, 0.401, 0.0, 0.0, 0.0, 0.17, 0.27, 0.1, 0.37, 0.0, 0.0, 0.404, 0.126, 0.0, 0.0, 0.0, 0.0, 0.104, 0.0, 0.252, 0.19, 0.0, 0.219, 0.0, 0.0, 0.128, 0.0, 0.0, 0.292, 0.0, 0.0, 0.11, 0.392, 0.0, 0.137, 0.0, 0.0, 0.318, 0.241, 0.414, 0.545, 0.0, 0.483, 0.253, 0.278, 0.164, 0.0, 0.109, 0.0, 0.0, 0.0, 0.556, 0.0, 0.152, 0.359, 0.0, 0.0, 0.0, 0.166, 0.0, 0.153, 0.0, 0.068, 0.056, 0.309, 0.0, 0.179, 0.089, 0.172, 0.273, 0.0, 0.0, 0.286, 0.0, 0.0, 0.0, 0.0, 0.623, 0.101, 0.0, 0.0, 0.0, 0.528, 0.0, 0.0, 0.0, 0.099, 0.0, 0.115, 0.373, 0.512, 0.512, 0.204, 0.459, 0.0, 0.036, 0.103, 0.094, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.286, 0.0, 0.0, 0.0, 0.483, 0.0, 0.0, 0.0, 0.338, 0.184, 0.342, 0.609, 0.0, 0.193, 0.092, 0.0, 0.0, 0.416, 0.344, 0.0, 0.35, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.149, 0.0, 0.423, 0.0, 0.423, 0.0, 0.0, 0.0, 0.123, 0.0, 0.0, 0.208, 0.0, 0.0, 0.0, 0.333, 0.0, 0.216, 0.0, 0.0, 0.0, 0.157, 0.0, 0.0, 0.185, 0.0, 0.0, 0.51, 0.0, 0.0, 0.0, 0.189, 0.195, 0.296, 0.266, 0.0, 0.0, 0.0, 0.33, 0.0, 0.0, 0.0, 0.103, 0.149, 0.0, 0.0, 0.338, 0.0, 0.0, 0.0, 0.0, 0.367, 0.0, 0.0, 0.0, 0.0, 0.336, 0.0, 0.559, 0.13, 0.131, 0.177, 0.1, 0.089, 0.0, 0.492, 0.137, 0.412, 0.0, 0.0, 0.0, 0.27, 0.105, 0.0, 0.311, 0.0, 0.129, 0.156, 0.462, 0.0, 0.0, 0.144, 0.457, 0.444, 0.0, 0.483, 0.0, 0.213, 0.0, 0.0, 0.0, 0.0, 0.492, 0.0, 0.0, 0.0, 0.24, 0.0, 0.333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16, 0.219, 0.198, 0.16, 0.0, 0.307, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.272, 0.141, 0.136, 0.099, 0.0, 0.0, 0.0, 0.42, 0.0, 0.513, 0.313, 0.0, 0.289, 0.074, 0.179, 0.0, 0.6, 0.428, 0.077, 0.351, 0.45, 0.0, 0.0, 0.474, 0.0, 0.0, 0.216, 0.458, 0.0, 0.0, 0.31, 0.287, 0.0, 0.0, 0.0, 0.0, 0.0, 0.091, 0.0, 0.157, 0.324, 0.0, 0.492, 0.0, 0.0, 0.382, 0.36, 0.255, 0.124, 0.0, 0.389, 0.119, 0.0, 0.428, 0.168, 0.162, 0.645, 0.203, 0.229, 0.0, 0.492, 0.0, 0.172, 0.0, 0.0, 0.304, 0.0, 0.0, 0.376, 0.273, 0.0, 0.388, 0.0, 0.161, 0.0, 0.0, 0.157, 0.244, 0.0, 0.112, 0.0, 0.483, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.24, 0.216, 0.0, 0.0, 0.0, 0.0, 0.116, 0.0, 0.0, 0.0, 0.378, 0.0, 0.0, 0.164, 0.0, 0.389, 0.0, 0.245, 0.0, 0.0, 0.0, 0.0, 0.0, 0.201, 0.205, 0.22, 0.0, 0.0, 0.253, 0.25, 0.375, 0.0, 0.28, 0.0, 0.0, 0.161, 0.136, 0.25, 0.101, 0.0, 0.0, 0.0, 0.0, 0.0, 0.333, 0.506, 0.4, 0.0, 0.0, 0.0, 0.213, 0.219, 0.409, 0.0, 0.0, 0.0, 0.0, 0.0, 0.189, 0.156, 0.071, 0.412, 0.152, 0.0, 0.0, 0.0, 0.404, 0.0, 0.0, 0.0, 0.259, 0.195, 0.252, 0.0, 0.213, 0.0, 0.0, 0.0, 0.0, 0.383, 0.0, 0.0, 0.494, 0.333, 0.0, 0.0, 0.0, 0.116, 0.0, 0.0, 0.317, 0.0, 0.0, 0.0, 0.115, 0.184, 0.367, 0.0, 0.242, 0.0, 0.0, 0.483, 0.0, 0.155, 0.247, 0.39, 0.453, 0.0, 0.259, 0.0, 0.136, 0.0, 0.0, 0.255, 0.156, 0.192, 0.134, 0.197, 0.145, 0.0, 0.289, 0.188, 0.302, 0.422, 0.271, 0.167, 0.197, 0.0, 0.367, 0.0, 0.375, 0.423, 0.635, 0.0, 0.264, 0.0, 0.325, 0.0, 0.39, 0.0, 0.074, 0.179, 0.16, 0.0, 0.118, 0.0, 0.0, 0.151, 0.073, 0.0, 0.279, 0.561, 0.092, 0.0, 0.0, 0.094, 0.0, 0.0, 0.436, 0.512, 0.0, 0.0, 0.13, 0.114, 0.264, 0.073, 0.144, 0.157, 0.135, 0.375, 0.0, 0.195, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.259, 0.3, 0.0, 0.439, 0.0, 0.401, 0.24, 0.354, 0.271, 0.117, 0.423, 0.0, 0.0, 0.0, 0.0, 0.246, 0.211, 0.0, 0.111, 0.0, 0.0, 0.245, 0.0, 0.0, 0.313, 0.104, 0.225, 0.0, 0.293, 0.0, 0.0, 0.0, 0.0, 0.208, 0.574, 0.0, 0.0, 0.0, 0.0, 0.0, 0.457, 0.235, 0.244, 0.0, 0.401, 0.0, 0.101, 0.0, 0.172, 0.0, 0.127, 0.169, 0.237, 0.329, 0.0, 0.524, 0.0, 0.0, 0.115, 0.224, 0.209, 0.576, 0.0, 0.0, 0.065, 0.084, 0.095, 0.0, 0.37, 0.31, 0.375, 0.207, 0.234, 0.112, 0.565, 0.07, 0.34, 0.528, 0.0, 0.0, 0.167, 0.0, 0.406, 0.293, 0.085, 0.0, 0.0, 0.0, 0.0, 0.232, 0.0, 0.055, 0.0, 0.0, 0.531, 0.307, 0.097, 0.0, 0.0, 0.0, 0.0, 0.174, 0.433, 0.246, 0.282, 0.104, 0.0, 0.333, 0.196, 0.276, 0.267, 0.0, 0.0, 0.134, 0.317, 0.0, 0.0, 0.0, 0.0, 0.218, 0.143, 0.266, 0.0, 0.0, 0.206, 0.222, 0.251, 0.0, 0.0, 0.0, 0.0, 0.574, 0.444, 0.169, 0.0, 0.318, 0.205, 0.394, 0.137, 0.0, 0.0, 0.0, 0.142, 0.259, 0.104, 0.0, 0.0, 0.43, 0.0, 0.0, 0.184, 0.318, 0.0, 0.0, 0.389, 0.412, 0.0, 0.343, 0.0, 0.353, 0.0, 0.353, 0.239, 0.425, 0.0, 0.0, 0.0, 0.266, 0.0, 0.412, 0.556, 0.0, 0.0, 0.171, 0.0, 0.0, 0.0, 0.0, 0.0, 0.152, 0.15, 0.0, 0.0, 0.0, 0.0, 0.0, 0.346, 0.0, 0.0, 0.186, 0.244, 0.0, 0.0, 0.274, 0.0, 0.0, 0.507, 0.417, 0.153, 0.0, 0.294, 0.375, 0.241, 0.423, 0.0, 0.221, 0.0, 0.097, 0.0, 0.0, 0.0, 0.255, 0.27, 0.358, 0.392, 0.204, 0.379, 0.172, 0.351, 0.219, 0.047, 0.222, 0.09, 0.193, 0.0, 0.273, 0.458, 0.155, 0.149, 0.0, 0.0, 0.0, 0.115, 0.0, 0.0, 0.293, 0.333, 0.336, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.21, 0.175, 0.191, 0.25, 0.0, 0.0, 0.196, 0.0, 0.0, 0.189, 0.239, 0.492, 0.286, 0.249, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.411, 0.0, 0.116, 0.114, 0.0, 0.5, 0.275, 0.059, 0.17, 0.0, 0.0, 0.124, 0.0, 0.0, 0.0, 0.245, 0.31, 0.0, 0.399, 0.161, 0.134, 0.186, 0.293, 0.0, 0.0, 0.0, 0.294, 0.194, 0.0, 0.122, 0.0, 0.0, 0.369, 0.0, 0.0, 0.153, 0.472, 0.198, 0.257, 0.242, 0.0, 0.0, 0.576, 0.0, 0.0, 0.0, 0.0, 0.111, 0.0, 0.052, 0.125, 0.151, 0.0, 0.08, 0.231, 0.104, 0.0, 0.0, 0.0, 0.0, 0.0, 0.359, 0.0, 0.0, 0.0, 0.098, 0.0, 0.0, 0.599, 0.0, 0.078, 0.0, 0.0, 0.0, 0.0, 0.239, 0.382, 0.0, 0.111, 0.0, 0.0, 0.412, 0.0, 0.126, 0.067, 0.0, 0.0, 0.141, 0.144, 0.0, 0.318, 0.286, 0.0, 0.0, 0.257, 0.0, 0.0, 0.0, 0.0, 0.279, 0.382, 0.449, 0.0, 0.441, 0.0, 0.0, 0.108, 0.0, 0.0, 0.158, 0.531, 0.508, 0.0, 0.574, 0.293, 0.292, 0.0, 0.0, 0.508, 0.0, 0.293, 0.0, 0.268, 0.0, 0.0, 0.229, 0.0, 0.492, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13, 0.272, 0.162, 0.333, 0.512, 0.0, 0.0, 0.0, 0.0, 0.174, 0.0, 0.389, 0.0, 0.0, 0.186, 0.0, 0.474, 0.0, 0.315, 0.0, 0.163, 0.0, 0.256, 0.269, 0.217, 0.252, 0.0, 0.271, 0.0, 0.205, 0.231, 0.322, 0.0, 0.0, 0.0, 0.0, 0.0, 0.084, 0.271, 0.0, 0.0, 0.0, 0.0, 0.0, 0.358, 0.252, 0.0, 0.182, 0.0, 0.091, 0.115, 0.182, 0.565, 0.259, 0.612, 0.308, 0.354, 0.282, 0.0, 0.0, 0.0, 0.722, 0.0, 0.0, 0.0, 0.506, 0.243, 0.312, 0.0, 0.285, 0.346, 0.0, 0.0, 0.0, 0.0, 0.39, 0.444, 0.104, 0.0, 0.0, 0.0, 0.088, 0.0, 0.0, 0.0, 0.103, 0.483, 0.0, 0.182, 0.0, 0.0, 0.0, 0.0, 0.24, 0.0, 0.149, 0.079, 0.0, 0.375, 0.363, 0.0, 0.252, 0.0, 0.395, 0.155, 0.112, 0.0, 0.078, 0.204, 0.102, 0.0, 0.0, 0.099, 0.125, 0.0, 0.192, 0.0, 0.0, 0.0, 0.0, 0.0, 0.216, 0.238, 0.0, 0.182, 0.326, 0.0, 0.232, 0.204, 0.505, 0.0, 0.171, 0.0, 0.0, 0.0, 0.0, 0.126, 0.318, 0.0, 0.474, 0.0, 0.0, 0.357, 0.244, 0.293, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13, 0.063, 0.0, 0.0, 0.365, 0.0, 0.343, 0.284, 0.0, 0.15, 0.0, 0.0, 0.0, 0.0, 0.071, 0.0, 0.275, 0.0, 0.0, 0.245, 0.292, 0.0, 0.198, 0.0, 0.0, 0.0, 0.203, 0.318, 0.162, 0.0, 0.0, 0.0, 0.183, 0.0, 0.412, 0.371, 0.0, 0.423, 0.0, 0.0, 0.0, 0.512, 0.0, 0.237, 0.386, 0.14, 0.348, 0.19, 0.251, 0.0, 0.0, 0.0, 0.286, 0.0, 0.0, 0.0, 0.127, 0.0, 0.0, 0.294, 0.61, 0.137, 0.186, 0.0, 0.0, 0.0, 0.114, 0.0, 0.0, 0.322, 0.0, 0.23, 0.0, 0.0, 0.0, 0.0, 0.286, 0.565, 0.412, 0.0, 0.0, 0.0, 0.211, 0.205, 0.189, 0.306, 0.286, 0.239, 0.0, 0.272, 0.132, 0.2, 0.354, 0.233, 0.0, 0.0, 0.244, 0.0, 0.08, 0.188, 0.13, 0.195, 0.0, 0.0, 0.0, 0.333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.129, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.196, 0.0, 0.0, 0.0, 0.0, 0.25, 0.327, 0.226, 0.0, 0.362, 0.0, 0.249, 0.0, 0.0, 0.197, 0.0, 0.203, 0.318, 0.465, 0.262, 0.257, 0.0, 0.357, 0.0, 0.439, 0.0, 0.359, 0.189, 0.29, 0.412, 0.0, 0.196, 0.231, 0.288, 0.0, 0.0, 0.0, 0.059, 0.17, 0.257, 0.0, 0.574, 0.0, 0.362, 0.0, 0.0, 0.0, 0.0, 0.302, 0.0, 0.16, 0.0, 0.271, 0.333, 0.255, 0.087, 0.172, 0.0, 0.0, 0.069, 0.132, 0.13, 0.181, 0.088, 0.31, 0.0, 0.355, 0.0, 0.0, 0.212, 0.0, 0.247, 0.133, 0.403, 0.146, 0.337, 0.297, 0.083, 0.0, 0.0, 0.571, 0.0, 0.0, 0.0, 0.0, 0.0, 0.081, 0.208, 0.0, 0.195, 0.281, 0.101, 0.0, 0.0, 0.125, 0.145, 0.081, 0.0, 0.0, 0.0, 0.324, 0.0, 0.091, 0.182, 0.0, 0.0, 0.299, 0.246, 0.0, 0.349, 0.195, 0.11, 0.0, 0.189, 0.0, 0.0, 0.077, 0.0, 0.0, 0.231, 0.0, 0.128, 0.36, 0.279, 0.106, 0.0, 0.721, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.126, 0.434, 0.206, 0.0, 0.0, 0.351, 0.0, 0.0, 0.205, 0.0, 0.12, 0.0, 0.633, 0.0, 0.0, 0.213, 0.089, 0.444, 0.0, 0.0, 0.0, 0.326, 0.347, 0.453, 0.149, 0.183, 0.0, 0.5, 0.0, 0.318, 0.0, 0.37, 0.0, 0.0, 0.0, 0.511, 0.0, 0.325, 0.0, 0.0, 0.0, 0.0, 0.592, 0.0, 0.0, 0.755, 0.374, 0.0, 0.215, 0.0, 0.0, 0.318, 0.0, 0.091, 0.146, 0.218, 0.0, 0.167, 0.0, 0.0, 0.0, 0.31, 0.213, 0.0, 0.0, 0.0, 0.0, 0.198, 0.285, 0.0, 0.0, 0.0, 0.0, 0.213, 0.049, 0.0, 0.0, 0.0, 0.474, 0.315, 0.318, 0.0, 0.0, 0.0, 0.317, 0.0, 0.0, 0.0, 0.0, 0.0, 0.362, 0.0, 0.299, 0.172, 0.0, 0.0, 0.143, 0.0, 0.0, 0.0, 0.0, 0.0, 0.238, 0.077, 0.0, 0.206, 0.0, 0.0, 0.25, 0.333, 0.318, 0.094, 0.0, 0.0, 0.0, 0.405, 0.0, 0.359, 0.0, 0.0, 0.183, 0.158, 0.0, 0.0, 0.266, 0.381, 0.462, 0.146, 0.0, 0.0, 0.0, 0.0, 0.0, 0.317, 0.404, 0.242, 0.14, 0.0, 0.0, 0.0, 0.0, 0.0, 0.282, 0.0, 0.0, 0.0, 0.286, 0.0, 0.0, 0.0, 0.121, 0.118, 0.265, 0.134, 0.231, 0.075, 0.0, 0.087, 0.136, 0.0, 0.0, 0.209, 0.184, 0.0, 0.136, 0.243, 0.438, 0.38, 0.2, 0.0, 0.0, 0.0, 0.506, 0.0, 0.132, 0.0, 0.0, 0.0, 0.107, 0.0, 0.07, 0.383, 0.0, 0.079, 0.0, 0.082, 0.0, 0.0, 0.141, 0.056, 0.0, 0.14, 0.0, 0.0, 0.467, 0.0, 0.0, 0.399, 0.097, 0.137, 0.215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.579, 0.508, 0.409, 0.187, 0.0, 0.0, 0.299, 0.0, 0.297, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.221, 0.153, 0.174, 0.0, 0.483, 0.351, 0.0, 0.167, 0.0, 0.076, 0.0, 0.0, 0.31, 0.0, 0.0, 0.0, 0.0, 0.0, 0.095, 0.287, 0.423, 0.0, 0.0, 0.064, 0.063, 0.0, 0.0, 0.0, 0.111, 0.0, 0.221, 0.185, 0.574, 0.0, 0.0, 0.0, 0.337, 0.0, 0.189, 0.188, 0.218, 0.439, 0.167, 0.226, 0.152, 0.0, 0.0, 0.056, 0.191, 0.132, 0.0, 0.0, 0.661, 0.0, 0.0, 0.0, 0.209, 0.483, 0.0, 0.415, 0.0, 0.0, 0.0, 0.139, 0.0, 0.286, 0.119, 0.0, 0.0, 0.084, 0.0, 0.0, 0.0, 0.095, 0.0, 0.14, 0.086, 0.0, 0.34, 0.14, 0.359, 0.0, 0.152, 0.115, 0.0, 0.161, 0.197, 0.574, 0.098, 0.197, 0.363, 0.483, 0.0, 0.086, 0.0, 0.367, 0.423, 0.156, 0.0, 0.319, 0.343, 0.357, 0.224, 0.237, 0.221, 0.0, 0.0, 0.115, 0.25, 0.184, 0.348, 0.128, 0.156, 0.269, 0.09, 0.0, 0.574, 0.085, 0.0, 0.167, 0.122, 0.583, 0.0, 0.457, 0.0, 0.28, 0.0, 0.0, 0.0, 0.0, 0.259, 0.0, 0.255, 0.0, 0.159, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.096, 0.133, 0.0, 0.311, 0.42, 0.0, 0.0, 0.0, 0.0, 0.44, 0.0, 0.0, 0.132, 0.388, 0.495, 0.202, 0.0, 0.0, 0.0, 0.13, 0.157, 0.439, 0.391, 0.239, 0.101, 0.0, 0.4, 0.275, 0.0, 0.0, 0.0, 0.084, 0.0, 0.119, 0.0, 0.0, 0.0, 0.0, 0.0, 0.493, 0.0, 0.0, 0.0, 0.061, 0.0, 0.09, 0.0, 0.17, 0.27, 0.0, 0.0, 0.131, 0.286, 0.146, 0.246, 0.0, 0.349, 0.127, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.198, 0.116, 0.0, 0.0, 0.455, 0.522, 0.178, 0.0, 0.0, 0.115, 0.545, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.192, 0.0, 0.188, 0.227, 0.374, 0.198, 0.0, 0.306, 0.216, 0.0, 0.0, 0.31, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.375, 0.389, 0.0, 0.132, 0.0, 0.423, 0.0, 0.111, 0.168, 0.0, 0.0, 0.188, 0.263, 0.0, 0.0, 0.155, 0.0, 0.139, 0.0, 0.0, 0.0, 0.0, 0.0, 0.172, 0.0, 0.597, 0.355, 0.214, 0.512, 0.211, 0.0, 0.279, 0.0, 0.0, 0.17, 0.375, 0.0, 0.548, 0.214, 0.0, 0.0, 0.5, 0.406, 0.0, 0.42, 0.589, 0.244, 0.237, 0.155, 0.0, 0.0, 0.0, 0.0, 0.0, 0.223, 0.0, 0.0, 0.113, 0.13, 0.0, 0.42, 0.0, 0.141, 0.0, 0.0, 0.167, 0.0, 0.0, 0.0, 0.0, 0.0, 0.124, 0.0, 0.252, 0.0, 0.182, 0.0, 0.0, 0.0, 0.172, 0.143, 0.785, 0.157, 0.135, 0.375, 0.0, 0.0, 0.247, 0.18, 0.187, 0.084, 0.391, 0.341, 0.377, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.14, 0.262, 0.0, 0.053, 0.184, 0.0, 0.0, 0.139, 0.294, 0.333, 0.318, 0.094, 0.0, 0.336, 0.529, 0.0, 0.286, 0.231, 0.288, 0.203, 0.0, 0.346, 0.118, 0.224, 0.0, 0.0, 0.0, 0.111, 0.216, 0.413, 0.0, 0.0, 0.15, 0.091, 0.106, 0.0, 0.231, 0.0, 0.236, 0.0, 0.0, 0.314, 0.494, 0.0, 0.344, 0.0, 0.183, 0.0, 0.0, 0.0, 0.0, 0.305, 0.0, 0.0, 0.0, 0.28, 0.133, 0.0, 0.229, 0.0, 0.0, 0.0, 0.236, 0.294, 0.127, 0.0, 0.0, 0.126, 0.0, 0.162, 0.22, 0.0, 0.0, 0.0, 0.255, 0.0, 0.0, 0.0, 0.0, 0.0, 0.264, 0.153, 0.212, 0.0, 0.0, 0.0, 0.0, 0.0, 0.524, 0.111, 0.182, 0.139, 0.158, 0.222, 0.528, 0.233, 0.0, 0.0, 0.0, 0.139, 0.441, 0.216, 0.071, 0.0, 0.391, 0.239, 0.125, 0.347, 0.6, 0.483, 0.244, 0.0, 0.0, 0.0, 0.0, 0.111, 0.0, 0.0, 0.0, 0.0, 0.245, 0.333, 0.219, 0.192, 0.355, 0.253, 0.147, 0.33, 0.251, 0.0, 0.0, 0.238, 0.106, 0.0, 0.135, 0.207, 0.0, 0.182, 0.483, 0.139, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.123, 0.08, 0.0, 0.0, 0.0, 0.093, 0.0, 0.255, 0.0, 0.306, 0.252, 0.229, 0.285, 0.0, 0.302, 0.492, 0.0, 0.386, 0.097, 0.0, 0.0, 0.263, 0.044, 0.211, 0.0, 0.193, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.384, 0.516, 0.152, 0.15, 0.194, 0.0, 0.286, 0.342, 0.237, 0.0, 0.176, 0.0, 0.148, 0.133, 0.0, 0.0, 0.0, 0.0, 0.0, 0.667, 0.443, 0.2, 0.331, 0.0, 0.2, 0.518, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.466, 0.0, 0.0, 0.142, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.391, 0.0, 0.483, 0.483, 0.0, 0.0, 0.374, 0.423, 0.132, 0.0, 0.735, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.397, 0.36, 0.0, 0.417, 0.0, 0.142, 0.122, 0.157, 0.185, 0.0, 0.086, 0.0, 0.0, 0.189, 0.0, 0.169, 0.083, 0.176, 0.323, 0.0, 0.0, 0.0, 0.223, 0.0, 0.0, 0.0, 0.0, 0.135, 0.0, 0.0, 0.0, 0.0, 0.233, 0.524, 0.209, 0.0, 0.0, 0.0, 0.421, 0.203, 0.0, 0.0, 0.0, 0.0, 0.21, 0.432, 0.0, 0.148, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.478, 0.128, 0.492, 0.0, 0.649, 0.0, 0.07, 0.0, 0.0, 0.0, 0.376, 0.353, 0.169, 0.237, 0.329, 0.273, 0.0, 0.0, 0.0, 0.135, 0.0, 0.149, 0.588, 0.0, 0.0, 0.235, 0.042, 0.271, 0.105, 0.0, 0.0, 0.271, 0.0, 0.253, 0.25, 0.5, 0.26, 0.0, 0.0, 0.0, 0.066, 0.0, 0.388, 0.0, 0.0, 0.0, 0.157, 0.196, 0.0, 0.15, 0.078, 0.0, 0.0, 0.474, 0.0, 0.0, 0.0, 0.0, 0.126, 0.317, 0.211, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07, 0.0, 0.0, 0.0, 0.0, 0.35, 0.187, 0.576, 0.0, 0.0, 0.0, 0.237, 0.0, 0.16, 0.184, 0.0, 0.333, 0.341, 0.091, 0.0, 0.0, 0.315, 0.0, 0.244, 0.308, 0.354, 0.388, 0.111, 0.0, 0.136, 0.263, 0.106, 0.0, 0.072, 0.0, 0.076, 0.0, 0.206, 0.0, 0.0, 0.0, 0.057, 0.0, 0.114, 0.089, 0.0, 0.128, 0.0, 0.0, 0.655, 0.286, 0.249, 0.172, 0.257, 0.0, 0.098, 0.0, 0.0, 0.16, 0.0, 0.118, 0.0, 0.32, 0.0, 0.0, 0.0, 0.579, 0.411, 0.271, 0.113, 0.0, 0.119, 0.524, 0.0, 0.0, 0.238, 0.212, 0.0, 0.326, 0.367, 0.398, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.275, 0.0, 0.244, 0.326, 0.221, 0.394, 0.0, 0.0, 0.0, 0.0, 0.435, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.299, 0.0, 0.0, 0.0, 0.152, 0.193, 0.0, 0.0, 0.0, 0.145, 0.186, 0.0, 0.271, 0.167, 0.236, 0.165, 0.194, 0.0, 0.0, 0.0, 0.0, 0.412, 0.225, 0.125, 0.204, 0.0, 0.311, 0.335, 0.0, 0.385, 0.0, 0.0, 0.0, 0.061, 0.0, 0.0, 0.206, 0.077, 0.0, 0.061, 0.0, 0.084, 0.492, 0.0, 0.207, 0.355, 0.571, 0.115, 0.206, 0.332, 0.0, 0.281, 0.355, 0.098, 0.0, 0.0, 0.419, 0.0, 0.0, 0.0, 0.0, 0.58, 0.429, 0.587, 0.0, 0.336, 0.541, 0.405, 0.483, 0.31, 0.301, 0.506, 0.322, 0.28, 0.714, 0.0, 0.0, 0.0, 0.787, 0.0, 0.464, 0.0, 0.0, 0.0, 0.0, 0.088, 0.109, 0.149, 0.142, 0.0, 0.0, 0.244, 0.0, 0.213, 0.0, 0.094, 0.196, 0.0, 0.219, 0.09, 0.0, 0.0, 0.0, 0.0, 0.501, 0.297, 0.267, 0.369, 0.0, 0.097, 0.0, 0.0, 0.142, 0.0, 0.0, 0.22, 0.164, 0.515, 0.241, 0.233, 0.243, 0.0, 0.429, 0.241, 0.0, 0.0, 0.0, 0.323, 0.0, 0.128, 0.0, 0.0, 0.0, 0.178, 0.0, 0.427, 0.162, 0.0, 0.0, 0.098, 0.282, 0.095, 0.115, 0.367, 0.0, 0.0, 0.178, 0.084, 0.0, 0.412, 0.435, 0.423, 0.0, 0.0, 0.458, 0.0, 0.21, 0.359, 0.0, 0.0, 0.0, 0.052, 0.0, 0.0, 0.0, 0.0, 0.196, 0.0, 0.055, 0.0, 0.0, 0.0, 0.0, 0.234, 0.095, 0.0, 0.74, 0.234, 0.0, 0.0, 0.0, 0.604, 0.0, 0.0, 0.385, 0.0, 0.0, 0.0, 0.527, 0.281, 0.0, 0.0, 0.0, 0.0, 0.0, 0.097, 0.0, 0.167, 0.0, 0.0, 0.0, 0.231, 0.0, 0.0, 0.0, 0.093, 0.0, 0.0, 0.0, 0.395, 0.0, 0.554, 0.0, 0.0, 0.0, 0.0, 0.318, 0.0, 0.0, 0.0, 0.0, 0.061, 0.0, 0.397, 0.335, 0.089, 0.209, 0.0, 0.0, 0.086, 0.344, 0.0, 0.367, 0.0, 0.483, 0.225, 0.0, 0.412, 0.556, 0.208, 0.0, 0.353, 0.0, 0.485, 0.397, 0.606, 0.0, 0.0, 0.304, 0.26, 0.0, 0.0, 0.0, 0.121, 0.137, 0.249, 0.0, 0.0, 0.277, 0.0, 0.0, 0.0, 0.0, 0.0, 0.219, 0.0, 0.405, 0.204, 0.0, 0.0, 0.0, 0.35, 0.0, 0.401, 0.15, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.167, 0.147, 0.097, 0.2, 0.0, 0.367, 0.423, 0.0, 0.0, 0.165, 0.0, 0.0, 0.0, 0.386, 0.262, 0.07, 0.29, 0.155, 0.174, 0.0, 0.0, 0.287, 0.255, 0.27, 0.311, 0.116, 0.238, 0.328, 0.125, 0.0, 0.0, 0.136, 0.0, 0.091, 0.151, 0.0, 0.0, 0.0, 0.0, 0.286, 0.0, 0.0, 0.404, 0.508, 0.112, 0.0, 0.0, 0.0, 0.0, 0.0, 0.505, 0.347, 0.0, 0.0, 0.0, 0.361, 0.0, 0.0, 0.0, 0.266, 0.267, 0.566, 0.0, 0.412, 0.0, 0.225, 0.0, 0.0, 0.534, 0.488, 0.0, 0.0, 0.237, 0.0, 0.374, 0.0, 0.166, 0.0, 0.141, 0.127, 0.0, 0.2, 0.272, 0.523, 0.0, 0.0, 0.0, 0.114, 0.138, 0.175, 0.0, 0.085, 0.0, 0.0, 0.422, 0.0, 0.144, 0.0, 0.0, 0.0, 0.223, 0.0, 0.286, 0.351, 0.327, 0.244, 0.0, 0.13, 0.0, 0.0, 0.0, 0.122, 0.0, 0.0, 0.433, 0.0, 0.235, 0.0, 0.0, 0.0, 0.168, 0.348, 0.31, 0.0, 0.492, 0.152, 0.0, 0.0, 0.34, 0.0, 0.0, 0.0, 0.0, 0.318, 0.0, 0.0, 0.0, 0.0, 0.0, 0.21, 0.0, 0.0, 0.163, 0.116, 0.0, 0.363, 0.113, 0.0, 0.0, 0.612, 0.0, 0.231, 0.0, 0.333, 0.0, 0.0, 0.135, 0.0, 0.178, 0.0, 0.119, 0.0, 0.268, 0.0, 0.567, 0.452, 0.042, 0.271, 0.105, 0.0, 0.0, 0.0, 0.203, 0.132, 0.0, 0.457, 0.592, 0.118, 0.091, 0.123, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.213, 0.0, 0.67, 0.0, 0.296, 0.0, 0.098, 0.0, 0.0, 0.362, 0.0, 0.0, 0.0, 0.0, 0.259, 0.0, 0.0, 0.229, 0.185, 0.0, 0.524, 0.0, 0.0, 0.0, 0.0, 0.0, 0.623, 0.0, 0.0, 0.29, 0.106, 0.0, 0.0, 0.204, 0.434, 0.0, 0.0, 0.134, 0.0, 0.0, 0.0, 0.0, 0.155, 0.403, 0.146, 0.337, 0.0, 0.259, 0.0, 0.0, 0.0, 0.0, 0.239, 0.0, 0.0, 0.0, 0.143, 0.18, 0.187, 0.084, 0.109, 0.0, 0.0, 0.189, 0.138, 0.206, 0.297, 0.112, 0.253, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.117, 0.0, 0.0, 0.0, 0.0, 0.165, 0.0, 0.0, 0.0, 0.0, 0.331, 0.0, 0.098, 0.151, 0.155, 0.0, 0.0, 0.161, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.285, 0.268, 0.0, 0.13, 0.0, 0.0, 0.0, 0.0, 0.328, 0.182, 0.314, 0.121, 0.659, 0.0, 0.138, 0.127, 0.165, 0.0, 0.0, 0.0, 0.0, 0.316, 0.0, 0.221, 0.381, 0.0, 0.183, 0.106, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.286, 0.0, 0.388, 0.0, 0.0, 0.184, 0.0, 0.0, 0.0, 0.19, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.267, 0.219, 0.285, 0.0, 0.178, 0.06, 0.114, 0.221, 0.0, 0.226, 0.131, 0.0, 0.36, 0.312, 0.239, 0.128, 0.259, 0.0, 0.0, 0.286, 0.231, 0.0, 0.052, 0.125, 0.151, 0.0, 0.0, 0.135, 0.207, 0.366, 0.211, 0.184, 0.0, 0.0, 0.288, 0.203, 0.0, 0.161, 0.0, 0.157, 0.0, 0.0, 0.0, 0.0, 0.412, 0.159, 0.0, 0.0, 0.149, 0.0, 0.137, 0.0, 0.119, 0.107, 0.106, 0.0, 0.0, 0.149, 0.0, 0.222, 0.0, 0.0, 0.205, 0.306, 0.244, 0.0, 0.0, 0.0, 0.415, 0.263, 0.247, 0.345, 0.333, 0.0, 0.0, 0.0, 0.29, 0.233, 0.0, 0.259, 0.31, 0.0, 0.0, 0.0, 0.0, 0.0, 0.238, 0.144, 0.0, 0.19, 0.178, 0.0, 0.154, 0.053, 0.184, 0.0, 0.278, 0.0, 0.193, 0.0, 0.16, 0.48, 0.297, 0.152, 0.0, 0.0, 0.0, 0.47, 0.512, 0.216, 0.071, 0.0, 0.233, 0.522, 0.178, 0.0, 0.219, 0.0, 0.229, 0.0, 0.0, 0.216, 0.0, 0.0, 0.835, 0.317, 0.0, 0.0, 0.0, 0.099, 0.386, 0.214, 0.516, 0.293, 0.0, 0.359, 0.0, 0.0, 0.362, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.172, 0.126, 0.0, 0.231, 0.0, 0.286, 0.351, 0.327, 0.398, 0.0, 0.0, 0.0, 0.273, 0.125, 0.0, 0.0, 0.136, 0.114, 0.0, 0.0, 0.483, 0.0, 0.15, 0.326, 0.224, 0.186, 0.121, 0.42, 0.0, 0.0, 0.0, 0.0, 0.259, 0.236, 0.0, 0.239, 0.0, 0.259, 0.217, 0.337, 0.121, 0.137, 0.249, 0.0, 0.094, 0.0, 0.0, 0.18, 0.195, 0.318, 0.0, 0.455, 0.204, 0.0, 0.0, 0.0, 0.0, 0.0, 0.271, 0.0, 0.0, 0.0, 0.457, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.43, 0.0, 0.0, 0.0, 0.0, 0.416, 0.091, 0.28, 0.0, 0.0, 0.0, 0.42, 0.0, 0.098, 0.0, 0.162, 0.0, 0.0, 0.134, 0.0, 0.084, 0.292, 0.19, 0.0, 0.37, 0.166, 0.0, 0.0, 0.0, 0.0, 0.0, 0.263, 0.355, 0.0, 0.162, 0.0, 0.168, 0.0, 0.0, 0.0, 0.615, 0.0, 0.0, 0.0, 0.388, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.412, 0.167, 0.0, 0.0, 0.209, 0.0, 0.215, 0.322, 0.0, 0.26, 0.568, 0.32, 0.492, 0.333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.162, 0.0, 0.0, 0.0, 0.263, 0.044, 0.211, 0.286, 0.0, 0.0, 0.0, 0.0, 0.412, 0.0, 0.42, 0.189, 0.117, 0.244, 0.0, 0.128, 0.238, 0.255, 0.0, 0.0, 0.391, 0.0, 0.0, 0.0, 0.0, 0.169, 0.0, 0.306, 0.445, 0.211, 0.257, 0.0, 0.0, 0.0, 0.383, 0.0, 0.263, 0.0, 0.069, 0.441, 0.092, 0.177, 0.0, 0.203, 0.394, 0.069, 0.25, 0.218, 0.0, 0.087, 0.0, 0.259, 0.0, 0.119, 0.0, 0.157, 0.0, 0.402, 0.141, 0.321, 0.32, 0.186, 0.102, 0.185, 0.0, 0.095, 0.508, 0.278, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.157, 0.0, 0.0, 0.0, 0.133, 0.154, 0.0, 0.0, 0.304, 0.18, 0.0, 0.0, 0.0, 0.0, 0.0, 0.096, 0.387, 0.0, 0.0, 0.0, 0.114, 0.167, 0.216, 0.155, 0.0, 0.0, 0.0, 0.492, 0.435, 0.281, 0.5, 0.532, 0.0, 0.265, 0.0, 0.08, 0.0, 0.287, 0.0, 0.0, 0.0, 0.0, 0.517, 0.661, 0.0, 0.0, 0.152, 0.0, 0.0, 0.0, 0.0, 0.0, 0.603, 0.0, 0.0, 0.213, 0.0, 0.116, 0.156, 0.574, 0.49, 0.423, 0.245, 0.0, 0.291, 0.0, 0.0, 0.206, 0.174, 0.287, 0.0, 0.0, 0.0, 0.0, 0.0, 0.255, 0.273, 0.156, 0.08, 0.2, 0.0, 0.0, 0.151, 0.0, 0.0, 0.487, 0.219, 0.0, 0.405, 0.0, 0.0, 0.141, 0.255, 0.0, 0.32, 0.081, 0.156, 0.574, 0.49, 0.423, 0.245, 0.0, 0.16, 0.391, 0.217, 0.177, 0.447, 0.175, 0.0, 0.0, 0.18, 0.195, 0.256, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.186, 0.244, 0.276, 0.377, 0.297, 0.0, 0.087, 0.141, 0.0, 0.0, 0.134, 0.182, 0.203, 0.204, 0.201, 0.144, 0.0, 0.0, 0.0, 0.091, 0.0, 0.0, 0.091, 0.294, 0.132, 0.066, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.307, 0.162, 0.0, 0.0, 0.365, 0.187, 0.357, 0.231, 0.139, 0.0, 0.0, 0.0, 0.184, 0.0, 0.237, 0.0, 0.0, 0.452, 0.086, 0.254, 0.0, 0.0, 0.127, 0.0, 0.0, 0.0, 0.0, 0.144, 0.421, 0.088, 0.111, 0.0, 0.22, 0.421, 0.0, 0.0, 0.0, 0.18, 0.0, 0.0, 0.0, 0.402, 0.091, 0.0, 0.492, 0.483, 0.384, 0.449, 0.29, 0.217, 0.0, 0.0, 0.0, 0.233, 0.309, 0.49, 0.3, 0.128, 0.285, 0.363, 0.269, 0.122, 0.398, 0.343, 0.729, 0.12, 0.672, 0.09, 0.259, 0.474, 0.05, 0.0, 0.291, 0.0, 0.0, 0.0, 0.094, 0.263, 0.0, 0.286, 0.0, 0.0, 0.254, 0.539, 0.268, 0.396, 0.282, 0.422, 0.317, 0.213, 0.14, 0.057, 0.333, 0.0, 0.091, 0.0, 0.324, 0.303, 0.305, 0.203, 0.0, 0.369, 0.0, 0.207, 0.412, 0.0, 0.427, 0.756, 0.0, 0.221, 0.079, 0.13, 0.0, 0.233, 0.137, 0.237, 0.0, 0.0, 0.091, 0.0, 0.0, 0.0, 0.0, 0.145, 0.0, 0.188, 0.238, 0.0, 0.0, 0.0, 0.0, 0.492, 0.315, 0.0, 0.0, 0.075, 0.0, 0.0, 0.114, 0.186, 0.32, 0.248, 0.0, 0.0, 0.33, 0.31, 0.0, 0.168, 0.0, 0.474, 0.165, 0.0, 0.32, 0.215, 0.232, 0.188, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.263, 0.169, 0.241, 0.347, 0.0, 0.0, 0.412, 0.0, 0.0, 0.302, 0.0, 0.206, 0.129, 0.125, 0.0, 0.152, 0.0, 0.0, 0.0, 0.421, 0.0, 0.124, 0.0, 0.318, 0.196, 0.267, 0.145, 0.152, 0.0, 0.111, 0.076, 0.48, 0.583, 0.213]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "eEIFUO5n_L7F",
        "colab_type": "code",
        "outputId": "127a7699-7358-498e-b28b-bb1b15a4f7d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import itertools\n",
        "from keras.preprocessing import sequence\n",
        "vocabulary_file = 'vocabulary_twitter'\n",
        "padded_context_file = 'Padded_context'\n",
        "padded_answers_file = 'Padded_answers'\n",
        "unknown_token = 'something'\n",
        "\n",
        "vocabulary_size = 10000\n",
        "max_features = vocabulary_size\n",
        "maxlen_input = 50\n",
        "maxlen_output = 50  # cut texts after this number of words\n",
        "\n",
        "all = ' '.join(all)\n",
        "tokenized_all = all.split()\n",
        "tokenized_context = [t.split() for t in context]\n",
        "tokenized_answers = [t.split() for t in answers]\n",
        "\n",
        "word_freq = nltk.FreqDist(itertools.chain(tokenized_all))\n",
        "print (\"Found %d unique words tokens.\" % len(word_freq.items()))\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Found 11854 unique words tokens.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mKy5TunA_PNv",
        "colab_type": "code",
        "outputId": "662711c1-c972-4ef5-8ea5-86383f6e9019",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "vocab = word_freq.most_common(vocabulary_size-1)\n",
        "with open(vocabulary_file, 'wb') as v:\n",
        "  pickle.dump(vocab, v)\n",
        "\n",
        "print(vocab[0:20])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('STA', 10514), ('END', 10514), ('.', 6665), ('i', 5236), ('the', 2913), ('to', 2577), ('you', 2550), ('is', 2536), ('!', 2242), (',', 2213), ('it', 2051), ('a', 2046), ('not', 1964), ('?', 1839), ('and', 1655), ('that', 1436), ('my', 1196), ('in', 1171), ('of', 1082), ('am', 1056)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "SS9Jbs0A_SCl",
        "colab_type": "code",
        "outputId": "09a87101-5cc0-4b54-9dfc-7031251ff113",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "cell_type": "code",
      "source": [
        "vocab = pickle.load(open(vocabulary_file, 'rb'))\n",
        "index_to_word = [x[0] for x in vocab]\n",
        "index_to_word.append(unknown_token)\n",
        "word_to_index = dict([(w,i) for i,w in enumerate(index_to_word)])\n",
        "\n",
        "print (\"Using vocabulary of size %d.\" % vocabulary_size)\n",
        "print (\"The least frequent word in our vocabulary is '%s' and appeared %d times.\" % (vocab[-1][0], vocab[-1][1]))\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using vocabulary of size 10000.\n",
            "The least frequent word in our vocabulary is 'mcdonalds' and appeared 1 times.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "rsElu_dP_Ua4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "# Replacing all words not in our vocabulary with the unknown token:\n",
        "for i, sent in enumerate(tokenized_answers):\n",
        "  tokenized_answers[i] = [w if w in word_to_index else unknown_token for w in sent]\n",
        "   \n",
        "for i, sent in enumerate(tokenized_context):\n",
        "  tokenized_context[i] = [w if w in word_to_index else unknown_token for w in sent]\n",
        "\n",
        "# Creating the training data:\n",
        "X = np.asarray([[word_to_index[w] for w in sent] for sent in tokenized_context])\n",
        "Y = np.asarray([[word_to_index[w] for w in sent] for sent in tokenized_answers])\n",
        "\n",
        "Q = sequence.pad_sequences(X, maxlen=maxlen_input, padding='post')\n",
        "A = sequence.pad_sequences(Y, maxlen=maxlen_output, padding='post')\n",
        "\n",
        "row, col = Q.shape\n",
        "for i in range(row):\n",
        "  Q[i,:] = Q[i,:]*context_sentiments[i]\n",
        "\n",
        "\n",
        "with open(padded_context_file, 'wb') as q:\n",
        "    pickle.dump(Q, q)\n",
        "    \n",
        "with open(padded_answers_file, 'wb') as a:\n",
        "    pickle.dump(A, a)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yg76ZiVv-1SG",
        "colab_type": "code",
        "outputId": "27411a64-482b-4f44-94be-785f4d14ee63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "print(Q.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5257, 50)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "V4hG9clF_Xlk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import requests\n",
        "file_url = \"http://nlp.stanford.edu/data/glove.6B.zip\"\n",
        "tr = requests.get(file_url, stream=True)\n",
        "with open(\"glove.6B.zip\", \"wb\") as f:\n",
        "    for chunk in tr.iter_content(chunk_size=1024):\n",
        "        if chunk:\n",
        "            f.write(chunk)\n",
        "            \n",
        "            \n",
        "import zipfile\n",
        "import os\n",
        "def un_zip(file_name):\n",
        "    \"\"\"unzip zip file\"\"\"\n",
        "    zip_file = zipfile.ZipFile(file_name)\n",
        "    if os.path.isdir(file_name + \"_files\"):\n",
        "        pass\n",
        "    else:\n",
        "        os.mkdir(file_name + \"_files\")\n",
        "    for names in zip_file.namelist():\n",
        "        zip_file.extract(names,file_name + \"_files/\")\n",
        "    zip_file.close()\n",
        "        \n",
        "glove = un_zip(\"glove.6B.zip\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "X0zFIatn_bn4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "padded_context_file = 'Padded_context'\n",
        "padded_answers_file = 'Padded_answers'\n",
        "unknown_token = 'something'\n",
        "word_embedding_size = 100\n",
        "sentence_embedding_size = 300\n",
        "dictionary_size = 10000\n",
        "maxlen_input = 50\n",
        "maxlen_output = 50\n",
        "num_subsets = 2\n",
        "Epochs = 50\n",
        "BatchSize = 128 \n",
        "Patience = 0\n",
        "dropout = .25\n",
        "n_test = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vPSeHTgw_eeP",
        "colab_type": "code",
        "outputId": "6c2d72a3-8236-447f-f1b3-b9ba70f78df8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        }
      },
      "cell_type": "code",
      "source": [
        "import _pickle\n",
        "import numpy as np\n",
        "\n",
        "embeddings_index = {}\n",
        "f = open('./glove.6B.zip_files/glove.6B.100d.txt')\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print('Found %s word vectors.' % len(embeddings_index))\n",
        "embedding_matrix = np.zeros((dictionary_size, word_embedding_size))\n",
        "\n",
        "# Loading our vocabulary:\n",
        "vocabulary = _pickle.load(open(vocabulary_file, 'rb'))\n",
        "\n",
        "# Using the Glove embedding:\n",
        "i = 0\n",
        "for word in vocabulary:\n",
        "    embedding_vector = embeddings_index.get(word[0])\n",
        "    \n",
        "    if embedding_vector is not None:\n",
        "        # words not found in embedding index will be all-zeros.\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "    i += 1\n",
        "    \n",
        "print(embedding_matrix)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 400000 word vectors.\n",
            "[[ 0.          0.          0.         ...  0.          0.\n",
            "   0.        ]\n",
            " [ 0.          0.          0.         ...  0.          0.\n",
            "   0.        ]\n",
            " [-0.33978999  0.20941     0.46348    ... -0.23394001  0.47297999\n",
            "  -0.028803  ]\n",
            " ...\n",
            " [-0.16700999  0.10193    -0.62102997 ... -0.55523002 -0.60065001\n",
            "  -0.22685   ]\n",
            " [ 0.31239     0.031386   -0.27726999 ...  0.28920999  0.82100999\n",
            "   0.84512001]\n",
            " [ 0.          0.          0.         ...  0.          0.\n",
            "   0.        ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "s-pWlk3-_hen",
        "colab_type": "code",
        "outputId": "f295ec95-67e6-4e4a-fbc1-e0bff7784a5e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 558
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.layers import Input, Embedding, LSTM, Dense, RepeatVector, Bidirectional, Dropout, concatenate\n",
        "from keras.optimizers import Adam, SGD\n",
        "from keras.models import Model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Activation, Dense\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.preprocessing import sequence\n",
        "import keras.backend as K\n",
        "import os\n",
        "import theano.tensor as T\n",
        "import os\n",
        "import pandas as pd\n",
        "import sys\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "weights_file = 'assignment4_twitter_pos.h5'\n",
        "ad = Adam(lr=0.00005) \n",
        "\n",
        "input_context = Input(shape=(maxlen_input,), name='input_context')\n",
        "input_answer = Input(shape=(maxlen_input,), name='input_answer')\n",
        "LSTM_encoder = LSTM(sentence_embedding_size, init= 'lecun_uniform')\n",
        "LSTM_decoder = LSTM(sentence_embedding_size, init= 'lecun_uniform')\n",
        "if os.path.isfile(weights_file):\n",
        "    Shared_Embedding = Embedding(output_dim=word_embedding_size, input_dim=dictionary_size, input_length=maxlen_input)\n",
        "else:\n",
        "    Shared_Embedding = Embedding(output_dim=word_embedding_size, input_dim=dictionary_size, weights=[embedding_matrix], input_length=maxlen_input)\n",
        "word_embedding_context = Shared_Embedding(input_context)\n",
        "context_embedding = LSTM_encoder(word_embedding_context)\n",
        "\n",
        "word_embedding_answer = Shared_Embedding(input_answer)\n",
        "answer_embedding = LSTM_decoder(word_embedding_answer)\n",
        "\n",
        "merge_layer = concatenate([context_embedding, answer_embedding])\n",
        "out = Dense(int(dictionary_size/2), activation=\"relu\")(merge_layer)\n",
        "out = Dense(dictionary_size, activation=\"softmax\")(out)\n",
        "\n",
        "model = Model(input=[input_context, input_answer], output = [out])\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer=ad)\n",
        "\n",
        "if os.path.isfile(weights_file):\n",
        "    model.load_weights(weights_file)\n",
        "    \n",
        "model.summary()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:21: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(300, kernel_initializer=\"lecun_uniform\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:22: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(300, kernel_initializer=\"lecun_uniform\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:37: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_context (InputLayer)      (None, 50)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_answer (InputLayer)       (None, 50)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         (None, 50, 100)      1000000     input_context[0][0]              \n",
            "                                                                 input_answer[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 300)          481200      embedding_1[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "lstm_2 (LSTM)                   (None, 300)          481200      embedding_1[1][0]                \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 600)          0           lstm_1[0][0]                     \n",
            "                                                                 lstm_2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 5000)         3005000     concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 10000)        50010000    dense_1[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 54,977,400\n",
            "Trainable params: 54,977,400\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "UmuRmGWd_kmf",
        "colab_type": "code",
        "outputId": "00ffb594-6ff0-401e-9444-ff1b8bfe5459",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "cell_type": "code",
      "source": [
        "import _pickle\n",
        "q = _pickle.load(open(padded_context_file, 'rb'))\n",
        "a = _pickle.load(open(padded_answers_file, 'rb'))\n",
        "n_exem, n_words = a.shape\n",
        "\n",
        "\n",
        "print('Number of exemples = %d'%(n_exem))\n",
        "step = int(np.around(n_exem/num_subsets))\n",
        "round_exem = int(step * num_subsets)\n",
        "print(step, round_exem)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of exemples = 5257\n",
            "2628 5256\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "7KpVa5oL_oMD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def print_result(input):\n",
        "  ans_partial = np.zeros((1,maxlen_input))\n",
        "  ans_partial[0,-1] = 0 #index of STA\n",
        "  for k in range(maxlen_input - 1):\n",
        "    ye = model.predict([input, ans_partial])\n",
        "    mp = np.argmax(ye)\n",
        "    ans_partial[0, 0:-1] = ans_partial[0, 1:]\n",
        "    ans_partial[0, -1] = mp\n",
        "  text = ''\n",
        "  for k in ans_partial[0]:\n",
        "    k = k.astype(int)\n",
        "    if k < (dictionary_size-2):\n",
        "      w = vocabulary[k]\n",
        "      text = text + w[0] + ' '\n",
        "  return(text)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "H11kNtzI_rJ0",
        "colab_type": "code",
        "outputId": "44160b35-1bb5-45c5-e197-e86ca924245f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 6757
        }
      },
      "cell_type": "code",
      "source": [
        "x = range(0,Epochs) \n",
        "valid_loss = np.zeros(Epochs)\n",
        "train_loss = np.zeros(Epochs)\n",
        "for m in range(Epochs):\n",
        "  # Loop over training batches due to memory constraints:\n",
        "  for n in range(0,round_exem,step):\n",
        "    q2 = q[n:n+step]\n",
        "    s = q2.shape\n",
        "    count = 0\n",
        "    for i, sent in enumerate(a[n:n+step]):\n",
        "      l = np.where(sent==1)\n",
        "      limit = l[0][0]\n",
        "      count += limit + 1\n",
        "    Q = np.zeros((count,maxlen_input))\n",
        "    A = np.zeros((count,maxlen_input))\n",
        "    Y = np.zeros((count,dictionary_size))\n",
        "    \n",
        "    count = 0\n",
        "    for i, sent in enumerate(a[n:n+step]):\n",
        "      ans_partial = np.zeros((1,maxlen_input))\n",
        "      # Loop over the positions of the current target output (the current output sequence):\n",
        "      l = np.where(sent==1)\n",
        "      limit = l[0][0]\n",
        "      for k in range(1,limit+1):\n",
        "        # Mapping the target output (the next output word) for one-hot codding:\n",
        "        y = np.zeros((1, dictionary_size))\n",
        "        y[0, sent[k]] = 1\n",
        "        # preparing the partial answer to input:\n",
        "        ans_partial[0,-k:] = sent[0:k]\n",
        "        # training the model for one epoch using teacher forcing:\n",
        "        Q[count, :] = q2[i:i+1] \n",
        "        A[count, :] = ans_partial \n",
        "        Y[count, :] = y\n",
        "        count += 1\n",
        "    print('Training epoch: %d, training examples: %d - %d'%(m,n, n + step))\n",
        "    model.fit([Q, A], Y, batch_size=BatchSize, epochs=1)\n",
        "    test_input = q[6:7]\n",
        "    print(print_result(test_input))\n",
        "  model.save_weights(weights_file, overwrite=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training epoch: 0, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "36967/36967 [==============================] - 87s 2ms/step - loss: 5.7332\n",
            "STA i am not not not not not not not not not not not not not not not not not not not not not not not not a a . END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 0, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38361/38361 [==============================] - 91s 2ms/step - loss: 5.1033\n",
            "STA i am not not not not not not not not not not not not not not not not not not to to to to END END END END END END END END END END END END END END END END END END END END ! ! ! ! \n",
            "Training epoch: 1, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "36967/36967 [==============================] - 87s 2ms/step - loss: 5.0519\n",
            "STA i am not not not not not not not not to be be to be be be a a . END END END END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 1, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38361/38361 [==============================] - 90s 2ms/step - loss: 4.8794\n",
            "STA i am not not not not not not not not not to be to be to be to be to be to be to be to be to be to be to be to be to be to be to be to be to be to be to be \n",
            "Training epoch: 2, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "36967/36967 [==============================] - 87s 2ms/step - loss: 4.8673\n",
            "STA i am not not not not not not not be a . END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 2, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38361/38361 [==============================] - 90s 2ms/step - loss: 4.7276\n",
            "STA i am not not not not not a . END . END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 3, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "36967/36967 [==============================] - 87s 2ms/step - loss: 4.7064\n",
            "STA i am not not not know to be a . END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 3, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38361/38361 [==============================] - 90s 2ms/step - loss: 4.5856\n",
            "STA i am not not not not a . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . \n",
            "Training epoch: 4, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "36967/36967 [==============================] - 87s 2ms/step - loss: 4.5517\n",
            "STA i am not not not know ? END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 4, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38361/38361 [==============================] - 90s 2ms/step - loss: 4.4416\n",
            "STA i am not not not not not not a . END . END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 5, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "36967/36967 [==============================] - 87s 2ms/step - loss: 4.3874\n",
            "STA i am not not know i am trying to be a bunch . END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 5, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38361/38361 [==============================] - 90s 2ms/step - loss: 4.3003\n",
            "STA i am not not not have a . END .\"donasia . END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 6, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "36967/36967 [==============================] - 87s 2ms/step - loss: 4.2168\n",
            "STA i am not not even the same thing i am trying to come to come to come the END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 6, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38361/38361 [==============================] - 90s 2ms/step - loss: 4.1604\n",
            "STA i am not not know that is a . END diversity gap END END END END END END END END grapes END END END grapes END grapes END grapes END grapes END grapes END grapes \n",
            "Training epoch: 7, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "36967/36967 [==============================] - 87s 2ms/step - loss: 4.0427\n",
            "STA i am not not even i am trying to go to come to come to come to come the marvel END END END END END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 7, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38361/38361 [==============================] - 90s 2ms/step - loss: 4.0197\n",
            "STA i am not not know that is a . END .\"donasia . END END END grapes END grapes END grapes END grapes END grapes END grapes END grapes END grapes END grapes END grapes END grapes END grapes END grapes END grapes END grapes END grapes END grapes \n",
            "Training epoch: 8, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "36967/36967 [==============================] - 87s 2ms/step - loss: 3.8785\n",
            "STA i am sure to see the . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . \n",
            "Training epoch: 8, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38361/38361 [==============================] - 90s 2ms/step - loss: 3.8850\n",
            "STA i am not not even that is a . END .\"donasia . END . END END END END END END END END END END END END END END END END END END END END END grapes END grapes END grapes END grapes END \n",
            "Training epoch: 9, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "36967/36967 [==============================] - 87s 2ms/step - loss: 3.7200\n",
            "STA i am so much much i am trying to go to go the marvel END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END ! ! ! ! \n",
            "Training epoch: 9, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38361/38361 [==============================] - 90s 2ms/step - loss: 3.7594\n",
            "STA i am not not even tomatoes . . . . . . . . . . . . . . . . . . . . . . . . . END . END END END END END END END END END END END END END END END END \n",
            "Training epoch: 10, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "36967/36967 [==============================] - 87s 2ms/step - loss: 3.5791\n",
            "STA i am so trying to be a disaster END books . END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 10, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38361/38361 [==============================] - 90s 2ms/step - loss: 3.6369\n",
            "STA i am not not even that is a explanation . . . . . . . . . . . . . . . . . END END END END END END END END END END END END END END END END END END ! ! ! ! \n",
            "Training epoch: 11, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "36967/36967 [==============================] - 87s 2ms/step - loss: 3.4466\n",
            "STA i am sure to clean the . END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 11, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38361/38361 [==============================] - 90s 2ms/step - loss: 3.5188\n",
            "STA i am not feelin . END END END ; END ; END ; ; ; ; END ; END ; END ; END ; END ; END ; END ; END ; \n",
            "Training epoch: 12, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "36967/36967 [==============================] - 87s 2ms/step - loss: 3.3225\n",
            "STA i am sure to clean the day . END guess END END naturals END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END ! ! ! ! \n",
            "Training epoch: 12, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38361/38361 [==============================] - 90s 2ms/step - loss: 3.4194\n",
            "STA i am not feelin jeffery END END END END END END END END END END ! ! END END END END END END END END END END END END END END END ! ! END END END ! ! END ! ! ! ! ! ! ! ! ! \n",
            "Training epoch: 13, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "36967/36967 [==============================] - 87s 2ms/step - loss: 3.2188\n",
            "STA i am not sure what is the same thing i am sure END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END ! ! ! ! ! ! \n",
            "Training epoch: 13, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38361/38361 [==============================] - 90s 2ms/step - loss: 3.3235\n",
            "STA i am not feelin jeffery END ; . pharma . ; ; ; ; END ; END ; END ; END ; END ; END ; END ; END \n",
            "Training epoch: 14, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "36967/36967 [==============================] - 87s 2ms/step - loss: 3.1145\n",
            "STA i am sorry i am sorry i am sorry . END same cleveland indians . END \"you . END \"you END END END END END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 14, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38361/38361 [==============================] - 90s 2ms/step - loss: 3.2283\n",
            "STA i am not feelin jeffery END ; i am not going to die END ; i am not willing to go to talk to get to get to get to get a room . END END END END END END END END END END END END END END END \n",
            "Training epoch: 15, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "36967/36967 [==============================] - 87s 2ms/step - loss: 3.0161\n",
            "STA i am sure i am not trying to come the same day i just wanted to be a mention of the same time . END #sorandom . END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 15, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38361/38361 [==============================] - 90s 2ms/step - loss: 3.1299\n",
            "STA i am not feelin jeffery END ; ; alter ; ; ; ; ; END ; END ; END ; END ; END END \n",
            "Training epoch: 16, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "36967/36967 [==============================] - 86s 2ms/step - loss: 2.9317\n",
            "STA i am sure i am not sure for sure ! END END END ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! \n",
            "Training epoch: 16, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38361/38361 [==============================] - 90s 2ms/step - loss: 3.0508\n",
            "STA i am not feelin class END ; i am not even sold a beast . END doj END ; i am ok . END END END END END END END END END END END END END END END END END END ! ! ! ! ! ! ! ! \n",
            "Training epoch: 17, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "36967/36967 [==============================] - 86s 2ms/step - loss: 2.8404\n",
            "STA i am sure what i am not sure END same cleveland indians ? END END indians , but i am not sure . END same thing . END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 17, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38361/38361 [==============================] - 90s 2ms/step - loss: 2.9722\n",
            "STA i am not feelin class END ; i am going to die END ; i am willing to y awake . END everything , i am excited . END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 18, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "36967/36967 [==============================] - 86s 2ms/step - loss: 2.7712\n",
            "STA i am sure to clean the warriors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . END . END END END END END \n",
            "Training epoch: 18, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38361/38361 [==============================] - 90s 2ms/step - loss: 2.8969\n",
            "STA i am not feelin class END ; i am not even sold a quick system . END END ; END ; END ; END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 19, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "36967/36967 [==============================] - 86s 2ms/step - loss: 2.6916\n",
            "STA i am sure what i am talking about baseball about the world ? END e-mail END show , but i watched , the phimosis , etc , pug , pug , pug , pug , i am END END END END END END END END END END END \n",
            "Training epoch: 19, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38361/38361 [==============================] - 90s 2ms/step - loss: 2.8266\n",
            "STA i am not feelin class END ; END ; END ; ; ; ; END ; ; END ; END ; END ; END ; END ; END END END END END END \n",
            "Training epoch: 20, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "36967/36967 [==============================] - 86s 2ms/step - loss: 2.6243\n",
            "STA i am sure what i am talking about baseball about the week ? END wanted into a bit of year END wanted to be END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 20, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38361/38361 [==============================] - 90s 2ms/step - loss: 2.7585\n",
            "STA i am not feelin class END ; i am not even sold a quick response END ; upset tummy END grapes , i am not going to get to get END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 21, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "36967/36967 [==============================] - 86s 2ms/step - loss: 2.5625\n",
            "STA i am sorry . i am talking about the week END am talking about the week ? END wanted into nov ? END END am : END END ; END END END ! END END END END END END END END END END END END END END END END \n",
            "Training epoch: 21, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38361/38361 [==============================] - 90s 2ms/step - loss: 2.7001\n",
            "STA i am not feelin class END ; i am not going to go to a sit to a explanation . END answer . END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 22, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "36967/36967 [==============================] - 86s 2ms/step - loss: 2.5089\n",
            "STA i am jealous . i am jealous . i hope you could write my schedule . END caraval . END thing ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! \n",
            "Training epoch: 22, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38361/38361 [==============================] - 89s 2ms/step - loss: 2.6457\n",
            "STA i am not feelin class END ; i am not even sold a quick ahead to the case . END guess is not be a . END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 23, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "36967/36967 [==============================] - 86s 2ms/step - loss: 2.4459\n",
            "STA i am jealous . i am talking about baseball about the world , but i am talking about the world dakota .people END ; END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 23, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38361/38361 [==============================] - 89s 2ms/step - loss: 2.5848\n",
            "STA i am not feelin class END ; i am not even sold a quick xbox for a quick . END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 24, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "36967/36967 [==============================] - 86s 2ms/step - loss: 2.3865\n",
            "STA i am jealous . i am jealous . i am talking about the . . . . . . . . . . . . . . END . END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 24, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38361/38361 [==============================] - 89s 2ms/step - loss: 2.5340\n",
            "STA i am not feelin class END ; END ; END ; ; ; ; END ; END ; END ; END ; END ; END ; END END END END END END END \n",
            "Training epoch: 25, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "36967/36967 [==============================] - 86s 2ms/step - loss: 2.3346\n",
            "STA i am sure . i am not talking baseball . END usually cleveland indians , but , but i am not sure . END same . END END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 25, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38361/38361 [==============================] - 89s 2ms/step - loss: 2.4751\n",
            "STA i am not feelin class END ; i am not going to go to the END ; but i can not pick it would be much . END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 26, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "36967/36967 [==============================] - 86s 2ms/step - loss: 2.2802\n",
            "STA i am sure what i am very full of the night of the wrestler direct the world END guess END guess END END END END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 26, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38361/38361 [==============================] - 89s 2ms/step - loss: 2.4214\n",
            "STA i am not feelin class END ; i am not even sold a quick xbox for a ticket for a room in a room . END ; END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 27, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "36967/36967 [==============================] - 85s 2ms/step - loss: 2.2328\n",
            "STA i am jealous . i am not talking baseball about the world , but i am talking . i can not get it . END quite ! END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 27, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38361/38361 [==============================] - 89s 2ms/step - loss: 2.3626\n",
            "STA i am glad jeffery END ; i think it is often a quick that quick that is . END etc . END etc END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 28, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "36967/36967 [==============================] - 85s 2ms/step - loss: 2.1816\n",
            "STA i am jealous . i am talking about the virtual indians ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? \n",
            "Training epoch: 28, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38361/38361 [==============================] - 91s 2ms/step - loss: 2.3135\n",
            "STA i am glad jeffery END ; i am glad for you snap you guys like that quick i am 72 in norte , but i am willing to pick again again END END ; END END END END END END END END END END END END END END END \n",
            "Training epoch: 29, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "36967/36967 [==============================] - 86s 2ms/step - loss: 2.1299\n",
            "STA i am sure what i am talking about the week ? END wanted to come ? END wanted into a bit of 2 year END got a tough for END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 29, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38361/38361 [==============================] - 89s 2ms/step - loss: 2.2563\n",
            "STA i am glad for you END ; but i am just sold a quick for a ticket for a ticket . END ; enjoying for no reason END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 30, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "36967/36967 [==============================] - 86s 2ms/step - loss: 2.0780\n",
            "STA i am sure what i am talking about the week ? END wanted to come ? END think i can be sure END END ; END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 30, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38361/38361 [==============================] - 89s 2ms/step - loss: 2.2036\n",
            "STA i am not feelin class END ; i am not going to see you END keep a ticket ! END END ! ! END ! END ! END ! END ! END ! END ! END ! END ! END ! END END ! END ! END ! ! \n",
            "Training epoch: 31, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "36967/36967 [==============================] - 86s 2ms/step - loss: 2.0321\n",
            "STA i am sorry . i am talking about the week . END guess END usually on my schedule . END god END do that END do END END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 31, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38361/38361 [==============================] - 89s 2ms/step - loss: 2.1553\n",
            "STA i am not feelin class END ; i am not even sold a quick that quick it is flooded in the first place END ; i got it END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 32, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "36967/36967 [==============================] - 86s 2ms/step - loss: 1.9976\n",
            "STA i am sorry i am not talking baseball about the week ? END wanted into a nice ? END got back for 2 weeks END END am no no . END same . END END END END END END END END END END END END END END END END \n",
            "Training epoch: 32, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38361/38361 [==============================] - 89s 2ms/step - loss: 2.1184\n",
            "STA i am glad for you END ; but i am glad you are going to y awake END ; everything my schedule &amp ; getting my collection &amp ; still still still still there END END END END END END END END END END END END END END END \n",
            "Training epoch: 33, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "36967/36967 [==============================] - 86s 2ms/step - loss: 1.9520\n",
            "STA i am sorry . but i am talking about it . i am talking about it . . . . . END believe ! END ! ! END ! END END ! END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 33, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38361/38361 [==============================] - 89s 2ms/step - loss: 2.0648\n",
            "STA i am not feelin class END ; i am glad for you END ; END money END ! ! END ! END ! END ! END ! END ! END ! END ! END ! END ! END ! END ! END ! ! END ! ! ! ! \n",
            "Training epoch: 34, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "36967/36967 [==============================] - 86s 2ms/step - loss: 1.9082\n",
            "STA i am sorry . i am talking about the week . END guess . END think i can not get to get the time . END END END END END END ; END ; END END END END END END END END END END END have no END END \n",
            "Training epoch: 34, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38361/38361 [==============================] - 89s 2ms/step - loss: 2.0247\n",
            "STA i am glad i could not even hang out of a quick around me lol END nigga END END ! END ! END ! END ! END ! END ! END ! END ! END ! END ! END ! i have not not END not ! ! i \n",
            "Training epoch: 35, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "36967/36967 [==============================] - 86s 2ms/step - loss: 1.8692\n",
            "STA i am sorry i am not talking baseball about the week ? END wanted to come ? END think i can get it END END END 3 END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 35, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38361/38361 [==============================] - 89s 2ms/step - loss: 1.9824\n",
            "STA i am not feelin class END ; END ; are END ; ; ; ; ; END ; END ; END ; END ; END ; END END END END END END END END END END ! ! \n",
            "Training epoch: 36, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "36967/36967 [==============================] - 86s 2ms/step - loss: 1.8301\n",
            "STA i am sorry what i am talking about this week END ; END am : END ? END have a great day END ; END ; END END END END END END END END END END END END END END END END END END END END END END i \n",
            "Training epoch: 36, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38361/38361 [==============================] - 89s 2ms/step - loss: 1.9400\n",
            "STA i am sorry jeffery END but i am glad you are going to the controller . END but i can not even pick along in hard . END plot&amp END ; END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 37, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "36967/36967 [==============================] - 86s 2ms/step - loss: 1.7884\n",
            "STA i am jealous . but i am talking about it as a brave enough of being being being being END . . . . END END END should have to be END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 37, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38361/38361 [==============================] - 89s 2ms/step - loss: 1.8994\n",
            "STA i am not feelin class END ; the of course . END available END are not that END END END END END END END END END END END END END END END END END END END END END END END END END END END END END END ! \n",
            "Training epoch: 38, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "36967/36967 [==============================] - 86s 2ms/step - loss: 1.7508\n",
            "STA i do not know what is the mouse was thinking ? ? ? why come into a house with 2 ferocious felines ? ? END did anything ? END END END END END ! END END END END END END END END END END END END END i ! \n",
            "Training epoch: 38, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38361/38361 [==============================] - 89s 2ms/step - loss: 1.8648\n",
            "STA i am sorry jeffery i am glad for you . i am talking to y END etc . END keep a lot of obama or ? END END END ! END END END END END END END END END END END END END END ! ! ! \n",
            "Training epoch: 39, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "36967/36967 [==============================] - 86s 2ms/step - loss: 1.7198\n",
            "STA i am jealous . i am talking about the week . END guess . END think i can help END END ; END END 3 in END END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 39, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38361/38361 [==============================] - 88s 2ms/step - loss: 1.8240\n",
            "STA i am not feelin class END ; i watched around my head in my head , but i do not want to pick up at all of class . END fuck that END did END END END END END END END END END END END END END END ! \n",
            "Training epoch: 40, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "36967/36967 [==============================] - 85s 2ms/step - loss: 1.6858\n",
            "STA i am sorry . but i am not talking baseball or anything . END usually out of the night , but i am in the world END ; END am . END should END END END END END END END END END END END END END END END END \n",
            "Training epoch: 40, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38361/38361 [==============================] - 89s 2ms/step - loss: 1.7854\n",
            "STA i am not feelin class of the lookin END have been a good night lol END do not know that END did lol END END END ! END END END END END END END END END END END END END END ! ! END ! ! i have have \n",
            "Training epoch: 41, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "36967/36967 [==============================] - 86s 2ms/step - loss: 1.6428\n",
            "STA i am sorry . but i am talking about it since i am talking about the week END ; after no damn END did END END END END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 41, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38361/38361 [==============================] - 89s 2ms/step - loss: 1.7477\n",
            "STA i am not feelin class END ; i am glad you END the lesson , wbu majority of you are not END but that have been there . END should be . END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 42, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "36967/36967 [==============================] - 86s 2ms/step - loss: 1.6073\n",
            "STA i do not know what the mouse was thinking ? ? why come into a house with 2 ferocious felines ? ? END got a for no reason END END END END END ! END END END END END END END END END END END END i ! \n",
            "Training epoch: 42, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38361/38361 [==============================] - 89s 2ms/step - loss: 1.7161\n",
            "STA i am glad for you . i am glad for you . END next ! END etc END ! END ! ! END ! END ! END ! END ! END ! END ! END ! END ! END ! END ! END ! ! i have not have \n",
            "Training epoch: 43, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "36967/36967 [==============================] - 85s 2ms/step - loss: 1.5739\n",
            "STA i am sorry . but i am talking about it since i saw the jet last night . END guess END think i can END END ; END END END END END END END END END END END END END END END have END have no END END END \n",
            "Training epoch: 43, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38361/38361 [==============================] - 89s 2ms/step - loss: 1.6758\n",
            "STA i am sorry jeffery i am glad for you . i am talking to y END , but i am , but many . END END END END END END END END END END END END END END END END i , not \n",
            "Training epoch: 44, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "36967/36967 [==============================] - 86s 2ms/step - loss: 1.5374\n",
            "STA i do not know what the mouse was thinking ? ? why come into a house with 2 ferocious felines ? ? END did a question ? END double for no no END did END END END END END END END END END END END END END i ! \n",
            "Training epoch: 44, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38361/38361 [==============================] - 89s 2ms/step - loss: 1.6339\n",
            "STA i am glad i had in class class of course . i do not know you guys to go to my own beer . END fuck that END fuck END END END END END END END END END END END END END END END ! END END END ! \n",
            "Training epoch: 45, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "36967/36967 [==============================] - 85s 2ms/step - loss: 1.5077\n",
            "STA i dmed the is so much a good END do not go . END wanted to make my own own beer . END . END END END ! END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 45, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38361/38361 [==============================] - 89s 2ms/step - loss: 1.6011\n",
            "STA i am sorry jeffery i am glad for you . i am talking to a boat explanation in a ticket . END usually in a . END END END ! END END END END END END END END END END END END END END END END END ! \n",
            "Training epoch: 46, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "36967/36967 [==============================] - 85s 2ms/step - loss: 1.4742\n",
            "STA i do not know what the mouse was thinking ? ? why come into a house with 2 ferocious felines ? ? END get a nice for sure END did not END END END ! END END ! END END END END END ! END END END i ! \n",
            "Training epoch: 46, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38361/38361 [==============================] - 88s 2ms/step - loss: 1.5688\n",
            "STA i am not feelin class tomorrow rn END ; i am not going to get a ticket to show END show END man END ; END END ! END ! END END END END END END END END END END END END END END END END END ! ! \n",
            "Training epoch: 47, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "36967/36967 [==============================] - 95s 3ms/step - loss: 1.4486\n",
            "STA i do not know what the mouse was thinking ? ? why come into a house with 2 ferocious felines ? ? END did a question ? END double for no reason END did END END END END ! END ! END ! END ! END END i i \n",
            "Training epoch: 47, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38361/38361 [==============================] - 89s 2ms/step - loss: 1.5376\n",
            "STA i am not feelin class tomorrow rn but i definitely could not find to keep my own beer or not END just get a guy END END END END END END END END END END END END END END END END END END END END END END ! \n",
            "Training epoch: 48, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "36967/36967 [==============================] - 85s 2ms/step - loss: 1.4196\n",
            "STA i do not know what the mouse was thinking ? ? why come into a house with 2 ferocious felines ? ? END like a lot for me END did END did END END END END END END END END END END END END END END END i ! \n",
            "Training epoch: 48, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38361/38361 [==============================] - 89s 2ms/step - loss: 1.5126\n",
            "STA i am not feelin class tomorrow rn END ; i definitely could not get it lol END ; ur END ; END get END END END END END END END END END END END END END END END END END END END END END END END ! ! \n",
            "Training epoch: 49, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "36967/36967 [==============================] - 85s 2ms/step - loss: 1.3956\n",
            "STA i do not know what the mouse was thinking ? ? why come into a house with 2 ferocious felines ? ? END get a lot for it END did not END END END END END END END END END END END END END END END END i ! \n",
            "Training epoch: 49, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38361/38361 [==============================] - 88s 2ms/step - loss: 1.4806\n",
            "STA i am not feelin class tomorrow rn END ; i definitely could not find a ticket to a lot of class would would be END get END get END END END END END END END END ! END END ! END END END ! END END END END ! \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ixfIVGjk_y1D",
        "colab_type": "code",
        "outputId": "f5103b3a-4b22-4b35-e12f-311f963cbf40",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 689
        }
      },
      "cell_type": "code",
      "source": [
        "for i in range(20):\n",
        "  print('input: %s'%context[i+20].replace('STA','').replace('END',''))\n",
        "  output = print_result(q[i+20:i+21])\n",
        "  output = output.replace('STA','')\n",
        "  output = re.split('END',output)[0]\n",
        "  print('output: %s'%output)\n",
        "\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input:  u cute now \n",
            "output:  i cant wait :-) i am going to sleep \n",
            "input:  any hot chicks wanna let me touch their but today ? \n",
            "output:  i am not even sleepy \n",
            "input:  i havent groped that ass in a while i need dat \n",
            "output:  i am not watched \"this is us\" and the tent and the speaker lol \n",
            "input:  so much to do before leaving for d .c . ahhh \n",
            "output:  i am not even sleepy \n",
            "input:   ? ? ? \n",
            "output:  i am not even sleepy \n",
            "input:  white girls singing about \"niggers\" stealing should not have had to face any repercussions ?  . . . . . \n",
            "output:  i am not even sleepy \n",
            "input:  they are not 5 year olds . them repeating what they heard is not an excuse . it does not matter if there was malicious intent . \n",
            "output:  i am not even sleepy \n",
            "input:  okay so i know we have had our differences on pineapples on pizza in the past but there is something else that needs attention . . . eggnog \n",
            "output:  i am not feeling any type of way i am just random thoughts in my head as i figure things ? ya feel ? \n",
            "input:  what in the tits is a coquito \n",
            "output:  i am not even sleepy \n",
            "input:  just riding these ol dirt roads . \n",
            "output:  i am not even sleepy \n",
            "input:  k babe . \n",
            "output:  i am not even sleepy \n",
            "input:  hate how y'all try to call me \"fake gay\" no . . trust &amp ; believe vagina is wonderful to me , but so is your father . *hint hint* idiots \n",
            "output:  i have never iowa . never visited , never traveled through , nothing . and . she is so excited . \n",
            "input:  but just plain dinner is lame . . i like to have something sweet sometimes to . they get to eat what they want , why i can not ? \n",
            "output:  i think joanne is missing a big dance hit me up in the 30 years . \n",
            "input:  just one ? \n",
            "output:  i am not even sleepy \n",
            "input:  yeah !  that sounds like the tracy in my timeline .  \n",
            "output:  i am not sure it is a . \n",
            "input:  dont you all het tired calling half of the country racist ? \n",
            "output:  i am not even sleepy \n",
            "input:  yes . good night . \n",
            "output:  i do not know what i am going to see . . . \n",
            "input:  my selfies got bad lighting ? lol \n",
            "output:  i am not sure yet . . . and i have a sewing machine . that is that the same . \n",
            "input:  rightttttt lmfao \n",
            "output:  pig \n",
            "input:  yeah i do \n",
            "output:  i do not know if i am going to take tomorrow tomorrow \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NHA-G3IO2j8j",
        "colab_type": "code",
        "outputId": "1e12570d-2b09-42b0-81fb-4e466179c726",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 6757
        }
      },
      "cell_type": "code",
      "source": [
        "x = range(0,Epochs) \n",
        "valid_loss = np.zeros(Epochs)\n",
        "train_loss = np.zeros(Epochs)\n",
        "for m in range(Epochs):\n",
        "  # Loop over training batches due to memory constraints:\n",
        "  for n in range(0,round_exem,step):\n",
        "    q2 = q[n:n+step]\n",
        "    s = q2.shape\n",
        "    count = 0\n",
        "    for i, sent in enumerate(a[n:n+step]):\n",
        "      l = np.where(sent==1)\n",
        "      limit = l[0][0]\n",
        "      count += limit + 1\n",
        "    Q = np.zeros((count,maxlen_input))\n",
        "    A = np.zeros((count,maxlen_input))\n",
        "    Y = np.zeros((count,dictionary_size))\n",
        "    \n",
        "    count = 0\n",
        "    for i, sent in enumerate(a[n:n+step]):\n",
        "      ans_partial = np.zeros((1,maxlen_input))\n",
        "      # Loop over the positions of the current target output (the current output sequence):\n",
        "      l = np.where(sent==1)\n",
        "      limit = l[0][0]\n",
        "      for k in range(1,limit+1):\n",
        "        # Mapping the target output (the next output word) for one-hot codding:\n",
        "        y = np.zeros((1, dictionary_size))\n",
        "        y[0, sent[k]] = 1\n",
        "        # preparing the partial answer to input:\n",
        "        ans_partial[0,-k:] = sent[0:k]\n",
        "        # training the model for one epoch using teacher forcing:\n",
        "        Q[count, :] = q2[i:i+1] \n",
        "        A[count, :] = ans_partial \n",
        "        Y[count, :] = y\n",
        "        count += 1\n",
        "    print('Training epoch: %d, training examples: %d - %d'%(m,n, n + step))\n",
        "    model.fit([Q, A], Y, batch_size=BatchSize, epochs=1)\n",
        "    test_input = q[6:7]\n",
        "    print(print_result(test_input))\n",
        "  model.save_weights(weights_file, overwrite=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training epoch: 0, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "36967/36967 [==============================] - 87s 2ms/step - loss: 1.3621\n",
            "STA i do not know what the mouse was thinking ? ? why come into a house with 2 ferocious felines ? ? END get a nice with a . END END END END END END END END END END END END END END END END END ! i \n",
            "Training epoch: 0, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38361/38361 [==============================] - 90s 2ms/step - loss: 1.4479\n",
            "STA i am not feelin class tomorrow rn but i definitely could not find out this new house END would not let me lol END get that END END END ! END ! END END END ! END END END END END END END END END END END END ! \n",
            "Training epoch: 1, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "36967/36967 [==============================] - 87s 2ms/step - loss: 1.3318\n",
            "STA i am sure what i am very suspicious of the of the and i just like that i just like i do not get the . END END END END END END END END END END END END END END END END i i like i \n",
            "Training epoch: 1, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38361/38361 [==============================] - 90s 2ms/step - loss: 1.4172\n",
            "STA i am glad for you . i am going to the grand , but alexithymia in the one of obama . END says that i get my pillow END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 2, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "36967/36967 [==============================] - 87s 2ms/step - loss: 1.3029\n",
            "STA i do not know what the mouse was thinking ? ? why come into a house with 2 ferocious felines ? ? END get a nice with it END needles END END END END END END END END END END END END END END END END END ! i \n",
            "Training epoch: 2, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38361/38361 [==============================] - 90s 2ms/step - loss: 1.3838\n",
            "STA i am not feelin class tomorrow rn but i definitely could not find out to new new END get over 6 and . END get END ! END ! END END ! END END END END END END END END END END END END END END ! \n",
            "Training epoch: 3, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "36967/36967 [==============================] - 87s 2ms/step - loss: 1.2772\n",
            "STA i do not know what the mouse was thinking ? ? why come into a house with 2 ferocious felines ? ? END get a for no reason END did END END END END ! END END ! END ! END ! END ! END i i am \n",
            "Training epoch: 3, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38361/38361 [==============================] - 90s 2ms/step - loss: 1.3547\n",
            "STA i am glad for you . i am going to put a nov . END am over to y awake . my schedule . my day my day END go END END END ! END ! END END END END END END END END END END END END END \n",
            "Training epoch: 4, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "36967/36967 [==============================] - 87s 2ms/step - loss: 1.2452\n",
            "STA i do not know what the mouse was thinking ? ? why come into a house with 2 ferocious felines ? ? END get a nice with it END needles END END END END END END ! END END END END END END END END END END i ! \n",
            "Training epoch: 4, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38361/38361 [==============================] - 90s 2ms/step - loss: 1.3231\n",
            "STA i am glad for you . but is just a filler vegetable breasts and . END grapes in END ! END ! END ! END END END ! END END ! END END ! END END END ! END END END END END END ! END ! i \n",
            "Training epoch: 5, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "36967/36967 [==============================] - 87s 2ms/step - loss: 1.2272\n",
            "STA i am sorry for you . i am talking about it . END just getting back in work . END get like END END END END END END END END END END END END END END END END END END END END END END END END END END i \n",
            "Training epoch: 5, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38361/38361 [==============================] - 90s 2ms/step - loss: 1.3014\n",
            "STA i am glad for you . i will not even many things , we are going to win to do at the same of you , you are not just like that . END should not . END END ! END ! END END END END END END END \n",
            "Training epoch: 6, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "36967/36967 [==============================] - 87s 2ms/step - loss: 1.2027\n",
            "STA i do not know what the mouse was thinking ? ? why come into a house with 2 ferocious felines ? ? END get a nice with a tough . END should END should END END ! END END ! END END ! END ! END END i i \n",
            "Training epoch: 6, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38361/38361 [==============================] - 91s 2ms/step - loss: 1.2711\n",
            "STA i am glad for you . i am going to a boat . END kinda like that quick ! END ! END ! END ! END ! END ! END ! END ! END ! END ! END ! END END ! END END END ! i i am \n",
            "Training epoch: 7, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "36967/36967 [==============================] - 87s 2ms/step - loss: 1.1702\n",
            "STA i do not know what the mouse was thinking ? ? why come into a house with 2 ferocious felines ? ? END did anything END double for no question END END END END ! END ! END END ! END END END ! END END END i i \n",
            "Training epoch: 7, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38361/38361 [==============================] - 90s 2ms/step - loss: 1.2436\n",
            "STA i am glad for you . but i am still talking about it . . . END believe you have a great collection . END still still in my pillow . END should not END END END ! END END END END END END END END END END END \n",
            "Training epoch: 8, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "36967/36967 [==============================] - 89s 2ms/step - loss: 1.1516\n",
            "STA i do not know what the mouse was thinking ? ? why come into a house with 2 ferocious felines ? ? END get a nice . END should help END END END END END END ! END END END END END END END END END END i ! \n",
            "Training epoch: 8, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38361/38361 [==============================] - 91s 2ms/step - loss: 1.2152\n",
            "STA i am glad for you . i am glad for you . and your is a explanation of my , but my pillow , my , END END END END END END END END END END END END END END END END ! i \n",
            "Training epoch: 9, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "36967/36967 [==============================] - 87s 2ms/step - loss: 1.1266\n",
            "STA i do not know what the mouse was thinking ? ? why come into a house with 2 ferocious felines ? ? END get a nice END ; END END END ! END ! END ! END END ! END END END END END END END END i ! \n",
            "Training epoch: 9, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38361/38361 [==============================] - 90s 2ms/step - loss: 1.1964\n",
            "STA i am glad for you . i will not even a filler explanation . END always in a new case , and should not even anything END END should END should END ! END ! END ! END ! END ! END END ! END END END END END \n",
            "Training epoch: 10, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "36967/36967 [==============================] - 87s 2ms/step - loss: 1.1137\n",
            "STA i do not know what the mouse was thinking ? ? why come into a house with 2 ferocious felines ? ? END get a lot for it END did not END END END END ! END END END END END END END END END END END i ! \n",
            "Training epoch: 10, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38361/38361 [==============================] - 90s 2ms/step - loss: 1.1709\n",
            "STA i am sorry jeffery i am glad for you . but alexithymia is so much . END should be more all-around ; many than of obama . END END END ! END END END END END END END END END END END END END END END END END ! \n",
            "Training epoch: 11, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "36967/36967 [==============================] - 86s 2ms/step - loss: 1.0842\n",
            "STA i do not know what the mouse was thinking ? ? why come into a house with 2 ferocious felines ? ? END be a guy END ; END END END END ! END END ! END END END END END END END END END END i ! \n",
            "Training epoch: 11, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38361/38361 [==============================] - 89s 2ms/step - loss: 1.1421\n",
            "STA i am glad for you and your family is for your family is for you , but things , and money from work , from work , love w the plot&amp ;other ppl END ppl END END END END END END END END END END END END END ! \n",
            "Training epoch: 12, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "36967/36967 [==============================] - 86s 2ms/step - loss: 1.0592\n",
            "STA i do not know what the mouse was thinking ? ? why come into a house with 2 ferocious felines ? ? END did anything END double for no END END END END END END END END END END END END END END END END END END i ! \n",
            "Training epoch: 12, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38361/38361 [==============================] - 90s 2ms/step - loss: 1.1216\n",
            "STA i am glad for you . i am glad for you . and your is so much ! i hope my my jenny , my jenny (forest gump voice) END END ! END ! END END END END END END END END END END END END i \n",
            "Training epoch: 13, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "36967/36967 [==============================] - 86s 2ms/step - loss: 1.0443\n",
            "STA i do not know what the mouse was thinking ? ? why come into a house with 2 ferocious felines ? ? END get a nice with a . END END END END END END END END END END END END END END END END END i ! \n",
            "Training epoch: 13, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38361/38361 [==============================] - 89s 2ms/step - loss: 1.0942\n",
            "STA i am glad for you and your family is a explanation ! END ! END ! END ! END , and my . END END END END END END END END END END END END END END i ! i i am ! i am \n",
            "Training epoch: 14, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "36967/36967 [==============================] - 86s 2ms/step - loss: 1.0154\n",
            "STA i do not know what the mouse was thinking ? ? why come into a house with 2 ferocious felines ? ? END get a nice END for sure END did END END END ! END END END ! END END END END END END END END i ! \n",
            "Training epoch: 14, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38361/38361 [==============================] - 89s 2ms/step - loss: 1.0702\n",
            "STA i am not trying to convince you , but alexithymia is a beast all-around show . END should be with that . its . END should not . END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 15, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "36967/36967 [==============================] - 86s 2ms/step - loss: 1.0018\n",
            "STA i do not know what the mouse was thinking ? ? why come into a house with 2 ferocious felines ? ? END get a nice END ; END END END END END END END END END END END END END END END END END END END i ! \n",
            "Training epoch: 15, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38361/38361 [==============================] - 89s 2ms/step - loss: 1.0472\n",
            "STA i am not feelin class tomorrow rn but i definitely will not be feeling it in the morning END ; but i am fucking pissed like that END now END END END END END END END END END END END END END END END END END END END i \n",
            "Training epoch: 16, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "36967/36967 [==============================] - 86s 2ms/step - loss: 0.9727\n",
            "STA i do not know what the mouse was thinking ? ? why come into a house with 2 ferocious felines ? ? END get a nice . END END END END END END END END END END END END END END END END END END END END i , \n",
            "Training epoch: 16, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38361/38361 [==============================] - 89s 2ms/step - loss: 1.0205\n",
            "STA i am in class bout to die man END ; i think i am 72 on ss indians in my pillow , but its show END END END END END END END END END END END END END END END END END END END END END END i \n",
            "Training epoch: 17, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "36967/36967 [==============================] - 86s 2ms/step - loss: 0.9628\n",
            "STA i do not know what the mouse was thinking ? ? why come into a house with 2 ferocious felines ? ? END did a double for END END END END END END END END END END END END END END END END END END i , \n",
            "Training epoch: 17, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38361/38361 [==============================] - 89s 2ms/step - loss: 1.0051\n",
            "STA i am not feelin class tomorrow rn but i definitely will not be feeling it in the morning END ; but i am fucking pissed over END still still fuck END END END END END END END END END END END END END END END END END END i \n",
            "Training epoch: 18, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "36967/36967 [==============================] - 86s 2ms/step - loss: 0.9402\n",
            "STA i do not know what the mouse was thinking ? ? why come into a house with 2 ferocious felines ? ? END like a question ? END double for sure END END END END END END END END END END END END END END END END i ! \n",
            "Training epoch: 18, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38361/38361 [==============================] - 89s 2ms/step - loss: 0.9810\n",
            "STA i am glad for you and your family ! no doubt , but alexithymia is a explanation of that , which you have not long known END END ! END ! END END END END END END END END END END END END END END END i ! \n",
            "Training epoch: 19, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "36967/36967 [==============================] - 86s 2ms/step - loss: 0.9266\n",
            "STA i do not know what the mouse was thinking ? ? why come into a house with 2 ferocious felines ? ? END be a guy END END END END END END END END END END END END END END END END END END END END i , \n",
            "Training epoch: 19, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38361/38361 [==============================] - 89s 2ms/step - loss: 0.9590\n",
            "STA i am glad for you and your family ! no doubt , but alexithymia is a explanation of that , which you have not long known END END END END ! END END END END END END END END END END END END END END END ! i \n",
            "Training epoch: 20, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "36967/36967 [==============================] - 86s 2ms/step - loss: 0.9036\n",
            "STA i do not know what the mouse was thinking ? ? why come into a house with 2 ferocious felines ? ? END get a nice END for sure END ; END END END ! END END END END END END END END END END END END i , \n",
            "Training epoch: 20, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38361/38361 [==============================] - 89s 2ms/step - loss: 0.9362\n",
            "STA i am glad for you and your family ! no doubt , but alexithymia is a explanation of that , which you have not long known END END END ! END ! END END END END END END END END END END END END END END ! i \n",
            "Training epoch: 21, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "36967/36967 [==============================] - 86s 2ms/step - loss: 0.8895\n",
            "STA i do not know what the mouse was thinking ? ? why come into a house with 2 ferocious felines ? ? END get a nice with it END needles END END END END END END END END END END END END END END END END END i , \n",
            "Training epoch: 21, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38361/38361 [==============================] - 89s 2ms/step - loss: 0.9275\n",
            "STA i am not feelin class tomorrow rn but i definitely will not be feeling it in the morning END i am over END but i still be doing many END ; END END END END END END END END END END END END END END END END END i \n",
            "Training epoch: 22, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "36967/36967 [==============================] - 86s 2ms/step - loss: 0.8707\n",
            "STA i do not know what the mouse was thinking ? ? why come into a house with 2 ferocious felines ? ? END be a END END END END END END END END END END END END END END END END END END END END END i ! \n",
            "Training epoch: 22, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38361/38361 [==============================] - 89s 2ms/step - loss: 0.9033\n",
            "STA i am glad for you . i am glad for you . and your for you , but things , just have a room , 2 my pillow hope , everything is my day . END END ! END ! END END END END END END END END \n",
            "Training epoch: 23, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "36967/36967 [==============================] - 85s 2ms/step - loss: 0.8549\n",
            "STA i do not know what the mouse was thinking ? ? why come into a house with 2 ferocious felines ? ? END get a nice END needles END END END END END END END END END END END END END END END END END END END i , \n",
            "Training epoch: 23, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38361/38361 [==============================] - 88s 2ms/step - loss: 0.8783\n",
            "STA i am not trying to convince you . money , i am still as real or in the , but i am fucking END ; END should be END ! END END END END END END END END END i . . i am \n",
            "Training epoch: 24, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "36967/36967 [==============================] - 85s 2ms/step - loss: 0.8363\n",
            "STA i do not know what the mouse was thinking ? ? why come into a house with 2 ferocious felines ? ? END be a END END END END END END END END END END END END END END END END END END END END END i ! \n",
            "Training epoch: 24, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38361/38361 [==============================] - 89s 2ms/step - loss: 0.8674\n",
            "STA i am not trying to convince you had a of in the case . END should be a good of her . END END END ! END END END END END END END END END END END END END END END END END END END END i \n",
            "Training epoch: 25, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "36967/36967 [==============================] - 86s 2ms/step - loss: 0.8276\n",
            "STA i do not know what the mouse was thinking ? ? why come into a house with 2 ferocious felines ? ? END get a nice END ; END END END END END END END END END END END END END END END END END END END i ! \n",
            "Training epoch: 25, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38361/38361 [==============================] - 91s 2ms/step - loss: 0.8514\n",
            "STA i am glad for you and your family ! no doubt , but alexithymia is a explanation of that , which you have not long known END END END END ! END END END END END END END END END END END END END END END ! i \n",
            "Training epoch: 26, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "36967/36967 [==============================] - 87s 2ms/step - loss: 0.8139\n",
            "STA i do not know what the mouse was thinking ? ? why come into a house with 2 ferocious felines ? ? END get a nice with it END needles END END END END END END END END END END END END END END END END END i , \n",
            "Training epoch: 26, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38361/38361 [==============================] - 90s 2ms/step - loss: 0.8352\n",
            "STA i am glad for you and your family ! no doubt , but alexithymia is a explanation of that , which you have not long known END END END END END END END END END END END END END END END END END END END END ! i \n",
            "Training epoch: 27, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "36967/36967 [==============================] - 87s 2ms/step - loss: 0.7986\n",
            "STA i do not know what the mouse was thinking ? ? why come into a house with 2 ferocious felines ? ? END be a guy END END END END END END END END END END END END END END END END END END END END i i \n",
            "Training epoch: 27, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38361/38361 [==============================] - 90s 2ms/step - loss: 0.8150\n",
            "STA i am glad for you and your family ! no doubt , but alexithymia is a explanation of that , which you have not long known END END END END ! END END END END END END END END END END END END END END END i ! \n",
            "Training epoch: 28, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "36967/36967 [==============================] - 87s 2ms/step - loss: 0.7882\n",
            "STA i do not know what the mouse was thinking ? ? why come into a house with 2 ferocious felines ? ? END get a nice END ; END END END END END END END END END END END END END END END END END END END i , \n",
            "Training epoch: 28, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38361/38361 [==============================] - 92s 2ms/step - loss: 0.8044\n",
            "STA i appreciate the text to my and . i think i believe) END do more text for the . END do not even END END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 29, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "36967/36967 [==============================] - 89s 2ms/step - loss: 0.7766\n",
            "STA i do not know what the mouse was thinking ? ? why come into a house with 2 ferocious felines ? ? END be a END ; END END END END END END END END END END END END END END END END END END END i i \n",
            "Training epoch: 29, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38361/38361 [==============================] - 91s 2ms/step - loss: 0.7880\n",
            "STA i appreciate the text END do not that you END END ! END ! END ! END ! END ! END ! END ! END ! END END ! END END ! END END ! END ! i END END , i have not . END END END END \n",
            "Training epoch: 30, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "36967/36967 [==============================] - 87s 2ms/step - loss: 0.7629\n",
            "STA i do not know what the mouse was thinking ? ? why come into a house with 2 ferocious felines ? ? END be a tough guy END ; END END END END END END END END END END END END END END END END END END i i \n",
            "Training epoch: 30, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38361/38361 [==============================] - 90s 2ms/step - loss: 0.7702\n",
            "STA i appreciate that by you , but its not down END do not american cap are not it END do not END expect END END END END END END END END END END END END END END END END END END END END END END i \n",
            "Training epoch: 31, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "36967/36967 [==============================] - 87s 2ms/step - loss: 0.7506\n",
            "STA i do not know what the mouse was thinking ? ? why come into a house with 2 ferocious felines ? ? END be a guy END END END END END END END END END END END END END END END END END END END END i i \n",
            "Training epoch: 31, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38361/38361 [==============================] - 91s 2ms/step - loss: 0.7563\n",
            "STA i am glad for you and your family ! no doubt , but alexithymia is a explanation of that , which you have not long known END END ! END END END END END END END END END END END END END END END END . i ! \n",
            "Training epoch: 32, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "36967/36967 [==============================] - 88s 2ms/step - loss: 0.7366\n",
            "STA i do not know what the mouse was thinking ? ? why come into a house with 2 ferocious felines ? ? END be a tough for sure . END question END END END END ! END END ! END END END END END END END END i i \n",
            "Training epoch: 32, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38361/38361 [==============================] - 91s 2ms/step - loss: 0.7426\n",
            "STA i am glad for you and your family ! no doubt , but alexithymia is a explanation of that , which you have not long known END END END ! END END END END END END END END END END END END END END END END i ! \n",
            "Training epoch: 33, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "36967/36967 [==============================] - 87s 2ms/step - loss: 0.7266\n",
            "STA i do not know what the mouse was thinking ? ? why come into a house with 2 ferocious felines ? ? END be a nice END ; END END END END END END END END END lol END END END END END END END END END i , \n",
            "Training epoch: 33, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38361/38361 [==============================] - 90s 2ms/step - loss: 0.7322\n",
            "STA i am glad for you and your family ! no doubt , but alexithymia is a explanation of that , which you have not long known END END END END ! END END END END END END END END END END END END END END END i ! \n",
            "Training epoch: 34, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "36967/36967 [==============================] - 87s 2ms/step - loss: 0.7113\n",
            "STA i do not know what the mouse was thinking ? ? why come into a house with 2 ferocious felines ? ? END be a tough for sure END ; END ; END END END END END END END END END END END END END END END , i \n",
            "Training epoch: 34, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38361/38361 [==============================] - 90s 2ms/step - loss: 0.7129\n",
            "STA i am glad for you . i will not allow you in the . END fuck you END ! END ! END ! END ! END ! END END END END END END END END END END END END END END END END END END END i . \n",
            "Training epoch: 35, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "36967/36967 [==============================] - 87s 2ms/step - loss: 0.6998\n",
            "STA i do not know what the mouse was thinking ? ? why come into a house with 2 ferocious felines ? ? END be a nice END ; END END END END END END END END END END END END END END END END END END END i i \n",
            "Training epoch: 35, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38361/38361 [==============================] - 90s 2ms/step - loss: 0.7048\n",
            "STA i am glad for you and your family ! no doubt , but alexithymia is a explanation of that , which you have not long known END END END ! END END END END END END END END END END END END END END END END i ! \n",
            "Training epoch: 36, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "36967/36967 [==============================] - 87s 2ms/step - loss: 0.6875\n",
            "STA i do not know what the mouse was thinking ? ? why come into a house with 2 ferocious felines ? ? END be a nice END ; END END END END END END END END END END END END END END END END END END END i i \n",
            "Training epoch: 36, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38361/38361 [==============================] - 90s 2ms/step - loss: 0.6880\n",
            "STA i am glad for you and your family ! no doubt , but alexithymia is a explanation of that , which you have not long known END END END END END END END END END END END END END END END END END END END END i i \n",
            "Training epoch: 37, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "36967/36967 [==============================] - 87s 2ms/step - loss: 0.6740\n",
            "STA i do not know what the mouse was thinking ? ? why come into a house with 2 ferocious felines ? ? END be a END ; END END END END END END END END END END END END END END END END END END END i i \n",
            "Training epoch: 37, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38361/38361 [==============================] - 98s 3ms/step - loss: 0.6717\n",
            "STA i appreciate the text END do not that its that it . END change by the same . END should should be on END ? END END END END END END END END END END END END END END END END END END END END END END \n",
            "Training epoch: 38, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "36967/36967 [==============================] - 88s 2ms/step - loss: 0.6746\n",
            "STA i do not know what the mouse was thinking ? ? why come into a house with 2 ferocious felines ? ? END be for a tough END ; END END END END END END END END END END END END END END END END END i , \n",
            "Training epoch: 38, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38361/38361 [==============================] - 91s 2ms/step - loss: 0.6743\n",
            "STA i appreciate that by all . . . . END believe you , and i think is more with yourself . END do not want to go home END END ; END still END END ! END END END END END END END END END END END END END \n",
            "Training epoch: 39, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "36967/36967 [==============================] - 88s 2ms/step - loss: 0.6578\n",
            "STA i do not know what the mouse was thinking ? ? why come into a house with 2 ferocious felines ? ? END be ? END double END END END END END END END END END END END END END END END END END END END END i i \n",
            "Training epoch: 39, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38361/38361 [==============================] - 91s 2ms/step - loss: 0.6547\n",
            "STA i am glad for you and your family ! no doubt , but alexithymia is a explanation of that , which you have not long known END END END END END END END END END END END END END END END END END END END END i ! \n",
            "Training epoch: 40, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "36967/36967 [==============================] - 87s 2ms/step - loss: 0.6561\n",
            "STA i do not know what the mouse was thinking ? ? why come into a house with 2 ferocious felines ? ? END be a tough END END END END END END END END END END END END END END END END END END END END i i \n",
            "Training epoch: 40, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38361/38361 [==============================] - 91s 2ms/step - loss: 0.6465\n",
            "STA i am glad for you and your family ! no doubt , but alexithymia is a explanation of that , which you have not long known END END END END END END END END END END END END END END END END END END END END i ! \n",
            "Training epoch: 41, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "36967/36967 [==============================] - 88s 2ms/step - loss: 0.6446\n",
            "STA i do not know what the mouse was thinking ? ? why come into a house with 2 ferocious felines ? ? END be for a tough END END END END END END END END ! END END END END END END END END END END i i \n",
            "Training epoch: 41, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38361/38361 [==============================] - 92s 2ms/step - loss: 0.6375\n",
            "STA i am glad for you and your family ! no doubt , but alexithymia is a explanation of that , which you have not long known END END END END END END END END END END END END END END END END END END END END i i \n",
            "Training epoch: 42, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "36967/36967 [==============================] - 88s 2ms/step - loss: 0.6391\n",
            "STA i do not know what the mouse was thinking ? ? why come into a house with 2 ferocious felines ? ? END be for a tough END ; END END END END END END END END END END END END END END END END END i i \n",
            "Training epoch: 42, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38361/38361 [==============================] - 91s 2ms/step - loss: 0.6257\n",
            "STA i am glad for you and your family ! no doubt , but alexithymia is a explanation of that , which you have not long known END END END ! END ! END END END END END END END END END END END END END END i ! \n",
            "Training epoch: 43, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "36967/36967 [==============================] - 88s 2ms/step - loss: 0.6285\n",
            "STA i do not know what the mouse was thinking ? ? why come into a house with 2 ferocious felines ? ? END be ? END double for sure END END END END END END END END END END END END END END END END END END i i \n",
            "Training epoch: 43, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38361/38361 [==============================] - 92s 2ms/step - loss: 0.6209\n",
            "STA i appreciate the text END do not that END END END ! END ! END ! END ! END END END ! END END END END END ! END END END END END END END END ! END END ! i END i i i am i am not \n",
            "Training epoch: 44, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "36967/36967 [==============================] - 88s 2ms/step - loss: 0.6220\n",
            "STA i do not know what the mouse was thinking ? ? why come into a house with 2 ferocious felines ? ? END be for a tough END ; END thing END END END END END END ! END END END END END END END END END i i \n",
            "Training epoch: 44, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38361/38361 [==============================] - 91s 2ms/step - loss: 0.6102\n",
            "STA i am glad for you and your family ! no doubt , but alexithymia is a explanation of that , which you have not long known END END END END ! END END END END END END END END END END END END END END END i ! \n",
            "Training epoch: 45, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "36967/36967 [==============================] - 88s 2ms/step - loss: 0.6135\n",
            "STA i do not know what the mouse was thinking ? ? why come into a house with 2 ferocious felines ? ? END be ? END double END END END END END END END END END END END END END END END END END END END END i i \n",
            "Training epoch: 45, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38361/38361 [==============================] - 91s 2ms/step - loss: 0.6098\n",
            "STA i appreciate that by the way END do not even that its . parts are not american that . END not END ! END END END END END END END END END END END END END END END END END END END END END END END i ! \n",
            "Training epoch: 46, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "36967/36967 [==============================] - 88s 2ms/step - loss: 0.6110\n",
            "STA i do not know what the mouse was thinking ? ? why come into a house with 2 ferocious felines ? ? END be for a tough END ; END END END END END END END END END END END END END END END END END i i \n",
            "Training epoch: 46, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38361/38361 [==============================] - 91s 2ms/step - loss: 0.5930\n",
            "STA i appreciate that by the way END do not even sold this . i am sure they are talk to win but at this . END usually END still for that . END should a good . END should END should END END END END END END END END \n",
            "Training epoch: 47, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "36967/36967 [==============================] - 88s 2ms/step - loss: 0.5999\n",
            "STA i do not know what the mouse was thinking ? ? why come into a house with 2 ferocious felines ? ? END be for a tough END ; END END END END END END END END END END END END END END END END END i i \n",
            "Training epoch: 47, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38361/38361 [==============================] - 91s 2ms/step - loss: 0.5864\n",
            "STA i am glad for you and your family ! no doubt , but alexithymia is a explanation of that , which you have not long known END END END END END END END END END END END END END END END END END END END END i ! \n",
            "Training epoch: 48, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "36967/36967 [==============================] - 88s 2ms/step - loss: 0.5931\n",
            "STA i do not know what the mouse was thinking ? ? why come into a house with 2 ferocious felines ? ? END be a nice END END END END END END END END END END END END END END END END END END END END END i i \n",
            "Training epoch: 48, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38361/38361 [==============================] - 91s 2ms/step - loss: 0.5761\n",
            "STA i am glad for you and your family ! no doubt , but alexithymia is a explanation of that , which you have not long known END END END END END END END END END END END END END END END END END END END END i , \n",
            "Training epoch: 49, training examples: 0 - 2628\n",
            "Epoch 1/1\n",
            "36967/36967 [==============================] - 88s 2ms/step - loss: 0.5912\n",
            "STA i do not know what the mouse was thinking ? ? why come into a house with 2 ferocious felines ? ? END be a nice END END END END END END END END ! END END END END END END END END END END END END i i \n",
            "Training epoch: 49, training examples: 2628 - 5256\n",
            "Epoch 1/1\n",
            "38361/38361 [==============================] - 91s 2ms/step - loss: 0.5710\n",
            "STA i am not feelin class tomorrow rn but i definitely will not be feeling it in the morning END ; i am over in a day but i had rather many END ; thx END END END END END END END END END END END END END END i \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "OGWIZpa9bboE",
        "colab_type": "code",
        "outputId": "0117c546-014a-4245-8a65-23fcd961ba4e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 689
        }
      },
      "cell_type": "code",
      "source": [
        "for i in range(20):\n",
        "  print('input: %s'%context[i].replace('STA','').replace('END',''))\n",
        "  output = print_result(q[i:i+1])\n",
        "  output = output.replace('STA','')\n",
        "  output = re.split('END',output)[0]\n",
        "  print('output: %s'%output)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input:  what is up dadyo when did you get back on twitter ? haha \n",
            "output:  i am not , i will not allow it . , there is not much to do at this time of the night . \n",
            "input:  literally never about that account , love it . \n",
            "output:  i am ready for them 7 12 is \n",
            "input:  about 50 times today . terminal vim user . \n",
            "output:  i am not even the basketball fan but this stuff me off its like are they that slow \n",
            "input:  cmd+opt+esc is good but still available via menubar \n",
            "output:  i am not sure yet , i might go \n",
            "input:  i am disgusted \n",
            "output:  i am not even the basketball fan but this stuff me off its like are they that slow \n",
            "input:  what a piece of shit \n",
            "output:  i am not even the basketball fan but this stuff me off its like are they that slow \n",
            "input:  yay , you great hunter . ive killed lots of lizards and bugs but never a mouse . \n",
            "output:  i am not feelin class tomorrow rn but i definitely will not be feeling it in the morning \n",
            "input:  and then that mouse had the nerve to try to eat our kibble !  let this be a lesson fur all the other mousies !   \n",
            "output:  i am not even the basketball fan but this stuff me off its like are they that slow \n",
            "input:  tomorrow \n",
            "output:  i am not even the basketball fan but this stuff me off its like are they that slow \n",
            "input:  make sure i have a bed and seat saved next to you ! \n",
            "output:  it is okay but is b sides every of my emo ass \n",
            "input:  wassup shorty .  \n",
            "output:  i am not even the basketball fan but this stuff me off its like are they that slow \n",
            "input:  appreciate that shorty , you too .  \n",
            "output:  i loveeeeeeeeeeee this fuck \n",
            "input:  yea \n",
            "output:  i am not even the basketball fan but this stuff me off its like are they that slow \n",
            "input:  gotchu \n",
            "output:  i am not even the basketball fan but this stuff me off its like are they that slow \n",
            "input:  good  wby \n",
            "output:  i cant wait :-) i do not have a costume yet tho \n",
            "input:  that is wassup  \n",
            "output:  i am not even the basketball fan but this stuff me off its like are they that slow \n",
            "input:  and the dash for cash races too \n",
            "output:  i am not even the basketball fan but this stuff me off its like are they that slow \n",
            "input:  do you think this wouldve happened anyway if it was another driver stinkin it up like kyle has recently ? \n",
            "output:  i am sorry , but there are so many things wrong with this tweet . \n",
            "input:  what did these niggas say \n",
            "output:  i am not even the basketball fan but this stuff me off its like are they that slow \n",
            "input:  kenny is wife is white on top of that . u not gonna tell me u put a flag over the color of your skin . \n",
            "output:  i am such a crafty/creative person , that when i do not do anything creative i feel sad . and i want to do so many things but can not decide \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "GKjidQewIBLK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        },
        "outputId": "493fb2cc-5d3e-4396-c850-ab75b9ecdc2a"
      },
      "cell_type": "code",
      "source": [
        "!pip install boto3"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (1.9.128)\n",
            "Requirement already satisfied: botocore<1.13.0,>=1.12.128 in /usr/local/lib/python3.6/dist-packages (from boto3) (1.12.128)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3) (0.9.4)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3) (0.2.0)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.128->boto3) (2.5.3)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.20; python_version >= \"3.4\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.128->boto3) (1.22)\n",
            "Requirement already satisfied: docutils>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.128->boto3) (0.14)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\"->botocore<1.13.0,>=1.12.128->boto3) (1.11.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "8YnOOctnHlqN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import boto3\n",
        "import json\n",
        "comprehend = boto3.client(service_name='comprehend', \n",
        "                          region_name='us-east-2', \n",
        "                          aws_access_key_id ='AKIAIGO76KMLNGYLNDOQ',\n",
        "                          aws_secret_access_key='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')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3Ph70J3THpX4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_sentiment(text):\n",
        "    sentiment = comprehend.detect_sentiment(Text=text, LanguageCode='en')['Sentiment']\n",
        "    if(sentiment == 'POSITIVE'):\n",
        "        sentiment = 1\n",
        "    elif(sentiment == 'NEGATIVE' or 'NEUTRAL' or 'MIXED'):\n",
        "        sentiment = 0\n",
        "    return sentiment"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bwuuRfDIH59D",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4237
        },
        "outputId": "1174e10f-004d-4b07-c7c4-78f70819dac0"
      },
      "cell_type": "code",
      "source": [
        "test = []\n",
        "origin = []\n",
        "for i in range(50):\n",
        "    print('input: %s'%context[i].replace('STA','').replace('END',''))\n",
        "    output = print_result(q[i:i+1])\n",
        "    output = output.replace('STA','')\n",
        "    output = re.split('END',output)[0]\n",
        "    score = get_sentiment(output)\n",
        "    test.append(score)\n",
        "    print('output: %s'%output)\n",
        "    print('sentiment: %s'%score)\n",
        "    original_output = answers[i].replace('STA', '').replace('END', '')\n",
        "    original_sentiment = get_sentiment(original_output)\n",
        "    origin.append(original_sentiment)\n",
        "    print('original_output: %s' %original_output)\n",
        "    print('original_sentiment: %s' %original_sentiment)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input:  what is up dadyo when did you get back on twitter ? haha \n",
            "output:  i am not , i will not allow it . , there is not much to do at this time of the night . \n",
            "sentiment: 0\n",
            "original_output:  like 2 weeks ago and it is going as terribly as i remember , but deg is still hilarious so it is ok \n",
            "original_sentiment: 0\n",
            "input:  literally never about that account , love it . \n",
            "output:  i am ready for them 7 12 is \n",
            "sentiment: 0\n",
            "original_output:  answer me this fellow apple peoples : how many times in the past year have you used the escape key ? \n",
            "original_sentiment: 0\n",
            "input:  about 50 times today . terminal vim user . \n",
            "output:  i am not even the basketball fan but this stuff me off its like are they that slow \n",
            "sentiment: 0\n",
            "original_output:  seems the major complaints so far are from vim users like yourself . im wondering how force quit is gonna work . \n",
            "original_sentiment: 0\n",
            "input:  cmd+opt+esc is good but still available via menubar \n",
            "output:  i am not sure yet , i might go \n",
            "sentiment: 0\n",
            "original_output:  there was a greasy kid at highline who was basically miles dipped in a bucket of ranch \n",
            "original_sentiment: 0\n",
            "input:  i am disgusted \n",
            "output:  i am not even the basketball fan but this stuff me off its like are they that slow \n",
            "sentiment: 0\n",
            "original_output:  he flashed us then we scored so he sadly put his shirt back on #fuckhighline \n",
            "original_sentiment: 0\n",
            "input:  what a piece of shit \n",
            "output:  i am not even the basketball fan but this stuff me off its like are they that slow \n",
            "sentiment: 0\n",
            "original_output:  i killed a mouse today !   \n",
            "original_sentiment: 0\n",
            "input:  yay , you great hunter . ive killed lots of lizards and bugs but never a mouse . \n",
            "output:  i am not feelin class tomorrow rn but i definitely will not be feeling it in the morning \n",
            "sentiment: 0\n",
            "original_output:  i do not know what the mouse was thinking ? ?  why come into a house with 2 ferocious felines ? ? \n",
            "original_sentiment: 0\n",
            "input:  and then that mouse had the nerve to try to eat our kibble !  let this be a lesson fur all the other mousies !   \n",
            "output:  i am not even the basketball fan but this stuff me off its like are they that slow \n",
            "sentiment: 0\n",
            "original_output:  what day are you coming to effie ? ? \n",
            "original_sentiment: 0\n",
            "input:  tomorrow \n",
            "output:  i am not even the basketball fan but this stuff me off its like are they that slow \n",
            "sentiment: 0\n",
            "original_output:  im leaving today once i get off work \n",
            "original_sentiment: 0\n",
            "input:  make sure i have a bed and seat saved next to you ! \n",
            "output:  it is okay but is b sides every of my emo ass \n",
            "sentiment: 0\n",
            "original_output:  avi  hey boo \n",
            "original_sentiment: 0\n",
            "input:  wassup shorty .  \n",
            "output:  i am not even the basketball fan but this stuff me off its like are they that slow \n",
            "sentiment: 0\n",
            "original_output:  you are beautiful ma  ! \n",
            "original_sentiment: 1\n",
            "input:  appreciate that shorty , you too .  \n",
            "output:  i loveeeeeeeeeeee this fuck \n",
            "sentiment: 0\n",
            "original_output:  does anyone have white spandex \n",
            "original_sentiment: 1\n",
            "input:  yea \n",
            "output:  i am not even the basketball fan but this stuff me off its like are they that slow \n",
            "sentiment: 0\n",
            "original_output:  bet lemme borrow them \n",
            "original_sentiment: 0\n",
            "input:  gotchu \n",
            "output:  i am not even the basketball fan but this stuff me off its like are they that slow \n",
            "sentiment: 0\n",
            "original_output:  what is going on fooly how u been doing \n",
            "original_sentiment: 0\n",
            "input:  good  wby \n",
            "output:  i cant wait :-) i do not have a costume yet tho \n",
            "sentiment: 0\n",
            "original_output:  working other than that living life you kno how that goes  \n",
            "original_sentiment: 0\n",
            "input:  that is wassup  \n",
            "output:  i am not even the basketball fan but this stuff me off its like are they that slow \n",
            "sentiment: 0\n",
            "original_output:  my take on new rule - does not go far enough . cup drivers will still dominate nxs . but happy they are out for chase races #smallvictories \n",
            "original_sentiment: 0\n",
            "input:  and the dash for cash races too \n",
            "output:  i am not even the basketball fan but this stuff me off its like are they that slow \n",
            "sentiment: 0\n",
            "original_output:  yes - although most (or some) of those are nd-alones anyway \n",
            "original_sentiment: 0\n",
            "input:  do you think this wouldve happened anyway if it was another driver stinkin it up like kyle has recently ? \n",
            "output:  i am sorry , but there are so many things wrong with this tweet . \n",
            "sentiment: 0\n",
            "original_output:  ernie lost a bit of my respect too , just when you think hey this white guy gets it . . . . nope \n",
            "original_sentiment: 0\n",
            "input:  what did these niggas say \n",
            "output:  i am not even the basketball fan but this stuff me off its like are they that slow \n",
            "sentiment: 0\n",
            "original_output:  kennys black , not for him . then ernie joins in \"i had never , it is time to nd together we will never be perfect so do not kneel\" \n",
            "original_sentiment: 0\n",
            "input:  kenny is wife is white on top of that . u not gonna tell me u put a flag over the color of your skin . \n",
            "output:  i am such a crafty/creative person , that when i do not do anything creative i feel sad . and i want to do so many things but can not decide \n",
            "sentiment: 1\n",
            "original_output:  i will be back cute in 2 weeks \n",
            "original_sentiment: 1\n",
            "input:  u cute now \n",
            "output:  i just watched the gilmore girls trailer and i am sobbing \n",
            "sentiment: 0\n",
            "original_output:   on the with irs \n",
            "original_sentiment: 0\n",
            "input:  any hot chicks wanna let me touch their but today ? \n",
            "output:  i am not even the basketball fan but this stuff me off its like are they that slow \n",
            "sentiment: 0\n",
            "original_output:  i got u \n",
            "original_sentiment: 0\n",
            "input:  i havent groped that ass in a while i need dat \n",
            "output:  i knew liz carpenter . if you look at that photo , ann was down and talking to liz carpenter . i wrote an article , 2 . \n",
            "sentiment: 0\n",
            "original_output:  lmfao \n",
            "original_sentiment: 1\n",
            "input:  so much to do before leaving for d .c . ahhh \n",
            "output:  i am not even the basketball fan but this stuff me off its like are they that slow \n",
            "sentiment: 0\n",
            "original_output:  i have stuff here \n",
            "original_sentiment: 0\n",
            "input:   ? ? ? \n",
            "output:  i am not even the basketball fan but this stuff me off its like are they that slow \n",
            "sentiment: 0\n",
            "original_output:  like if you forget to pack something \n",
            "original_sentiment: 0\n",
            "input:  white girls singing about \"niggers\" stealing should not have had to face any repercussions ?  . . . . . \n",
            "output:  i am not even the basketball fan but this stuff me off its like are they that slow \n",
            "sentiment: 0\n",
            "original_output:  nope bc evidently it was 1) a famous vine they repeated 2) there was no malicious intent 3) it was all freaking jokes \n",
            "original_sentiment: 0\n",
            "input:  they are not 5 year olds . them repeating what they heard is not an excuse . it does not matter if there was malicious intent . \n",
            "output:  i am not even the basketball fan but this stuff me off its like are they that slow \n",
            "sentiment: 0\n",
            "original_output:  of every race say that word all the time to their fris . there should have been no repercussions , if there had to be then \n",
            "original_sentiment: 0\n",
            "input:  okay so i know we have had our differences on pineapples on pizza in the past but there is something else that needs attention . . . eggnog \n",
            "output:  i have changed my mind , some good tracks have grown on me . \n",
            "sentiment: 1\n",
            "original_output:  i fucks with coquito thoughhh . which is prob the same thing but who cares it sounds better lmao \n",
            "original_sentiment: 0\n",
            "input:  what in the tits is a coquito \n",
            "output:  i am not even the basketball fan but this stuff me off its like are they that slow \n",
            "sentiment: 0\n",
            "original_output:  puerto rican eggnog shit is f i r e \n",
            "original_sentiment: 0\n",
            "input:  just riding these ol dirt roads . \n",
            "output:  i am not even the basketball fan but this stuff me off its like are they that slow \n",
            "sentiment: 0\n",
            "original_output:  bitch . \n",
            "original_sentiment: 0\n",
            "input:  k babe . \n",
            "output:  i am not even the basketball fan but this stuff me off its like are they that slow \n",
            "sentiment: 0\n",
            "original_output:  where you at \n",
            "original_sentiment: 0\n",
            "input:  hate how y'all try to call me \"fake gay\" no . . trust &amp ; believe vagina is wonderful to me , but so is your father . *hint hint* idiots \n",
            "output:  i have never iowa . never visited , never traveled through , nothing . the school i am interviewing at is in des moines . \n",
            "sentiment: 0\n",
            "original_output:   yeah but some people do not like when you like dessert . . . they just want you to like the dinner \n",
            "original_sentiment: 0\n",
            "input:  but just plain dinner is lame . . i like to have something sweet sometimes to . they get to eat what they want , why i can not ? \n",
            "output:  i try not to care but you know me i care too much for people who do not deserve it \n",
            "sentiment: 0\n",
            "original_output:  whatever , i am gonna uncork this bottle and have me a glass of wine . .  \n",
            "original_sentiment: 0\n",
            "input:  just one ? \n",
            "output:  i am not even the basketball fan but this stuff me off its like are they that slow \n",
            "sentiment: 0\n",
            "original_output:  naw . . .already on my 2nd glass lol \n",
            "original_sentiment: 0\n",
            "input:  yeah !  that sounds like the tracy in my timeline .  \n",
            "output:  i am off friday but i picked up this shift from somebody i get off at 9 . how far is it ? \n",
            "sentiment: 0\n",
            "original_output:  you are a crazy man trump , you speak without thinking what you are going to say . and your supporters are very racist . \n",
            "original_sentiment: 0\n",
            "input:  dont you all het tired calling half of the country racist ? \n",
            "output:  i am not even the basketball fan but this stuff me off its like are they that slow \n",
            "sentiment: 0\n",
            "original_output:  are you finish for the night ? \n",
            "original_sentiment: 0\n",
            "input:  yes . good night . \n",
            "output:  i do not know if i am going to take tomorrow off or not . just have to see how i feel in the morning . \n",
            "sentiment: 0\n",
            "original_output:  bad lighting fit selfies &gt ; \n",
            "original_sentiment: 0\n",
            "input:  my selfies got bad lighting ? lol \n",
            "output:  i do not know if i am offed or happy . what does that mean lmao \n",
            "sentiment: 0\n",
            "original_output:  nah my lighting on my phone was all the way down lmao \n",
            "original_sentiment: 0\n",
            "input:  rightttttt lmfao \n",
            "output:  me too , how do you like nccu so far ? \n",
            "sentiment: 0\n",
            "original_output:  you work thursday  ? \n",
            "original_sentiment: 0\n",
            "input:  yeah i do \n",
            "output:  i just watched the gilmore girls trailer and i am sobbing \n",
            "sentiment: 0\n",
            "original_output:  awesome ! i was thinking about sping some time there to work on my schedule . think there would be room for me ? \n",
            "original_sentiment: 1\n",
            "input:  yeah there should be room . usually the desks behind where i sit . \n",
            "output:  i have not watched it yet . so i have not sure . it is good good \n",
            "sentiment: 1\n",
            "original_output:  jordan is fucking retarded \n",
            "original_sentiment: 0\n",
            "input:  i will beat your ass  \n",
            "output:  i am not even the basketball fan but this stuff me off its like are they that slow \n",
            "sentiment: 0\n",
            "original_output:  i am waiting \n",
            "original_sentiment: 0\n",
            "input:  i am near the gym fight me pussy \n",
            "output:  i am not even the basketball fan but this stuff me off its like are they that slow \n",
            "sentiment: 0\n",
            "original_output:  happy birthday kid . read this in 8 hours \n",
            "original_sentiment: 1\n",
            "input:  alright kiddo . 8 hours . \n",
            "output:  i heard someone saying my name but no one up \n",
            "sentiment: 0\n",
            "original_output:  okay great . now you can scream all the thank you you want . wait there is no cake . . . keep it . give me my happy birthday back  \n",
            "original_sentiment: 1\n",
            "input:  i bought you a fudge brownie . it counts as cake . \n",
            "output:  i am not even the basketball fan but this stuff me off its like are they that slow \n",
            "sentiment: 0\n",
            "original_output:  i had the filthiest dream about one of my followers . filthy . like , i need to delete my account kind of filthy . it was that disgusting . smfh \n",
            "original_sentiment: 0\n",
            "input:  i see \n",
            "output:  i am not even the basketball fan but this stuff me off its like are they that slow \n",
            "sentiment: 0\n",
            "original_output:  i think you are the bee is knees , but no . i can not even go along with this . \n",
            "original_sentiment: 0\n",
            "input:  sooo . . . . did everyone on uapb campus tv is loose signal ? \n",
            "output:  i am not even the basketball fan but this stuff me off its like are they that slow \n",
            "sentiment: 0\n",
            "original_output:  girl yeah . \n",
            "original_sentiment: 0\n",
            "input:  well idk but mine did \n",
            "output:  i do not know if i am going to take tomorrow off or not . just have to see how i feel in the morning . \n",
            "sentiment: 0\n",
            "original_output:  my ls at the jbj said she did too \n",
            "original_sentiment: 0\n",
            "input:  will you be at the phi delts friday ? ? ? \n",
            "output:  i am not even the basketball fan but this stuff me off its like are they that slow \n",
            "sentiment: 0\n",
            "original_output:  is that their halloween party ?  if you go i could maybe make an appearance  . i miss you ! \n",
            "original_sentiment: 0\n",
            "input:  yes it is and yes i am !  \n",
            "output:  i am right here bro wassup \n",
            "sentiment: 0\n",
            "original_output:  i coach till 9 :30 but i could drive up ! \n",
            "original_sentiment: 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "KrESS8kNH-gA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 502
        },
        "outputId": "65331a0d-95e0-4613-b4dc-df863eb1f564"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from mlxtend.plotting import plot_confusion_matrix\n",
        "cm=confusion_matrix(origin, test)\n",
        "plot_confusion_matrix(cm,figsize=(12,8), hide_ticks=True,cmap=plt.cm.Blues)\n",
        "plt.xticks(range(2), ['NON-POSITIVE','POSITIVE'], fontsize=16)\n",
        "plt.yticks(range(2), ['NON-POSITIVE','POSITIVE'], fontsize=16)\n",
        "plt.show()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkgAAAHlCAYAAAAOdAMCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 864x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment4_test.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Allen001822480/Info7374SpringTeam5/blob/Assignment4/Assignment4_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "pnYyrLTNDIZG",
        "colab_type": "code",
        "outputId": "f9a40565-57ef-41e8-f63b-d2516cd49b17",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "cell_type": "code",
      "source": [
        "import theano\n",
        "import os.path\n",
        "import sys\n",
        "import nltk\n",
        "import re\n",
        "import time\n",
        "import nltk\n",
        "import _pickle\n",
        "nltk.download('punkt')\n",
        "\n",
        "from keras.utils import plot_model\n",
        "\n",
        "word_embedding_size = 100\n",
        "sentence_embedding_size = 300\n",
        "dictionary_size = 10000\n",
        "maxlen_input = 50\n",
        "\n",
        "vocabulary_file = 'vocabulary_twitter'\n",
        "weights_file = 'assignment4_twitter.h5'\n",
        "unknown_token = 'something'\n",
        "file_saved_context = 'saved_context'\n",
        "file_saved_answer = 'saved_answer'\n",
        "name_of_computer = 'team5_chatbot'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "ylLpvkGMDuII",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def greedy_decoder(input):\n",
        "\n",
        "    flag = 0\n",
        "    prob = 1\n",
        "    ans_partial = np.zeros((1,maxlen_input))\n",
        "    ans_partial[0, -1] = 0  #  the index of the symbol STA (begin of sentence)\n",
        "    for k in range(maxlen_input - 1):\n",
        "        ye = model.predict([input, ans_partial])\n",
        "        yel = ye[0,:]\n",
        "        p = np.max(yel)\n",
        "        mp = np.argmax(ye)\n",
        "        ans_partial[0, 0:-1] = ans_partial[0, 1:]\n",
        "        ans_partial[0, -1] = mp\n",
        "        if mp == 1:  #  he index of the symbol END (end of sentence)\n",
        "            flag = 1\n",
        "        if flag == 0:    \n",
        "            prob = prob * p\n",
        "    text = ''\n",
        "    for k in ans_partial[0]:\n",
        "        k = k.astype(int)\n",
        "        if k < (dictionary_size-2):\n",
        "            w = vocabulary[k]\n",
        "            text = text + w[0] + ' '\n",
        "    return(text, prob)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HNRRM_sDD1Z4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def preprocess(raw_word, name):\n",
        "    \n",
        "    l1 = ['won’t','won\\'t','wouldn’t','wouldn\\'t','’m', '’re', '’ve', '’ll', '’s','’d', 'n’t', '\\'m', '\\'re', '\\'ve', '\\'ll', '\\'s', '\\'d', 'can\\'t', 'n\\'t', 'B: ', 'A: ', ',', ';', '.', '?', '!', ':', '. ?', ',   .', '. ,', 'SAT', 'END', 'sta', 'end','ight']\n",
        "    l2 = ['will not','will not','would not','would not',' am', ' are', ' have', ' will', ' is', ' had', ' not', ' am', ' are', ' have', ' will', ' is', ' had', 'can not', ' not', '', '', ' ,', ' ;', ' .', ' ?', ' !', ' :', '? ', '.', ',', '', '', '', '','']\n",
        "    l3 = ['-', '_', ' *', ' /', '* ', '/ ', '\\\"', ' \\\\\"', '\\\\ ', '--', '...', '. . .']\n",
        " \n",
        "    raw_word = raw_word.lower()\n",
        "    raw_word = raw_word.replace(', ' + name_of_computer, '')\n",
        "    raw_word = raw_word.replace(name_of_computer + ' ,', '')\n",
        "\n",
        "    for j, term in enumerate(l1):\n",
        "        raw_word = raw_word.replace(term,l2[j])\n",
        "        \n",
        "    for term in l3:\n",
        "        raw_word = raw_word.replace(term,' ')\n",
        "    \n",
        "    for j in range(30):\n",
        "        raw_word = raw_word.replace('. .', '')\n",
        "        raw_word = raw_word.replace('.  .', '')\n",
        "        raw_word = raw_word.replace('..', '')\n",
        "       \n",
        "    for j in range(5):\n",
        "        raw_word = raw_word.replace('  ', ' ')\n",
        "        \n",
        "    if raw_word[-1] !=  '!' and raw_word[-1] != '?' and raw_word[-1] != '.' and raw_word[-2:] !=  '! ' and raw_word[-2:] != '? ' and raw_word[-2:] != '. ':\n",
        "        raw_word = raw_word + ' .'\n",
        "    \n",
        "    if raw_word == ' !' or raw_word == ' ?' or raw_word == ' .' or raw_word == ' ! ' or raw_word == ' ? ' or raw_word == ' . ':\n",
        "        raw_word = 'what ?'\n",
        "    \n",
        "    if raw_word == '  .' or raw_word == ' .' or raw_word == '  . ':\n",
        "        raw_word = 'i do not want to talk about it .'\n",
        "      \n",
        "    return raw_word"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nL5ayvtbEBn0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "def tokenize(sentences):\n",
        "\n",
        "    # Tokenizing the sentences into words:\n",
        "    tokenized_sentences = nltk.word_tokenize(sentences)\n",
        "    index_to_word = [x[0] for x in vocabulary]\n",
        "    word_to_index = dict([(w,i) for i,w in enumerate(index_to_word)])\n",
        "    tokenized_sentences = [w if w in word_to_index else unknown_token for w in tokenized_sentences]\n",
        "    X = np.asarray([word_to_index[w] for w in tokenized_sentences])\n",
        "    s = X.size\n",
        "    Q = np.zeros((1,maxlen_input))\n",
        "    if s < (maxlen_input + 1):\n",
        "        Q[0,:s] = X\n",
        "    else:\n",
        "        Q[0,:] = X[:maxlen_input]\n",
        "    analyzer = SentimentIntensityAnalyzer()\n",
        "    score = analyzer.polarity_scores(sentences)\n",
        "    pos = score['pos']\n",
        "    Q = Q*pos\n",
        "    \n",
        "    return Q"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rLF6QCLwF7Im",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.layers import Input, Embedding, LSTM, Dense, RepeatVector, Dropout, merge\n",
        "from keras.optimizers import Adam \n",
        "from keras.models import Model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Activation, Dense\n",
        "from keras.preprocessing import sequence\n",
        "from keras.layers import concatenate\n",
        "ad = Adam(lr=0.00005) \n",
        "\n",
        "input_context = Input(shape=(maxlen_input,), dtype='int32', name='context_text')\n",
        "input_answer = Input(shape=(maxlen_input,), dtype='int32', name='answer_text')\n",
        "LSTM_encoder = LSTM(sentence_embedding_size, kernel_initializer= 'lecun_uniform', name='Encode_context')\n",
        "LSTM_decoder = LSTM(sentence_embedding_size, kernel_initializer= 'lecun_uniform', name='Encode_answer')\n",
        "if os.path.isfile(weights_file):\n",
        "    Shared_Embedding = Embedding(output_dim=word_embedding_size, input_dim=dictionary_size, input_length=maxlen_input, name='Shared')\n",
        "else:\n",
        "    Shared_Embedding = Embedding(output_dim=word_embedding_size, input_dim=dictionary_size, weights=[embedding_matrix], input_length=maxlen_input, name='Shared')\n",
        "word_embedding_context = Shared_Embedding(input_context)\n",
        "context_embedding = LSTM_encoder(word_embedding_context)\n",
        "\n",
        "word_embedding_answer = Shared_Embedding(input_answer)\n",
        "answer_embedding = LSTM_decoder(word_embedding_answer)\n",
        "\n",
        "merge_layer = concatenate([context_embedding, answer_embedding], axis=1)\n",
        "out = Dense(int(dictionary_size/2), activation=\"relu\")(merge_layer)\n",
        "out = Dense(dictionary_size, activation=\"softmax\")(out)\n",
        "\n",
        "model = Model(inputs=[input_context, input_answer], outputs = [out])\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer=ad)\n",
        "\n",
        "#plot_model(model, to_file='model_graph.png')    \n",
        "\n",
        "if os.path.isfile(weights_file):\n",
        "    model.load_weights(weights_file)\n",
        "\n",
        "\n",
        "# Loading the data:\n",
        "vocabulary = _pickle.load(open(vocabulary_file, 'rb'))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kwRVpBGFGI0E",
        "colab_type": "code",
        "outputId": "b2368366-252b-4364-fab6-4349d3002c3a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 541
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "print(\"\\n \\n \\n \\n    CHAT:     \\n \\n\")\n",
        "prob = 0\n",
        "que = ''\n",
        "last_query  = ' '\n",
        "last_last_query = ''\n",
        "text = ' '\n",
        "last_text = ''\n",
        "print('computer: hi ! please type your name.\\n')\n",
        "name = input('user: ')\n",
        "print('computer: hi , ' + name +' ! My name is ' + name_of_computer + '.\\n') \n",
        "\n",
        "\n",
        "while que != 'exit .':\n",
        "    \n",
        "    que = input('user: ')\n",
        "    que = preprocess(que, name_of_computer)\n",
        "    # Collecting data for training:\n",
        "    q = last_query + ' ' + text\n",
        "    a = que\n",
        "    # Composing the context:\n",
        "    if prob > 0.2:\n",
        "        query = text + ' ' + que\n",
        "    else:    \n",
        "        query = que\n",
        "   \n",
        "    last_text = text\n",
        "    \n",
        "    Q = tokenize(query)\n",
        "    \n",
        "    # Using the trained model to predict the answer:\n",
        "    \n",
        "    predout, prob = greedy_decoder(Q[0:1])\n",
        "    start_index = predout.find('sta')\n",
        "    text = preprocess(predout[0:start_index], name)\n",
        "    print ('computer: ' + text + '    (with probability of %f)'%prob)\n",
        "    \n",
        "    last_last_query = last_query    \n",
        "    last_query = que\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " \n",
            " \n",
            " \n",
            "    CHAT:     \n",
            " \n",
            "\n",
            "computer: hi ! please type your name.\n",
            "\n",
            "user: yi\n",
            "computer: hi , yi ! My name is team5_chatbot.\n",
            "\n",
            "user: there was a greasy kid at highline who was basically miles dipped in a bucket of ranch\n",
            "computer:  i am just trying to convince you and wake up having in 7 minutes in the am . . shit do not expect me down  .    (with probability of 0.000001)\n",
            "user: i killed a mouse today!  \n",
            "computer:  i am gonna call it in the first part of you know you expect you will ? and you can tell them you feel ?     (with probability of 0.000000)\n",
            "user: so much to do before leaving for d.c. ahhh\n",
            "computer:  i do not know it was either or so i can not get of the benefits of the week and all defense and keep all . all . all . it ? ?     (with probability of 0.000000)\n",
            "user: whatever, i'm gonna uncork this bottle and have me a glass of wine.. \n",
            "computer:  wyd you know me then lets me get out there ? is where you know not listen to see again ?     (with probability of 0.097071)\n",
            "user: i don't always drink on tuesday nights/wednesday mornings, but when i do... it's when i just worked from 8am to 2am.\n",
            "computer:  i am gonna do not even do it like that .but i really want to be much though or try to be perfect of us president not  .    (with probability of 0.000000)\n",
            "user: it was so hard getting out of bed this morning \n",
            "computer:  i am gonna do not even do it . like . i know you though , but they do not so much .     (with probability of 0.000009)\n",
            "user: what is it to do on the tuesday\n",
            "computer:  i am not gonna call it back there and the shit . . shit i thought you , i would have to wait .     (with probability of 0.000037)\n",
            "user: bye \n",
            "computer:  i really do not like this ? i do not get my laundry done #showerthoughts and i ai not no . pharma . getting . is .    (with probability of 0.000000)\n",
            "user: exit\n",
            "computer:  i really do not ? i do not know anymore . and so i can keep them pharma getting so  .    (with probability of 0.019276)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
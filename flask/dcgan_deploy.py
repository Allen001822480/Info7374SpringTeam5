# -*- coding: utf-8 -*-
"""dcgan_deploy.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13uXlbZbqfIpCY2ccP6oUFCeIBViBbxSD
"""

import os
import math
import numpy as np
from PIL import Image
from keras.models import Model
from keras.layers import Input, Dense, Reshape, Flatten, Dropout, Embedding, LSTM, concatenate
from keras.layers import BatchNormalization, Activation, ZeroPadding2D
from keras.layers.advanced_activations import LeakyReLU
from keras.layers.convolutional import UpSampling2D, Conv2D
import pickle


class dcgan:
  noise_dim = 20
  embedding_dim = 100
  maxlen = 20
  max_words = 5000
  epoch = 50
  batch_size = 16
  glove_path = './dcgan/glove.6B.100d.txt'# path='./glove.6B.zip_files/glove.6B.100d.txt'
  word_index_path = './dcgan/word_index.pickle'# path='./word_index.pickle'
  generator_model_path = './dcgan/dcgan_01_generator.h5'
  discriminator_model_path = './dcgan/dcgan_01_discriminator.h5'
  snapshot_dir_path = './static'
  
  
  def get_word_index(self):
    with open(self.word_index_path, 'rb') as v:
         word_index = pickle.load(v)
    return word_index
    
  def glove_embed(self):
    embeddings_index = {}
    f = open(self.glove_path)
    for line in f:
      values = line.split()
      word = values[0]
      coefs = np.asarray(values[1:], dtype='float32')
      embeddings_index[word] = coefs
    f.close()
    word_index = self.get_word_index()
    embedding_matrix = np.zeros((self.max_words, self.embedding_dim))
    for word, i in word_index.items():
      embedding_vector = embeddings_index.get(word)
      if i < self.max_words:
          if embedding_vector is not None:
            # Words not found in embedding index will be all-zeros.
             embedding_matrix[i] = embedding_vector
    return embedding_matrix

  def build_model(self):
    embedding_matrix = self.glove_embed()
    # generator
    noise_input = Input(shape=(self.noise_dim,))
    noise_layer = Dense(self.embedding_dim)(noise_input)
    sentence = Input(shape=(self.maxlen,))
    sentence_layer = Embedding(output_dim=self.embedding_dim, input_dim=self.max_words, weights=[embedding_matrix], input_length=self.maxlen)(sentence)
    sentence_layer = LSTM(self.embedding_dim)(sentence_layer)
    merged_layer = concatenate([noise_layer, sentence_layer])
    generator_layer = Dense(128 * 16 * 16, activation="relu")(merged_layer)
    generator_layer = Reshape((16, 16, -1))(generator_layer)
    generator_layer = UpSampling2D()(generator_layer)
    generator_layer = Conv2D(128, kernel_size=3, padding="same")(generator_layer)
    generator_layer = BatchNormalization(momentum=0.8)(generator_layer)
    generator_layer = Activation("relu")(generator_layer)
    generator_layer = UpSampling2D()(generator_layer)
    generator_layer = Conv2D(64, kernel_size=3, padding="same")(generator_layer)
    generator_layer = BatchNormalization(momentum=0.8)(generator_layer)
    generator_layer = Activation("relu")(generator_layer)
    generator_layer = UpSampling2D()(generator_layer)
    generator_layer = Conv2D(32, kernel_size=3, padding="same")(generator_layer)
    generator_layer = BatchNormalization(momentum=0.8)(generator_layer)
    generator_layer = Activation("relu")(generator_layer)
    generator_layer = Conv2D(3, kernel_size=3, padding="same")(generator_layer)
    generator_output = Activation("tanh")(generator_layer)
    self.generator = Model([noise_input, sentence], generator_output)
  
    # discriminator
    image_input = Input(shape=(128, 128, 3))
    image_layer = Conv2D(16, kernel_size=3, strides=2, padding="same")(image_input)
    image_layer = LeakyReLU(alpha=0.2)(image_input)
    image_layer = Dropout(0.25)(image_input)
    image_layer = Conv2D(32, kernel_size=3, strides=2, padding="same")(image_input)
    image_layer = ZeroPadding2D(padding=((0, 1), (0, 1)))(image_layer)
    image_layer = BatchNormalization(momentum=0.8)(image_layer)
    image_layer = LeakyReLU(alpha=0.2)(image_layer)
    image_layer = Dropout(0.25)(image_layer)
    image_layer = Conv2D(64, kernel_size=3, strides=2, padding="same")(image_layer)
    image_layer = BatchNormalization(momentum=0.8)(image_layer)
    image_layer = LeakyReLU(alpha=0.2)(image_layer)
    image_layer = Dropout(0.25)(image_layer)
    image_layer = Conv2D(128, kernel_size=3, strides=1, padding="same")(image_layer)
    image_layer = BatchNormalization(momentum=0.8)(image_layer)
    image_layer = LeakyReLU(alpha=0.2)(image_layer)
    image_layer = Dropout(0.25)(image_layer)
    image_layer = Flatten()(image_layer)
    image_layer = Dense(100)(image_layer)
    text_input = Input(shape=(self.maxlen,))
    text_embedd = Embedding(output_dim=self.embedding_dim, input_dim=self.max_words, weights=[embedding_matrix], input_length=self.maxlen)(text_input)
    text_rnn = LSTM(self.embedding_dim)(text_embedd)
    merged = concatenate([image_layer, text_rnn])
    discriminator_layer = Activation('tanh')(merged)
    discriminator_layer = Dense(1)(discriminator_layer)
    validity = Activation('sigmoid')(discriminator_layer)
    self.discriminator = Model([image_input, text_input], validity)
  
    model_output = self.discriminator([self.generator.output, sentence])
    self.combined_model = Model([noise_input, sentence], model_output)
  
  def load_weights(self):
    if os.path.isfile(self.generator_model_path):
         self.generator.load_weights(self.generator_model_path)
    if os.path.isfile(self.discriminator_model_path):
         self.discriminator.load_weights(self.discriminator_model_path)
      
#  def process_text(self, input_text):
#    word_index = self.get_word_index()
#    input_words = input_text.lower().split()
#    input_vector = []
#    for word, i in word_index.items():
#      for w in input_words:
#        if w == word:
#             input_vector.append(i)
#    x = len(input_vector)
#    input_vector = np.array(input_vector)
#    Q = np.zeros((1,self.maxlen))
#    if x < (self.maxlen):
#         Q = input_vector[:self.maxlen]
#    else:
#         Q = input_vector[:self.maxlen]
#    return Q
         
#  def process_text(self, input_text):
#    word_index = self.get_word_index()
#    input_words = input_text.lower().split()
#    input_vector = []
#    for word, i in word_index.items():
#      for w in input_words:
#        if w == word:
#             input_vector.append(i)
#    x = len(input_vector)
#    input_vector = np.array(input_vector)
#    Q = np.zeros((1, self.maxlen))
#    if x < (self.maxlen):
#         Q = np.zeros(self.maxlen)
#    else:
#         Q = input_vector[:self.maxlen]
#    return Q
         
  def process_text(self, input_text):
    word_index = self.get_word_index()
    input_words = input_text.lower().split()
    input_vector = []
    for word, i in word_index.items():
      for w in input_words:
        if w == word:
             input_vector.append(i)
    x = len(input_vector)
    input_vector = np.array(input_vector).reshape(1,len(input_vector))
    Q = np.zeros((1,self.maxlen))
    if x < (self.maxlen):
         Q[0,-x:] = input_vector
    else:
         Q[0,:] = input_vector[0,:self.maxlen]
    return Q
         
  def generate_image(self, input_text):
    noise = np.zeros((1,self.noise_dim))
    noise[:] = np.random.uniform(-1, 1, self.noise_dim)
    input_vector = self.process_text(input_text)
    scaled_image = self.generator.predict([noise, input_vector], verbose=0)
    image = self.img_from_normalized_img(scaled_image)
    return image
  
#  def generate_image(self, input_text):
#    noise = np.zeros((1, self.noise_dim))
#    noise = np.random.uniform(-1, 1, self.noise_dim)
#    input_vector = self.process_text(input_text)
##    noise_input = np.concatenate((noise, input_vector), axis=0)
#    print(noise.shape)
#    print(input_vector.shape)
#    noise = noise.reshape(20,).T
#    input_vector = input_vector.reshape(20,).T
#    scaled_image = self.generator.predict([noise, input_vector], verbose=0)
#    image = self.img_from_normalized_img(scaled_image)
#    return image

    
  def img_from_normalized_img(self, normalized_img):
    image = normalized_img * 127.5 + 127.5
    return image

#  def combine_normalized_images(self, generated_images):
#    num = generated_images.shape[0]
#    width = int(math.sqrt(num))
#    height = int(math.ceil(float(num) / width))
#    shape = generated_images.shape[1:]
#    image = np.zeros((height * shape[0], width * shape[1], shape[2]),dtype=generated_images.dtype)
#    for index, img in enumerate(generated_images):
#      i = int(index / width)
#      j = index % width
#      image[i * shape[0]:(i + 1) * shape[0], j * shape[1]:(j + 1) * shape[1], :] = img
#    return image

  def get_image(self,input_text):
    model = dcgan()
    model.build_model()
    model.load_weights()
    generated_image = model.generate_image(input_text)
    generated_image = generated_image.reshape(128,128,3)
    generated_image = Image.fromarray(generated_image.astype(np.uint8))
#    generated_image = dcgan.combine_normalized_images(generated_image)
    file_name = "predict.png"
    file_path = os.path.join(self.snapshot_dir_path, file_name)
    generated_image = generated_image.save(file_path)
    return file_name